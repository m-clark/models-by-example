[["index.html", "Model Estimation by Example Demonstrations with R", " Model Estimation by Example Demonstrations with R Michael Clark m-clark.github.io 2020-11-07 "],["introduction.html", "Introduction", " Introduction This document provides ‘by-hand’ demonstrations of various models and algorithms. The goal is to take away some of the mystery of them by providing clean code examples that are easy to run and compare with other tools. The code was collected over several years without respect to any previous code, so is not exactly consistent in style. But in general, within each demo you will find some imported/simulated data, a primary estimating function, a comparison of results with some R package, and a link to the source code that is demonstrated. This code is not meant to be extensive, or used in production, and in fact, some of these would probably be considered of historical interest only. To be clear, almost everything here has a package/module that would do the same thing far better and efficiently. Note also, the document itself is also not an introduction to any of these methods, and in fact contains very little expository text, assuming the reader has some familiarity with the model and possibly some reference text. This document is merely a learning tool for those wanting to dive a little deeper. The Original code for these demonstrations may be found at their original home here: https://github.com/m-clark/Miscellaneous-R-Code. Many examples require some initial data processing or visualization via ggplot2, so it’s assumed the tidyverse set of packages is loaded. While I’m happy to fix any glaring errors and broken links, this is pretty much a completed document, except on the off chance I add to it on rare occasion. This code has accumulated over years, and I just wanted it in a nicer format. Perhaps if others would like to add to it via pull requests, I would do so. "],["linear-regression.html", "Linear Regression Data Setup Functions Estimation Comparison Source", " Linear Regression A standard regression model via maximum likelihood or least squares loss. Also examples for QR decomposition and normal equations. Can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Least squares loss is not confined to the standard regression setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. Data Setup library(tidyverse) set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X, y) Functions A maximum likelihood approach. lm_ML = function(par, X, y) { # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } An approach via least squares loss function. lm_LS = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link # calculate least squares loss function L = crossprod(y - mu) } Estimation Setup for use with optim. X = cbind(1, X) Initial values. Note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach as it is the objective function. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = lm_ML, X = X, y = y, control = list(reltol = 1e-8) ) optlmLS = optim( par = init[-1], fn = lm_LS, X = X, y = y, control = list(reltol = 1e-8) ) pars_ML = optlmML$par pars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par) # calculate sigma2 and add Comparison Compare to lm which uses QR decomposition. modlm = lm(y ~ ., dfXy) Example of QR. # QRX = qr(X) # Q = qr.Q(QRX) # R = qr.R(QRX) # Bhat = solve(R) %*% crossprod(Q, y) # alternate: qr.coef(QRX, y) round( rbind( pars_ML, pars_LS, modlm = c(summary(modlm)$sigma^2, coef(modlm))), digits = 3 ) sigma2 intercept b1 b2 pars_ML 0.219 -0.432 0.133 0.112 pars_LS 0.226 -0.432 0.133 0.112 modlm 0.226 -0.432 0.133 0.112 The slight difference in sigma is roughly dividing by N vs. N-k-1 in the traditional least squares approach. It diminishes with increasing N as both tend toward whatever sd^2 you specify when creating the y response above. Compare to glm, which by default assumes gaussian family with identity link and uses lm.fit. modglm = glm(y ~ ., data = dfXy) summary(modglm) Call: glm(formula = y ~ ., data = dfXy) Deviance Residuals: Min 1Q Median 3Q Max -0.93651 -0.33037 -0.06222 0.31068 1.03991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.43247 0.04807 -8.997 1.97e-14 *** X1 0.13341 0.05243 2.544 0.0125 * X2 0.11191 0.04950 2.261 0.0260 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.2262419) Null deviance: 24.444 on 99 degrees of freedom Residual deviance: 21.945 on 97 degrees of freedom AIC: 140.13 Number of Fisher Scoring iterations: 2 Via normal equations. coefs = solve(t(X) %*% X) %*% t(X) %*% y # coefficients Compare. sqrt(crossprod(y - X %*% coefs) / (N - k - 1)) [,1] [1,] 0.4756489 summary(modlm)$sigma [1] 0.4756489 sqrt(modglm$deviance / modglm$df.residual) [1] 0.4756489 c(sqrt(pars_ML[1]), sqrt(pars_LS[1])) sigma2 sigma2 0.4684616 0.4756490 # rerun by adding 3-4 zeros to the N Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_lm.R "],["logistic-regression.html", "Logistic Regression Data Setup Functions Estimation Comparison Source", " Logistic Regression A standard logistic regression model via maximum likelihood or exponential loss. Can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Exponential loss is not confined to the standard GLM setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. This follows the linear regression model approach. Data Setup Predictors and target. library(tidyverse) set.seed(1235) # ensures replication N = 10000 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) # the linear predictor lp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these y = rbinom(N, size = 1, prob = plogis(lp)) dfXy = data.frame(X, y) Functions A maximum likelihood approach. logreg_ML = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = plogis(LP) # logit link # calculate likelihood L = dbinom(y, size = 1, prob = mu, log = TRUE) # log likelihood # L = y*log(mu) + (1 - y)*log(1-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } Another approach via exponential loss function. logreg_exp = function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor # calculate exponential loss function (convert y to -1:1 from 0:1) L = sum(exp(-ifelse(y, 1, -1) * .5 * LP)) } Estimation Setup for use with optim. X = cbind(1, X) # initial values init = rep(0, ncol(X)) names(init) = c(&#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = logreg_ML, X = X, y = y, control = list(reltol = 1e-8) ) optglmClass = optim( par = init, fn = logreg_exp, X = X, y = y, control = list(reltol = 1e-15) ) pars_ML = optlmML$par pars_exp = optglmClass$par Comparison Compare to glm. modglm = glm(y ~ ., dfXy, family = binomial) rbind( pars_ML, pars_exp, pars_GLM = coef(modglm) ) intercept b1 b2 pars_ML -0.5117658 0.2378927 0.08019841 pars_exp -0.5114284 0.2368478 0.07907056 pars_GLM -0.5117321 0.2378743 0.08032617 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_logistic.R "],["mixed-model-one-factor.html", "One-factor Mixed Model Data Setup Function Estimation Comparison Source", " One-factor Mixed Model An approach for one factor random effects model via maximum likelihood in R Matlab and Julia. It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. Unfortunately I did this before knowing they had both Matlab and R code on their website, though the R code here is a little cleaner and has comments. The data regards crop yield from 10 randomly selected locations and three collections at each. See one_factor_RE.m and one_factor_RE.jl for the related Matlab and Julia files, and the respective twofactorRE.* for the associated two factor random effects examples. Data Setup library(tidyverse) y = matrix(c(22.6,20.5,20.8, 22.6,21.2,20.5, 17.3,16.2,16.6, 21.4,23.7,23.2, 20.9,22.2,22.6, 14.5,10.5,12.3, 20.8,19.1,21.3, 17.4,18.6,18.6, 25.1,24.8,24.9, 14.9,16.3,16.6), 10, 3, byrow=T) Function one_factor_re = function(mu, sigma2_mu, sigma2){ # Args - # mu: intercept # sigma2_mu: variance of intercept # sigma2: residual variance of y # I follow their notation for consistency d = nrow(y) ni = ncol(y) # covariance matrix of observations Sigmai = sigma2 * diag(ni) + sigma2_mu * matrix(1, ni, ni) # log likelihood l = rep(NA, 10) # iterate over the rows for(i in 1:d){ l[i] = .5 * t(y[i, ] - mu) %*% chol2inv(chol(Sigmai)) %*% (y[i, ] - mu) } ll = -(ni*d) / 2*log(2*pi) - d / 2*log(det(Sigmai)) - sum(l) return(-ll) } Estimation Starting values starts = list( mu = mean(y), sigma2_mu = var(rowMeans(y)), sigma2 = mean(apply(y, 1, var)) ) Estimate one_factor_re(starts[[1]], starts[[2]], starts[[3]]) [1] 62.30661 Comparison Package bbmle has an mle2 function for maximum likelihood estimation based on underlying R functions like optim. LBFGS-B is used to place lower bounds on the variance estimates. library(bbmle) mlout = mle2( one_factor_re , start = starts, method = &#39;L-BFGS-B&#39;, lower = c( mu = -Inf, sigma2_mu = 0, sigma2 = 0 ), trace = T ) library(lme4) library(tidyverse) d = data.frame(y) %&gt;% pivot_longer(everything(), names_to = &#39;x&#39;, values_to = &#39;value&#39;) %&gt;% arrange(x) %&gt;% group_by(x) %&gt;% mutate(group = 1:n()) lme = lmer(value ~ 1 | group, data = d, REML = F) summary(mlout) Maximum likelihood estimation Call: mle2(minuslogl = one_factor_re, start = starts, method = &quot;L-BFGS-B&quot;, trace = T, lower = c(mu = -Inf, sigma2_mu = 0, sigma2 = 0)) Coefficients: Estimate Std. Error z value Pr(z) mu 19.60000 1.12173 17.4729 &lt; 2.2e-16 *** sigma2_mu 12.19400 5.62858 2.1664 0.030277 * sigma2 1.16667 0.36893 3.1623 0.001565 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: 124.5288 summary(lme) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: value ~ 1 | group Data: d AIC BIC logLik deviance df.resid 130.5 134.7 -62.3 124.5 27 Scaled residuals: Min 1Q Median 3Q Max -1.9950 -0.6555 0.1782 0.4870 1.7083 Random effects: Groups Name Variance Std.Dev. group (Intercept) 12.194 3.492 Residual 1.167 1.080 Number of obs: 30, groups: group, 10 Fixed effects: Estimate Std. Error t value (Intercept) 19.600 1.122 17.47 -2 * logLik(lme) &#39;log Lik.&#39; 124.5288 (df=3) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/one_factor_RE.R "],["mixed-model-two-factor.html", "Two-factor Mixed Model Data Setup Function Estimation Comparison Source", " Two-factor Mixed Model An approach for two factor random effects model via maximum likelihood in R Matlab and Julia. It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. The data regards the breeding value of a set of five sires in raising pigs. Each sire is mated to a random group of dams, with the response being the average daily weight gain in pounds of two piglets in each litter. See the previous chapter for a one factor model, and two_factor_RE.m and two_factor_RE.jl for the Matlab and Julia versions of this example on the GitHub site. Note that the text has a typo on the sigma2 variance estimate (value should be .0023 not .023). Data Setup library(tidyverse) y = c(1.39,1.29,1.12,1.16,1.52,1.62,1.88,1.87,1.24,1.18, .95,.96,.82,.92,1.18,1.20,1.47,1.41,1.57,1.65) # for use in lme4, but also a more conceptual representation of the data d = expand.grid(sire = rep(1:5, 2), dam = 1:2) d = data.frame(d[order(d$sire), ], y) Function The function takes the log variances eta* as input to keep positive. two_factor_re = function(mu, eta_alpha, eta_gamma, eta) { # Args # mu: intercept # eta_alpha: random effect one # eta_gamma: random effect two # eta: residual variance of y sigma2_alpha = exp(eta_alpha) sigma2_gamma = exp(eta_gamma) sigma2 = exp(eta) n = length(y) # covariance matrix of observations Sigma = sigma2 * diag(n) + sigma2_alpha * tcrossprod(Xalpha) + sigma2_gamma * tcrossprod(Xgamma) # log likelihood ll = -n / 2 * log(2 * pi) - sum(log(diag(chol(Sigma)))) - .5 * t(y - mu) %*% chol2inv(chol(Sigma)) %*% (y - mu) return(-ll) } Estimation Starting values and test. starts = list( mu = mean(y), eta_alpha = var(tapply(y, d$sire, mean)), eta_gamma = var(y) / 3, eta = var(y) / 3 ) Xalpha = diag(5) %x% rep(1,4) Xgamma = diag(10) %x% rep(1,2) Estimation. two_factor_re(starts[[1]], starts[[2]], starts[[3]], starts[[4]]) [,1] [1,] 26.53887 Comparison library(bbmle) mlout = mle2(two_factor_re, start=starts, method=&#39;BFGS&#39;) ### lme4 comparison library(lme4) lme = lmer(y ~ (1 | sire) + (1 | dam:sire), d, REML = F) summary(mlout) Maximum likelihood estimation Call: mle2(minuslogl = two_factor_re, start = starts, method = &quot;BFGS&quot;) Coefficients: Estimate Std. Error z value Pr(z) mu 1.32000 0.11848 11.1410 &lt; 2.2e-16 *** eta_alpha -2.92393 0.84877 -3.4449 0.0005712 *** eta_gamma -3.44860 0.65543 -5.2616 1.428e-07 *** eta -6.07920 0.44721 -13.5935 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: -23.98631 exp(coef(mlout)[-1]) eta_alpha eta_gamma eta 0.05372198 0.03178996 0.00229000 summary(lme) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: y ~ (1 | sire) + (1 | dam:sire) Data: d AIC BIC logLik deviance df.resid -16 -12 12 -24 16 Scaled residuals: Min 1Q Median 3Q Max -1.21052 -0.59450 0.02314 0.61984 1.10386 Random effects: Groups Name Variance Std.Dev. dam:sire (Intercept) 0.03179 0.17830 sire (Intercept) 0.05372 0.23178 Residual 0.00229 0.04785 Number of obs: 20, groups: dam:sire, 10; sire, 5 Fixed effects: Estimate Std. Error t value (Intercept) 1.3200 0.1185 11.14 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/two_factor_RE.R "],["mixed-model-ML.html", "Mixed Model via ML Introduction Maximum Likelihood Estimation Source", " Mixed Model via ML Introduction The following is based on the @Wood text on additive models, chapter 6 in particular. It assumes familiarity with standard regression from a matrix perspective and at least passing familiarity with mixed models. The full document this chapter is based on can be found here, and contains more detail and exposition. Maximum Likelihood Estimation For this we’ll use the sleepstudy data from the lme4 package. The data has reaction times for 18 individuals over 10 days each (see the help file for the sleepstudy object for more details). Data Setup library(tidyverse) data(sleepstudy, package = &#39;lme4&#39;) X = model.matrix(~Days, sleepstudy) Z = model.matrix(~factor(sleepstudy$Subject) - 1) colnames(Z) = paste0(&#39;Subject_&#39;, unique(sleepstudy$Subject)) # for cleaner presentation later rownames(Z) = paste0(&#39;Subject_&#39;, sleepstudy$Subject) y = sleepstudy$Reaction Function The following is based on the code in Wood (6.2.2), with a couple modifications for consistent nomenclature and presentation. We use optim and a minimizing function, in this case the negative log likelihood, to estimate the parameters of interest, collectively \\(\\theta\\), in the code below. The (square root of the) variances will be estimated on the log scale. In Wood, he simply extracts the ‘fixed effects’ for the intercept and days effects using lm (6.2.3), and we’ll do the same. ll_mixed = function(y, X, Z, theta){ tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 L = chol(e) # L&#39;L = e # transform dependent linear model to independent y = backsolve(L, y, transpose=TRUE) X = backsolve(L, X, transpose=TRUE) b = coef(lm(y~X-1)) LP = X %*% b ll = -n/2*log(2*pi) -sum(log(diag(L))) - crossprod(y-LP)/2 -ll } Here is an alternative function using a multivariate normal approach that doesn’t use the transformation to independent, and might provide additional perspective. ll_mixedMV = function(y, X, Z, theta){ tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 b = coef(lm.fit(X, y)) mu = X %*% b ll = mvtnorm::dmvnorm(y, mu, e, log=T) -ll } Results Now we’re ready to use the optim function for estimation. A slight change to tolerance is included to get closer estimates to lme4, which we will compare shortly. paramInit = c(0, 0) names(paramInit) = c(&#39;tau&#39;, &#39;sigma&#39;) modelResults = optim( fn = ll_mixed, X = X, y = y, Z = Z, par = paramInit, control = list(reltol = 1e-10) ) modelResultsMV = optim( fn = ll_mixedMV, X = X, y = y, Z = Z, par = paramInit, control = list(reltol = 1e-10) ) rbind( c(exp(modelResults$par), negLogLik = modelResults$value, coef(lm(y ~ X - 1))), c(exp(modelResultsMV$par), negLogLik = modelResultsMV$value, coef(lm(y ~ X - 1))) ) %&gt;% round(2) tau sigma negLogLik X(Intercept) XDays [1,] 36.02 30.9 897.04 251.41 10.47 [2,] 36.02 30.9 897.04 251.41 10.47 As we can see, both formulations produce identical results. We can now compare those results to the lme4 output for the same model, and see that we’re getting what we should. library(lme4) lmeMod = lmer(Reaction ~ Days + (1|Subject), sleepstudy, REML = FALSE) lmeMod Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: Reaction ~ Days + (1 | Subject) Data: sleepstudy AIC BIC logLik deviance df.resid 1802.0786 1814.8505 -897.0393 1794.0786 176 Random effects: Groups Name Std.Dev. Subject (Intercept) 36.01 Residual 30.90 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 We can also predict the random effects (Wood, 6.2.4), and after doing so again compare the results to the lme4 estimates. tau = exp(modelResults$par)[1] tausq = tau^2 sigma = exp(modelResults$par)[2] sigmasq = sigma^2 Sigma = tcrossprod(Z)*tausq/sigmasq + diag(length(y)) ranefEstimated = tausq*t(Z)%*%solve(Sigma) %*% resid(lm(y~X-1))/sigmasq data.frame( ranefEstimated, lme4 = ranef(lmeMod)$Subject[[1]] ) %&gt;% round(2) ranefEstimated lme4 Subject_308 40.64 40.64 Subject_309 -77.57 -77.57 Subject_310 -62.88 -62.88 Subject_330 4.39 4.39 Subject_331 10.18 10.18 Subject_332 8.19 8.19 Subject_333 16.44 16.44 Subject_334 -2.99 -2.99 Subject_335 -45.12 -45.12 Subject_337 71.92 71.92 Subject_349 -21.12 -21.12 Subject_350 14.06 14.06 Subject_351 -7.83 -7.83 Subject_352 36.25 36.25 Subject_369 7.01 7.01 Subject_370 -6.34 -6.34 Subject_371 -3.28 -3.28 Subject_372 18.05 18.05 Issues with ML estimation Situations arise in which using maximum likelihood for mixed models would result in notably biased estimates (e.g. small N, lots of fixed effects), and so it is typically not used. Standard software usually defaults to restricted maximum likelihood. However, our purpose here has been served, so we will not dwell further on mixed model estimation. Link with penalized regression A link exists between mixed models and a penalized likelihood approach. For a penalized approach with the SLiM, the objective function we want to minimize can be expressed as follows: \\[ \\lVert y- X\\beta \\rVert^2 + \\beta^\\intercal\\beta \\] The added component to the sum of the squared residuals is the penalty. By adding the sum of the squared coefficients, we end up keeping them from getting too big, and this helps to avoid overfitting. Another interesting aspect of this approach is that it is comparable to using a specific prior on the coefficients in a Bayesian framework. We can now see mixed models as a penalized technique. If we knew \\(\\sigma\\) and \\(\\psi_\\theta\\), then the predicted random effects \\(g\\) and estimates for the fixed effects \\(\\beta\\) are those that minimize the following objective function: \\[ \\frac{1}{\\sigma^2}\\lVert y - X\\beta - Zg \\rVert^2 + g^\\intercal\\psi_\\theta^{-1}g \\] Source Main doc found at https://m-clark.github.io/docs/mixedModels/mixedModelML.html "],["probit.html", "Probit &amp; Bivariate Probit Standard Probit Bivariate Probit Source", " Probit &amp; Bivariate Probit Stata users seem to be the primary audience concerned with these models, but I thought I’d play around with one here (I’ve never had reason to use a probit model in practice). Stata examples come from the UCLA ATS website and the Stata manual, so one can investigate the Stata result for comparison. Standard Probit Function probit_ll = function(beta, X, y){ mu = X %*% beta # these produce identical results, but the second is the typical depiction ll = sum(dbinom( y, size = 1, prob = pnorm(mu), log = T )) # ll = sum(y * pnorm(mu, log = T) + (1 - y) * log(1 - pnorm(mu))) -ll } Examples Example 1 available at https://stats.idre.ucla.edu/stata/dae/probit-regression/ library(tidyverse) admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) head(admit) # A tibble: 6 x 4 admit gre gpa rank &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 380 3.61 3 2 1 660 3.67 3 3 1 800 4 1 4 1 640 3.19 4 5 0 520 2.93 4 6 1 760 3 2 X = model.matrix(admit~ gre + gpa + factor(rank), admit) y = admit$admit init = rep(0, ncol(X)) optimResult = optim( fn = probit_ll, par = init, X = X, y = y, method = &#39;BFGS&#39; ) optimResult $par [1] -2.2518260555 0.0007915268 0.5453357468 -0.4211611887 -0.8285356685 -0.9457812896 $value [1] 229.6227 $counts function gradient 140 11 $convergence [1] 0 $message NULL Example 2 from Stata manual on standard probit We have data on the make, weight, and mileage rating of 22 foreign and 52 domestic automobiles. We wish to fit a probit model explaining whether a car is foreign based on its weight and mileage.\" auto = haven::read_dta(&#39;http://www.stata-press.com/data/r13/auto.dta&#39;) head(auto) # A tibble: 6 x 12 make price mpg rep78 headroom trunk weight length turn displacement gear_ratio foreign &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; 1 AMC Concord 4099 22 3 2.5 11 2930 186 40 121 3.58 0 [Domestic] 2 AMC Pacer 4749 17 3 3 11 3350 173 40 258 2.53 0 [Domestic] 3 AMC Spirit 3799 22 NA 3 12 2640 168 35 121 3.08 0 [Domestic] 4 Buick Century 4816 20 3 4.5 16 3250 196 40 196 2.93 0 [Domestic] 5 Buick Electra 7827 15 4 4 20 4080 222 43 350 2.41 0 [Domestic] 6 Buick LeSabre 5788 18 3 4 21 3670 218 43 231 2.73 0 [Domestic] X = model.matrix(foreign~ weight + mpg, auto) y = auto$foreign init = rep(0, ncol(X)) optimResult = optim( fn = probit_ll, par = init, X = X, y = y ) optimResult $par [1] 8.277015097 -0.002335939 -0.103973147 $value [1] 26.84419 $counts function gradient 380 NA $convergence [1] 0 $message NULL Bivariate Probit Main function. bivariateprobit_ll = function(pars, X, y1, y2) { rho = pars[1] mu1 = X %*% pars[2:(ncol(X) + 1)] mu2 = X %*% pars[(ncol(X) + 2):length(pars)] q1 = ifelse(y1 == 1, 1,-1) q2 = ifelse(y2 == 1, 1,-1) require(mnormt) eta1 = q1 * mu1 eta2 = q2 * mu2 ll = matrix(NA, nrow = nrow(X)) for (i in 1:length(ll)) { corr = q1[i] * q2[i] * rho corr = matrix(c(1, corr, corr, 1), 2) ll[i] = log( pmnorm( x = c(eta1[i], eta2[i]), mean = c(0, 0), varcov = corr ) ) } # the loop is probably clearer, and there is no difference in time, but here&#39;s # a oneliner ll = mapply(function(e1, e2, q1, q2) log(pmnorm(x = c(e1, e2), # varcov = matrix(c(1,q1*q2*rho,q1*q2*rho,1),2))), eta1, eta2, q1, q2) -sum(ll) } Example From stata manual on bivariate probit: We wish to model the bivariate outcomes of whether children attend private school and whether the head of the household voted for an increase in property tax based on the other covariates. school = haven::read_dta(&#39;http://www.stata-press.com/data/r13/school.dta&#39;) head(school) # A tibble: 6 x 11 obs pub12 pub34 pub5 private years school loginc logptax vote logeduc &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0 1 0 0 10 1 9.77 7.05 1 7.21 2 2 0 1 0 0 8 0 10.0 7.05 0 7.61 3 3 1 0 0 0 4 0 10.0 7.05 0 8.28 4 4 0 1 0 0 13 0 9.43 6.40 0 6.82 5 5 0 1 0 0 3 1 10.0 7.28 1 7.69 6 6 1 0 0 0 5 0 10.5 7.05 0 6.91 X = model.matrix(private ~ years + logptax + loginc, school) y1 = school$private y2 = school$vote init = c(0, rep(0, ncol(X)*2)) # you&#39;ll probably get a warning or two, ignore; takes a couple seconds optimResult = optim( fn = bivariateprobit_ll, par = init, X = X, y1 = y1, y2 = y2, method = &#39;BFGS&#39; ) loglik = optimResult$value rho = optimResult$par[1] coefsPrivate = optimResult$par[2:(ncol(X) + 1)] coefsVote = optimResult$par[(ncol(X) + 2):length(init)] names(coefsPrivate) = names(coefsVote) = c(&#39;Int&#39;, &#39;years&#39;, &#39;logptax&#39;, &#39;loginc&#39;) list( loglik = loglik, rho = rho, Private = coefsPrivate, Vote = coefsVote ) $loglik [1] 89.25407 $rho [1] -0.2695802 $Private Int years logptax loginc -4.14327955 -0.01193699 -0.11030513 0.37459892 $Vote Int years logptax loginc -0.52933721 -0.01686685 -1.28983223 0.99840976 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/bivariateProbit.R "],["heckman-selection.html", "Heckman Selection Data Setup Two step approach Maximum Likelihood Source", " Heckman Selection Based on Bleven’s example here: https://www3.nd.edu/~wevans1/ecoe60303/sample_selection_example.ppt, but which is more or less the ‘classic’ example, variations of which you’ll find all over regarding women’s wages. Description of the data: Draw 10,000 obs at random educ uniform over [0,16] age uniform over [18,64] wearnl = 4.49 + 0.08 * educ + 0.012 * age + ε Generate missing data for wearnl drawn z from standard normal [0,1]. z is actually never explained in the slides, I think it’s left out on slide 3 and just represents an additional covariate. d*=-1.5+0.15*educ+0.01*age+0.15*z+v wearnl missing if d*≤0 wearn reported if d*&gt;0 wearnl_all = wearnl with non-missing obs Data Setup library(tidyverse) set.seed(123456) N = 10000 educ = sample(1:16, N, replace = T) age = sample(18:64, N, replace = T) covmat = matrix(c(.46^2, .25*.46, .25*.46, 1), ncol = 2) errors = mvtnorm::rmvnorm(N, sigma = covmat) z = rnorm(N) e = errors[, 1] v = errors[, 2] wearnl = 4.49 + .08 * educ + .012 * age + e d_star = -1.5 + 0.15 * educ + 0.01 * age + 0.15 * z + v observed_index = d_star &gt; 0 d = data.frame(wearnl, educ, age, z, observed_index) Examine linear regression approaches if desired. # lm based on full data lm_all = lm(wearnl ~ educ + age, data=d) # lm based on observed data lm_obs = lm(wearnl ~ educ + age, data=d[observed_index,]) summary(lm_all) Call: lm(formula = wearnl ~ educ + age, data = d) Residuals: Min 1Q Median 3Q Max -2.03286 -0.31240 0.00248 0.31578 1.50828 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.4691266 0.0171413 260.72 &lt;2e-16 *** educ 0.0798814 0.0010005 79.84 &lt;2e-16 *** age 0.0124381 0.0003398 36.60 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4611 on 9997 degrees of freedom Multiple R-squared: 0.4331, Adjusted R-squared: 0.433 F-statistic: 3820 on 2 and 9997 DF, p-value: &lt; 2.2e-16 summary(lm_obs) # smaller coefs, resid standard error Call: lm(formula = wearnl ~ educ + age, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75741 -0.30289 -0.00133 0.30918 1.50032 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.6713823 0.0258865 180.46 &lt;2e-16 *** educ 0.0705849 0.0014760 47.82 &lt;2e-16 *** age 0.0114758 0.0004496 25.52 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4507 on 5517 degrees of freedom Multiple R-squared: 0.3374, Adjusted R-squared: 0.3372 F-statistic: 1405 on 2 and 5517 DF, p-value: &lt; 2.2e-16 Two step approach The two-step approach first conducts a probit model regarding whether the individual is observed or not, in order to calculate the inverse mills ratio, or ‘nonselection hazard’. The second step is a standard lm. Step 1: Probit Model probit = glm(observed_index ~ educ + age + z, data = d, family = binomial(link = &#39;probit&#39;)) summary(probit) Call: glm(formula = observed_index ~ educ + age + z, family = binomial(link = &quot;probit&quot;), data = d) Deviance Residuals: Min 1Q Median 3Q Max -2.4674 -0.9062 0.4628 0.8800 2.2674 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.519248 0.052819 -28.763 &lt;2e-16 *** educ 0.150027 0.003220 46.588 &lt;2e-16 *** age 0.010072 0.001015 9.926 &lt;2e-16 *** z 0.159292 0.013889 11.469 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 13755 on 9999 degrees of freedom Residual deviance: 11119 on 9996 degrees of freedom AIC: 11127 Number of Fisher Scoring iterations: 4 # http://www.stata.com/support/faqs/statistics/inverse-mills-ratio/ probit_lp = predict(probit) mills0 = dnorm(probit_lp)/pnorm(probit_lp) summary(mills0) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.38632 0.70027 0.75664 1.09246 1.96602 # identical formulation # probit_lp = -predict(probit) # imr = dnorm(probit_lp)/(1-pnorm(probit_lp)) imr = mills0[observed_index] summary(imr) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.28739 0.48466 0.57015 0.77617 1.87858 Take a look at the distribution. ggplot2::qplot(imr, geom = &#39;histogram&#39;) Step 2: Estimate via Linear Regression Standard regression model using the inverse mills ratio as covariate lm_select = lm(wearnl ~ educ + age + imr, data = d[observed_index, ]) summary(lm_select) Call: lm(formula = wearnl ~ educ + age + imr, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75994 -0.30293 -0.00186 0.31049 1.48179 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1063144 42.477 &lt;2e-16 *** educ 0.0782580 0.0052989 14.769 &lt;2e-16 *** age 0.0119700 0.0005564 21.513 &lt;2e-16 *** imr 0.0955209 0.0633557 1.508 0.132 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4506 on 5516 degrees of freedom Multiple R-squared: 0.3377, Adjusted R-squared: 0.3373 F-statistic: 937.4 on 3 and 5516 DF, p-value: &lt; 2.2e-16 Compare to sampleSelection package. library(sampleSelection) selection_2step = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;2step&#39;) summary(selection_2step) -------------------------------------------- Tobit 2 model (sample selection model) 2-step Heckman / heckit estimation 10000 observations (4480 censored and 5520 observed) 10 free parameters (df = 9991) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.519248 0.052725 -28.815 &lt;2e-16 *** educ 0.150027 0.003221 46.577 &lt;2e-16 *** age 0.010072 0.001014 9.934 &lt;2e-16 *** z 0.159292 0.013937 11.430 &lt;2e-16 *** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1066914 42.33 &lt;2e-16 *** educ 0.0782580 0.0053181 14.71 &lt;2e-16 *** age 0.0119700 0.0005592 21.41 &lt;2e-16 *** Multiple R-Squared:0.3377, Adjusted R-Squared:0.3373 Error terms: Estimate Std. Error t value Pr(&gt;|t|) invMillsRatio 0.09552 0.06354 1.503 0.133 sigma 0.45550 NA NA NA rho 0.20970 NA NA NA -------------------------------------------- coef(lm_select)[&#39;imr&#39;] / summary(lm_select)$sigma # slightly off imr 0.2119813 coef(lm_select)[&#39;imr&#39;] / summary(selection_2step)$estimate[&#39;sigma&#39;, &#39;Estimate&#39;] imr 0.2097041 Maximum Likelihood The following likelihood function takes arguments as follows: - par: the regression coefficients pertaining to the two models, the residual standard error - sigma and rho for the correlation estimate - X: observed data model matrix for the linear regression model - Z: complete data model matrix for the probit model - y: the target variable - observed_index: an index denoting whether y is observed select_ll &lt;- function(par, X, Z, y, observed_index) { gamma = par[1:4] lp_probit = Z %*% gamma beta = par[5:7] lp_lm = X %*% beta sigma = par[8] rho = par[9] ll = sum(log(1-pnorm(lp_probit[!observed_index]))) + - log(sigma) + sum(dnorm(y, mean = lp_lm, sd = sigma, log = TRUE)) + sum( pnorm((lp_probit[observed_index] + rho/sigma * (y-lp_lm)) / sqrt(1-rho^2), log.p = TRUE) ) - ll } X = model.matrix(lm_select) Z = model.matrix(probit) # initial values init = c(coef(probit), coef(lm_select)[-4], 1, 0) Estimate via optim. Without bounds for sigma and rho you’ll get warnings, but does fine anyway heckman_ml_unbounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;BFGS&#39;, control = list(maxit = 1000, reltol = 1e-15), hessian = T ) heckman_ml_bounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;L-BFGS&#39;, lower = c(rep(-Inf, 7), 1e-10,-1), upper = c(rep(Inf, 8), 1), control = list(maxit = 1000, factr = 1e-15), hessian = T ) Comparison Comparison model. selection_ml = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;ml&#39;) # summary(selection_ml) library(tidyverse) # compare coefficients tibble( model = rep(c(&#39;probit&#39;, &#39;lm&#39;, &#39;both&#39;), c(4, 4, 1)), par = names(coef(selection_ml)), sampselpack_ml = coef(selection_ml), unbounded_ml = heckman_ml_unbounded$par, bounded_ml = heckman_ml_bounded$par, explicit_twostep = c( coef(probit), coef(lm_select)[1:3], summary(lm_select)$sigma, coef(lm_select)[&#39;imr&#39;] / summary(lm_select)$sigma ), sampselpack_2step = coef(selection_2step)[-8] ) %&gt;% mutate_if(is.numeric, round, digits = 3) # A tibble: 9 x 7 model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step &lt;chr&gt; &lt;chr&gt; &lt;cf.slctn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 probit (Intercept) -1.520 -1.52 -1.52 -1.52 -1.52 2 probit educ 0.150 0.15 0.15 0.15 0.15 3 probit age 0.010 0.01 0.01 0.01 0.01 4 probit z 0.157 0.158 0.158 0.159 0.159 5 lm (Intercept) 4.478 4.48 4.48 4.52 4.52 6 lm educ 0.080 0.08 0.08 0.078 0.078 7 lm age 0.012 0.012 0.012 0.012 0.012 8 lm sigma 0.458 0.458 0.458 0.451 0.456 9 both rho 0.259 0.257 0.255 0.212 0.21 # compare standard errors tibble( model = rep(c(&#39;probit&#39;, &#39;lm&#39;, &#39;both&#39;), c(4, 4, 1)), par = names(coef(selection_ml)), sampselpack_ml = sqrt(diag(solve( -selection_ml$hessian ))), unbounded_ml = sqrt(diag(solve( heckman_ml_unbounded$hessian ))), bounded_ml = sqrt(diag(solve( heckman_ml_bounded$hessian ))), explicit_twostep = c( summary(probit)$coefficients[, 2], summary(lm_select)$coefficients[-4, 2], NA, NA ), sampselpack_2step = summary(selection_2step)$estimate[-8, 2] ) %&gt;% mutate(across(where(is.numeric), round, digits = 3)) # A tibble: 9 x 7 model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 probit (Intercept) 0.053 0.053 0.053 0.053 0.053 2 probit educ 0.003 0.003 0.003 0.003 0.003 3 probit age 0.001 0.001 0.001 0.001 0.001 4 probit z 0.014 0.014 0.014 0.014 0.014 5 lm (Intercept) 0.09 0.09 0.09 0.106 0.107 6 lm educ 0.004 0.004 0.005 0.005 0.005 7 lm age 0.001 0.001 0.001 0.001 0.001 8 lm sigma 0.008 0.008 0.008 NA NA 9 both rho 0.112 0.112 0.112 NA NA Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/heckman_selection.R "],["marginal-structural.html", "Marginal Structural Model Data Setup Function Estimation Comparison Source", " Marginal Structural Model This is a demonstration of a simple marginal structural model for estimation of so-called ‘causal’ effects using inverse probability weighting. Example data is from, and comparison made to, the ipw package. See more here. Data Setup This example is from the helpfile at ?ipwpoint. library(tidyverse) library(ipw) set.seed(16) n = 1000 simdat = data.frame(l = rnorm(n, 10, 5)) a.lin = simdat$l - 10 pa = plogis(a.lin) simdat = simdat %&gt;% mutate( a = rbinom(n, 1, prob = pa), y = 10 * a + 0.5 * l + rnorm(n, -10, 5) ) ipw_result = ipwpoint( exposure = a, family = &quot;binomial&quot;, link = &quot;logit&quot;, numerator = ~ 1, denominator = ~ l, data = simdat ) summary(ipw_result$ipw.weights) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.4810 0.5127 0.5285 0.9095 0.6318 74.6994 ipwplot(ipw_result$ipw.weights) Create the Weights ps_num = fitted(glm(a ~ 1, data = simdat, family = &#39;binomial&#39;)) ps_num[simdat$a == 0] = 1 - ps_num[simdat$a == 0] ps_den = fitted(glm(a ~ l, data = simdat, family = &#39;binomial&#39;)) ps_den[simdat$a == 0] = 1 - ps_den[simdat$a == 0] wts = ps_num / ps_den Compare the weights. rbind(summary(wts), summary(ipw_result$ipw.weights)) Min. 1st Qu. Median Mean 3rd Qu. Max. [1,] 0.481 0.5127181 0.5284768 0.9094652 0.631794 74.6994 [2,] 0.481 0.5127181 0.5284768 0.9094652 0.631794 74.6994 Add inverse probability weights to the data if desired. simdat = simdat %&gt;% mutate(sw = ipw_result$ipw.weights) Function Create the likelihood function for using the weights. maxlike = function( par, # parameters to be estimated; first is taken to be sigma X, # model matrix y, # target variable wts # estimated weights ) { beta = par[-1] lp = X %*% beta sigma = exp(par[1]) # exponentiated value to stay positive ll = dnorm(y, mean = lp, sd = sigma, log = TRUE) -sum(ll * wts) # weighted likelihood # same as # ll = dnorm(y, mean = lp, sd = sigma)^wts # -sum(log(ll)) } Estimation We want to estimate the marginal structural model for the causal effect of a on y corrected for confounding by l, using inverse probability weighting with robust standard error from the survey package. Create the matrices for estimation, estimate the model, and extract results. X = cbind(1, simdat$a) y = simdat$y result = optim( par = c(sigma = 0, intercept = 0, b = 0), fn = maxlike, X = X, y = y, wts = wts, hessian = TRUE, method = &#39;BFGS&#39;, control = list(abstol = 1e-12) ) dispersion = exp(result$par[1])^2 beta = result$par[-1] Now we compute the standard errors. The following uses the survey package raw version to get the appropriate standard errors, which the ipw approach uses. glm_basic = glm(y ~ a, data = simdat, weights = wts) # to get unscaled cov res = resid(glm_basic, type = &#39;working&#39;) # residuals glm_vcov_unsc = summary(glm_basic)$cov.unscaled # weighted vcov unscaled by dispersion solve(crossprod(qr(X))) estfun = X * res * wts x = estfun %*% glm_vcov_unsc Comparison library(&quot;survey&quot;) msm = svyglm( y ~ a, design = svydesign(~ 1, weights = ~ sw, data = simdat) ) summary(msm) Call: svyglm(formula = y ~ a, design = svydesign(~1, weights = ~sw, data = simdat)) Survey design: svydesign(~1, weights = ~sw, data = simdat) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -4.375 1.142 -3.832 0.000135 *** a 10.647 1.190 8.948 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 29.58889) Number of Fisher Scoring iterations: 2 Now get the standard errors. se = sqrt(diag(crossprod(x) * n/(n-1))) # a robust standard error se_robust = sqrt(diag(sandwich::sandwich(glm_basic))) # an easier way to get it se_msm = sqrt(diag(vcov(msm))) # extract from msm model Compare standard errors. tibble(se, se_robust, se_msm) # A tibble: 2 x 3 se se_robust se_msm &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.14 1.14 1.14 2 1.19 1.19 1.19 Compare general result. tibble( Estimate = beta, init_se = sqrt(diag(solve(result$hessian)))[c(&#39;intercept&#39;, &#39;b&#39;)], # same as scaled se from glm_basic se_robust = se_robust, t = Estimate/se, p = 2*pt(abs(t), df = n - ncol(X), lower.tail = FALSE), dispersion = dispersion ) # A tibble: 2 x 6 Estimate init_se se_robust t p dispersion &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 -4.38 0.247 1.14 -3.83 1.34e- 4 29.6 2 10.6 0.361 1.19 8.95 1.71e-18 29.6 # compare to msm broom::tidy(msm) # A tibble: 2 x 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) -4.38 1.14 -3.83 1.35e- 4 2 a 10.6 1.19 8.95 1.73e-18 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ipw.R "],["tobit.html", "Tobit Demonstrate Censoring with an Upper Limit Demonstrate censoring with a Lower Limit Source", " Tobit A simple demonstration of tobit regression via maximum likelihood. The issue is one where data is censored such that while we observe the value, it is not the true value, which would extend beyond the range of the observed data. This is very commonly seen in cases where the dependent variable has been given some arbitrary cutoff at the lower or upper end of the range, often resulting in floor or ceiling effects respectively. The conceptual idea is that we are interested in modeling the underlying latent variable that would not have such restriction if it was actually observed. Demonstrate Censoring with an Upper Limit Data Setup Data regards academic aptitude (GRE scores) with will be modeled using reading and math test scores, as well as the type of program the student is enrolled in (academic, general, or vocational). See this for an applied example and more detail- https://stats.idre.ucla.edu/r/dae/tobit-models/ library(tidyverse) acad_apt = read_csv(&quot;https://stats.idre.ucla.edu/stat/data/tobit.csv&quot;) %&gt;% mutate(prog = factor(prog, labels = c(&#39;acad&#39;, &#39;general&#39;, &#39;vocational&#39;))) Setup data and initial values. initmod = lm(apt ~ read + math + prog, data = acad_apt) X = model.matrix(initmod) init = c(coef(initmod), log_sigma = log(summary(initmod)$sigma)) Function tobit &lt;- function(par, X, y, ul = -Inf, ll = Inf) { # this function only takes a lower OR upper limit # parameters sigma = exp(par[length(par)]) beta = par[-length(par)] # create indicator depending on chosen limit if (!is.infinite(ll)) { limit = ll indicator = y &gt; ll } else { limit = ul indicator = y &lt; ul } # linear predictor lp = X %*% beta # log likelihood ll = sum(indicator * log((1/sigma)*dnorm((y-lp)/sigma)) ) + sum((1-indicator) * log(pnorm((lp-limit)/sigma, lower=is.infinite(ll)))) -ll } Estimation Estimate via optim. res = optim( par = init, tobit, y = acad_apt$apt, X = X, ul = 800, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) # this would be more akin to the default Stata default approach # optim( # par = init, # tobit, # y = acad_apt$apt, # X = X, # ul = 800, # control = list(maxit = 16000, reltol = 1e-15) # ) Comparison Compare to AER package tobit function. library(survival) aer_mod = AER::tobit( apt ~ read + math + prog, data = acad_apt, left = -Inf, right = 800 ) rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)) ) %&gt;% round(3) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 AER is actually just using survreg from the survival package. Survival models are usually for modeling time to some event, e.g. death in medical studies, and the censoring comes from the fact that the observed event does not occur for some people. Like our tobit function, an indicator is needed to denote who is or isn’t censored. In survival models, the indicator is for the event itself, and means they are NOT censored. So we’ll reverse the indicator used in the tobit function for survreg. surv_mod = survreg(Surv(apt, apt &lt; 800, type = &#39;right&#39;) ~ read + math + prog, data = acad_apt, dist = &#39;gaussian&#39;) Compare all results. rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)), survival = c(coef(surv_mod), surv_mod$scale, logLik(surv_mod)) ) %&gt;% round(3) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 survival 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 Demonstrate censoring with a Lower Limit Create a censored data situation for the low end. The scale itself would be censored for anyone scoring a 200, but that basically doesn’t happen. In this data, 15 are less than a score of 500, so we’ll do that. acad_apt = acad_apt %&gt;% mutate(apt2 = apt, apt2 = if_else(apt2 &lt; 500, 500, apt2)) Estimate and use AER for comparison. res = optim( par = init, tobit, y = acad_apt$apt2, X = X, ll = 400, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) aer_mod = AER::tobit(apt2 ~ read + math + prog, data = acad_apt, left = 400) Comparison rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)) ) %&gt;% round(3) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 270.408 2.328 5.086 -11.331 -38.606 57.024 -1092.483 AER 270.409 2.328 5.085 -11.331 -38.606 57.024 -1092.483 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/tobit.R "],["hurdle.html", "Hurdle Model Poisson Negative Binomial Source", " Hurdle Model Hurdle models are applied to situations in which target data has relatively many of one value, usually zero, to go along with the other observed values. They are two-part models, a logistic model for whether an observation is zero or not, and a count model for the other part. The key distinction from the usual ‘zero-inflated’ count models, is that the count distribution does not contribute to the excess zeros. While the typical application is count data, the approach can be applied to any distribution in theory. Poisson Data Setup Here we import a simple data set. The example comes from the Stata help file for zinb command. One can compare results with hnblogit command in Stata. library(tidyverse) fish = haven::read_dta(&quot;http://www.stata-press.com/data/r11/fish.dta&quot;) Function The likelihood function is of two parts, one a logistic model, the other, a poisson count model. hurdpoisloglik = function(y, X, par) { # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] poispars = par[grep(&#39;pois&#39;, names(par))] # Logit model part Xlogit = X ylogit = ifelse(y == 0, 0, 1) LPlogit = Xlogit %*% logitpars mulogit = plogis(LPlogit) # Calculate the likelihood logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) ) # Poisson part Xpois = X[y &gt; 0, ] ypois = y[y &gt; 0] mupois = exp(Xpois %*% poispars) # Calculate the likelihood loglik0 = -mupois loglikpois = -sum(dpois(ypois, lambda = mupois, log = TRUE)) + sum(log(1 - exp(loglik0))) # combine likelihoods loglik = loglikpois + logliklogit loglik } Get some starting values from glm For these functions, and create a named vector for them. init_mod = glm( count ~ persons + livebait, data = fish, family = poisson, x = TRUE, y = TRUE ) starts = c(logit = coef(init_mod), pois = coef(init_mod)) Estimation Use optim. to estimate parameters. I fiddle with some options to reproduce the hurdle function as much as possible. optPois1 = optim( par = starts, fn = hurdpoisloglik, X = init_mod$x, y = init_mod$y, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # optPois1 Extract the elements from the output to create a summary table. B = optPois1$par se = sqrt(diag(solve(optPois1$hessian))) Z = B/se p = ifelse(Z &gt;= 0, pnorm(Z, lower = FALSE)*2, pnorm(Z)*2) summarytable = round(data.frame(B, se, Z, p), 3) list(summary = summarytable, ll = optPois1$value) $summary B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.761 0.078 logit.livebait 0.711 0.403 1.766 0.077 pois.(Intercept) -2.057 0.341 -6.035 0.000 pois.persons 0.750 0.043 17.378 0.000 pois.livebait 1.851 0.307 6.023 0.000 $ll [1] 882.2514 Comparison Compare to hurdle from pscl package. library(pscl) poismod = hurdle( count ~ persons + livebait, data = fish, zero.dist = &quot;binomial&quot;, dist = &quot;poisson&quot; ) summary(poismod)$coefficients $count Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.0574429 0.34092748 -6.034840 1.591200e-09 persons 0.7496946 0.04314126 17.377670 1.218024e-67 livebait 1.8512952 0.30735592 6.023294 1.709022e-09 $zero Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.4173042 0.4907691 -2.887925 0.003877928 persons 0.2057555 0.1168027 1.761565 0.078142847 livebait 0.7109464 0.4026898 1.765494 0.077480807 summarytable B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.761 0.078 logit.livebait 0.711 0.403 1.766 0.077 pois.(Intercept) -2.057 0.341 -6.035 0.000 pois.persons 0.750 0.043 17.378 0.000 pois.livebait 1.851 0.307 6.023 0.000 Negative Binomial Function The likelihood function. hurdNBloglik = function(y, X, par) { # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] NegBinpars = par[grep(&#39;NegBin&#39;, names(par))] theta = exp(par[grep(&#39;theta&#39;, names(par))]) # Logit model part Xlogit = X ylogit = ifelse(y == 0, 0, 1) LPlogit = Xlogit%*%logitpars mulogit = plogis(LPlogit) # Calculate the likelihood logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) ) #NB part XNB = X[y &gt; 0, ] yNB = y[y &gt; 0] muNB = exp(XNB %*% NegBinpars) # Calculate the likelihood loglik0 = dnbinom(0, mu = muNB, size = theta, log = TRUE) loglik1 = dnbinom(yNB, mu = muNB, size = theta, log = TRUE) loglikNB = -( sum(loglik1) - sum(log(1 - exp(loglik0))) ) # combine likelihoods loglik = loglikNB + logliklogit loglik } Estimation starts = c( logit = coef(init_mod), NegBin = coef(init_mod), theta = 1 ) optNB1 = optim( par = starts, fn = hurdNBloglik, X = init_mod$x, y = init_mod$y, control = list(maxit = 5000, reltol = 1e-12), method = &quot;BFGS&quot;, hessian = TRUE ) # optNB1 B = optNB1$par se = sqrt(diag(solve(optNB1$hessian))) Z = B/se p = ifelse(Z &gt;= 0, pnorm(Z, lower = FALSE)*2, pnorm(Z)*2) summarytable = round(data.frame(B, se, Z, p), 3) list(summary = summarytable, ll = optNB1$value) $summary B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.762 0.078 logit.livebait 0.711 0.403 1.765 0.077 NegBin.(Intercept) -3.461 0.869 -3.984 0.000 NegBin.persons 0.941 0.153 6.154 0.000 NegBin.livebait 1.985 0.639 3.109 0.002 theta -1.301 0.576 -2.257 0.024 $ll [1] 439.3686 Comparison NBmod = hurdle( count ~ persons + livebait, data = fish, zero.dist = &quot;binomial&quot;, dist = &quot;negbin&quot; ) summary(NBmod)$coefficients $count Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.4607351 0.8685859 -3.984333 6.767006e-05 persons 0.9406299 0.1528503 6.153930 7.558587e-10 livebait 1.9851725 0.6385774 3.108742 1.878857e-03 Log(theta) -1.3008799 0.5762777 -2.257384 2.398411e-02 $zero Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.4173042 0.4907691 -2.887925 0.003877928 persons 0.2057555 0.1168027 1.761565 0.078142847 livebait 0.7109464 0.4026898 1.765494 0.077480807 summarytable B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.762 0.078 logit.livebait 0.711 0.403 1.765 0.077 NegBin.(Intercept) -3.461 0.869 -3.984 0.000 NegBin.persons 0.941 0.153 6.154 0.000 NegBin.livebait 1.985 0.639 3.109 0.002 theta -1.301 0.576 -2.257 0.024 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hurdle.R "],["zi.html", "Zero-Inflated Model Poisson Negative Binomial Supplemental Example Source", " Zero-Inflated Model Log likelihood function to estimate parameters for a Zero-inflated Poisson model. With examples and comparison to pscl package output. Also includes approach based on Hilbe GLM text. Poisson Data Setup Get the data. library(tidyverse) fish = haven::read_dta(&quot;http://www.stata-press.com/data/r11/fish.dta&quot;) Function The log likelihood function. ZIP = function(y, X, par) { # arguments are response y, predictor matrix X, and parameter named starting points of &#39;logit&#39; and &#39;pois&#39; # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] poispars = par[grep(&#39;pois&#39;, names(par))] # Logit part; in this function Xlogit = Xpois but one could split X argument into Xlogi and Xpois for example Xlogit = X LPlogit = Xlogit %*% logitpars logi0 = plogis(LPlogit) # alternative 1/(1+exp(-LPlogit)) # Poisson part Xpois = X mupois = exp(Xpois %*% poispars) # LLs logliklogit = log( logi0 + exp(log(1 - logi0) - mupois) ) loglikpois = log(1 - logi0) + dpois(y, lambda = mupois, log = TRUE) # Hilbe formulation # logliklogit = log(logi0 + (1 - logi0)*exp(- mupois) ) # loglikpois = log(1-logi0) -mupois + log(mupois)*y #not necessary: - log(gamma(y+1)) y0 = y == 0 # 0 values yc = y &gt; 0 # Count part loglik = sum(logliklogit[y0]) + sum(loglikpois[yc]) -loglik } Estimation Get starting values or simply do zeros. # for zip: need &#39;logit&#39;, &#39;pois&#39; initial_model = glm( count ~ persons + livebait, data = fish, x = TRUE, y = TRUE, &quot;poisson&quot; ) # starts = c(logit = coef(initial_model), pois = coef(initial_model)) starts = c(rep(0, 3), rep(0, 3)) names(starts) = c(paste0(&#39;pois.&#39;, names(coef(initial_model))), paste0(&#39;logit.&#39;, names(coef(initial_model)))) Estimate with optim. optPois1 = optim( par = starts , fn = ZIP, X = initial_model$x, y = initial_model$y, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # optPois1 Comparison Extract for clean display. B = optPois1$par se = sqrt(diag(solve((optPois1$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 Results from pscl. library(pscl) zipoismod = zeroinfl(count ~ persons + livebait, data = fish, dist = &quot;poisson&quot;) Compare. summary(zipoismod)$coefficients $count Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.0059968 0.32375278 -6.196076 5.788823e-10 persons 0.7470095 0.04264632 17.516387 1.074350e-68 livebait 1.8093790 0.29207011 6.195016 5.827908e-10 $zero Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.30253716 0.6741897 0.44874186 0.6536179 persons -0.06911031 0.1287075 -0.53695647 0.5912977 livebait -0.03103393 0.5577545 -0.05564084 0.9556279 round(data.frame(B, se, Z, p), 4) B se Z p pois.(Intercept) -2.0060 0.3238 -6.1960 0.0000 pois.persons 0.7470 0.0426 17.5163 0.0000 pois.livebait 1.8094 0.2921 6.1950 0.0000 logit.(Intercept) 0.3026 0.6742 0.4488 0.6536 logit.persons -0.0691 0.1287 -0.5370 0.5913 logit.livebait -0.0310 0.5578 -0.0556 0.9556 Negative Binomial Function ZINB = function(y, X, par) { # arguments are response y, predictor matrix X, and parameter named starting points of &#39;logit&#39;, &#39;negbin&#39;, and &#39;theta&#39; # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] negbinpars = par[grep(&#39;negbin&#39;, names(par))] theta = exp(par[grep(&#39;theta&#39;, names(par))]) # Logit part; in this function Xlogit = Xnegbin but one could split X argument into Xlogit and Xnegbin for example Xlogit = X LPlogit = Xlogit %*% logitpars logi0 = plogis(LPlogit) # Negbin part Xnegbin = X munb = exp(Xnegbin %*% negbinpars) # LLs logliklogit = log( logi0 + exp(log(1 - logi0) + suppressWarnings(dnbinom(0, size = theta, mu = munb, log = TRUE))) ) logliknegbin = log(1 - logi0) + suppressWarnings(dnbinom(y, size = theta, mu = munb, log = TRUE)) # Hilbe formulation # theta part # alpha = 1/theta # m = 1/alpha # p = 1/(1 + alpha*munb) # logliklogit = log( logi0 + (1 - logi0)*(p^m) ) # logliknegbin = log(1-logi0) + log(gamma(m+y)) - log(gamma(m)) + m*log(p) + y*log(1-p) # gamma(y+1) not needed y0 = y == 0 # 0 values yc = y &gt; 0 # Count part loglik = sum(logliklogit[y0]) + sum(logliknegbin[yc]) -loglik } Estimation Get starting values or simply do zeros. # for zinb: &#39;logit&#39;, &#39;negbin&#39;, &#39;theta&#39; initial_model = model.matrix(count ~ persons + livebait, data = fish) # to get X matrix startlogi = glm(count == 0 ~ persons + livebait, data = fish, family = &quot;binomial&quot;) startcount = glm(count ~ persons + livebait, data = fish, family = &quot;poisson&quot;) starts = c( negbin = coef(startcount), logit = coef(startlogi), theta = 1 ) # starts = c(negbin = rep(0, 3), # logit = rep(0, 3), # theta = log(1)) Estimate with optim. optNB1 = optim( par = starts , fn = ZINB, X = initial_model, y = fish$count, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # optNB1 Comparison Extract for clean display. B = optNB1$par se = sqrt(diag(solve((optNB1$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 Results from pscl. # pscl results library(pscl) zinbmod1 = zeroinfl(count ~ persons + livebait, data = fish, dist = &quot;negbin&quot;) Compare. summary(zinbmod1) Call: zeroinfl(formula = count ~ persons + livebait, data = fish, dist = &quot;negbin&quot;) Pearson residuals: Min 1Q Median 3Q Max -0.51947 -0.51472 -0.48259 0.07548 8.92954 Count model coefficients (negbin with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.8031 0.5577 -5.026 5.01e-07 *** persons 0.8492 0.1243 6.833 8.29e-12 *** livebait 1.7907 0.5110 3.504 0.000458 *** Log(theta) -0.9688 0.3022 -3.206 0.001348 ** Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -4.2762 4.2780 -1.000 0.318 persons 0.5603 0.5171 1.084 0.279 livebait 1.1683 3.6612 0.319 0.750 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Theta = 0.3795 Number of iterations in BFGS optimization: 27 Log-likelihood: -440.1 on 7 Df round(data.frame(B, se, Z, p), 4) # note that theta here is actually log(theta) B se Z p negbin.(Intercept) -2.8031 0.5577 -5.0259 0.0000 negbin.persons 0.8492 0.1243 6.8335 0.0000 negbin.livebait 1.7907 0.5110 3.5043 0.0005 logit.(Intercept) -4.2760 4.2777 -0.9996 0.3175 logit.persons 0.5603 0.5171 1.0836 0.2785 logit.livebait 1.1683 3.6609 0.3191 0.7496 theta -0.9688 0.3022 -3.2056 0.0013 Supplemental Example data(&quot;bioChemists&quot;, package = &quot;pscl&quot;) initial_model = model.matrix(art ~ fem + mar + kid5 + phd + ment, data = bioChemists) # to get X matrix startlogi = glm(art==0 ~ fem + mar + kid5 + phd + ment, data = bioChemists, family = &quot;binomial&quot;) startcount = glm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists, family = &quot;quasipoisson&quot;) starts = c( negbin = coef(startcount), logit = coef(startlogi), theta = summary(startcount)$dispersion ) # starts = c(negbin = rep(0, 6), # logit = rep(0, 6), # theta = 1) optNB2 = optim( par = starts , fn = ZINB, X = initial_model, y = bioChemists$art, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # optNB2 B = optNB2$par se = sqrt(diag(solve((optNB2$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 library(pscl) zinbmod = zeroinfl(art ~ . | ., data = bioChemists, dist = &quot;negbin&quot;) summary(zinbmod)$coefficients $count Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.4167465901 0.143596450 2.90220678 3.705439e-03 femWomen -0.1955076374 0.075592558 -2.58633447 9.700275e-03 marMarried 0.0975826042 0.084451953 1.15548073 2.478936e-01 kid5 -0.1517320709 0.054206071 -2.79917119 5.123397e-03 phd -0.0006997593 0.036269674 -0.01929323 9.846072e-01 ment 0.0247861500 0.003492672 7.09661548 1.278491e-12 Log(theta) 0.9763577454 0.135469554 7.20721163 5.710921e-13 $zero Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.19160645 1.3227962 -0.1448496 0.884829645 femWomen 0.63587048 0.8488959 0.7490559 0.453823498 marMarried -1.49943716 0.9386562 -1.5974296 0.110169987 kid5 0.62840922 0.4427746 1.4192531 0.155825245 phd -0.03773288 0.3080059 -0.1225070 0.902497523 ment -0.88227364 0.3162186 -2.7900755 0.005269575 round(data.frame(B,se, Z, p), 4) B se Z p negbin.(Intercept) 0.4167 0.1436 2.9021 0.0037 negbin.femWomen -0.1955 0.0756 -2.5863 0.0097 negbin.marMarried 0.0976 0.0845 1.1555 0.2479 negbin.kid5 -0.1517 0.0542 -2.7992 0.0051 negbin.phd -0.0007 0.0363 -0.0195 0.9845 negbin.ment 0.0248 0.0035 7.0967 0.0000 logit.(Intercept) -0.1916 1.3229 -0.1449 0.8848 logit.femWomen 0.6359 0.8490 0.7491 0.4538 logit.marMarried -1.4995 0.9387 -1.5974 0.1102 logit.kid5 0.6284 0.4428 1.4192 0.1558 logit.phd -0.0377 0.3080 -0.1225 0.9025 logit.ment -0.8823 0.3162 -2.7901 0.0053 theta 0.9763 0.1355 7.2071 0.0000 Source Original code for ZIP found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/poiszeroinfl.R Original code for ZINB found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/NBzeroinfl.R "],["cox.html", "Cox Survival Standard Proportional Hazards Time-varying coefficients Stratified Cox Model Source", " Cox Survival Some simple demonstrations of a standard Cox, Cox with time-varying covariates and a stratified Cox. Standard Proportional Hazards Data Setup set.seed(12) dur = 1:10 kittyblarg = rnorm(10) # something happened to kitty! kittyhappy = rep(0:1, times = 5) # is kitty happy? kittydied = sample(0:1, 10, replace = TRUE) # kitty died! oh no! d = data.frame(kittyblarg, kittyhappy, dur, kittydied) # Inspect d kittyblarg kittyhappy dur kittydied 1 -1.4805676 0 1 0 2 1.5771695 1 2 1 3 -0.9567445 0 3 0 4 -0.9200052 1 4 1 5 -1.9976421 0 5 1 6 -0.2722960 1 6 1 7 -0.3153487 0 7 0 8 -0.6282552 1 8 1 9 -0.1064639 0 9 0 10 0.4280148 1 10 1 Function Create a the likelihood function to feed to optim. pl &lt;- function(pars, preds, died, t) { # Arguments- # pars: coefficients of interest # preds: predictor matrix # died: death # t: time b = pars X = as.matrix(preds[order(t), ]) died2 = died[order(t)] LP = X%*%b # Linear predictor # initialize log likelihood due to looping, not necessary ll = numeric(nrow(X)) rows = 1:nrow(preds) for (i in rows){ riskset = ifelse(rows &lt; i, FALSE, TRUE) # identify risk set ll[i] = died2[i]*(LP[i] - log(sum(exp(LP[riskset]))) ) # log likelihood } -sum(ll) } Estimation Estimate with optim. initial_values = c(0, 0) out = optim( par = initial_values, fn = pl, preds = d[, c(&#39;kittyblarg&#39;, &#39;kittyhappy&#39;)], died = d[, &#39;kittydied&#39;], t = dur, method = &quot;BFGS&quot;, hessian = T ) out $par [1] -0.5827125 1.3803731 $value [1] 7.783878 $counts function gradient 14 5 $convergence [1] 0 $message NULL $hessian [,1] [,2] [1,] 1.9913282 0.4735968 [2,] 0.4735968 0.7780126 Comparison Extract results. B = out$par se = sqrt(diag(solve(out$hessian))) Z = B/se # create a summary table result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE)*2, pnorm(Z, lower = TRUE)*2) ) Compare to survival package. library(survival) cox_model = coxph(Surv(dur, kittydied) ~ kittyblarg + kittyhappy) summary(cox_model)$coefficients coef exp(coef) se(coef) z Pr(&gt;|z|) kittyblarg -0.5827128 0.5583816 0.7662808 -0.7604429 0.4469899 kittyhappy 1.3803735 3.9763866 1.2259311 1.1259797 0.2601741 result_tbl B exp se Z p 1 -0.5827125 0.5583817 0.7662807 -0.7604425 0.4469901 2 1.3803731 3.9763850 1.2259310 1.1259795 0.2601742 Time-varying coefficients Note that technically nothing new is going on here relative to the previous model. See the vignette for the survival package for further details. Data Setup In the following we’ll first create some noisy time points. set.seed(123) t1 = rep(NA, 20) t2 = rep(NA, 20) t1[seq(1, 20, by = 2)] = 1:10 t2[seq(1, 20, by = 2)] = t1[seq(1, 20, by = 2)] + sample(1:5, 10, replace = TRUE) + abs(rnorm(10)) t1[seq(2, 20, by = 2)] = t2[seq(1, 20, by = 2)] t2[seq(2, 20, by = 2)] = t1[seq(2, 20, by = 2)] + sample(1:5) + abs(rnorm(10)) kitty = rep(1:10, e = 2) kittyblarg = t2 + rnorm(20, sd = 5) kittyhappy = rep(0:1, times = 5, e = 2) die = 0:1 cens = c(0, 0) kittydied = ifelse(runif(20)&gt;=.5, die, cens) d = data.frame(kitty, kittyblarg, kittyhappy, t1, t2, kittydied) # Inspect the Surv object if desired # Surv(t1,t2, kittydied) # Inspect the data d kitty kittyblarg kittyhappy t1 t2 kittydied 1 1 -1.870759 0 1.000000 4.686853 0 2 1 4.904677 0 4.686853 7.902718 0 3 2 4.798609 1 2.000000 5.445662 0 4 2 15.214255 1 5.445662 10.780575 0 5 3 5.467102 0 3.000000 6.224082 0 6 3 9.958737 0 6.224082 8.309781 1 7 4 -9.776800 1 4.000000 6.359814 0 8 4 4.586278 1 6.359814 8.445237 0 9 5 9.833514 0 5.000000 8.400771 0 10 5 7.368822 0 8.400771 13.471382 1 11 6 13.283435 1 6.000000 11.110683 0 12 6 18.256961 1 11.110683 14.256076 0 13 7 10.736186 0 7.000000 11.555841 0 14 7 23.935980 0 11.555841 17.721386 1 15 8 6.114988 1 8.000000 10.786913 0 16 8 14.573972 1 10.786913 12.605429 1 17 9 13.516008 0 9.000000 11.497850 0 18 9 9.750603 0 11.497850 14.182787 1 19 10 8.371929 1 10.000000 14.966617 0 20 10 19.430893 1 14.966617 19.286674 1 Function pl_tv &lt;- function(pars, preds, died, t1, t2, data) { # Same arguments as before though will take a data object # plus variable names via string input. Also requires beginning # and end time point (t1, t2) dat = data[,c(preds, died, t1, t2)] dat = dat[order(dat$t2), ] b = pars X = as.matrix(dat[, preds]) died2 = dat[, died] # linear predictor LP = X%*%b # log likelihood ll = numeric(nrow(X)) rows = 1:nrow(dat) for (i in rows){ st_i = dat$t2[i] # if they have already died/censored (row &lt; i) or if the initial time is # greater than current end time (t1 &gt; st_i), they are not in the risk set, # else they are. riskset = ifelse(rows &lt; i | dat$t1 &gt; st_i, FALSE, TRUE) ll[i] = died2[i]*(LP[i] - log(sum(exp(LP[riskset]))) ) } -sum(ll) } Estimation Estimate with optim. initial_values = c(0, 0) out = optim( par = initial_values, fn = pl_tv, preds = c(&#39;kittyblarg&#39;, &#39;kittyhappy&#39;), died = &#39;kittydied&#39;, data = d, t1 = &#39;t1&#39;, t2 = &#39;t2&#39;, method = &quot;BFGS&quot;, hessian = TRUE ) # out Comparison Extract results. B = out$par se = sqrt(diag(solve(out$hessian))) Z = B/se result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE) * 2, pnorm(Z, lower = TRUE) * 2) ) Compare to survival package. cox_model_tv = coxph( Surv(t1, t2, kittydied) ~ kittyblarg + kittyhappy, method = &#39;breslow&#39;, control = coxph.control(iter.max = 1000) ) # cox_model_tv; cox_model_tv$loglik[2] summary(cox_model_tv)$coefficients coef exp(coef) se(coef) z Pr(&gt;|z|) kittyblarg -0.09159497 0.9124747 0.1074014 -0.8528288 0.3937542 kittyhappy -1.51322382 0.2201990 1.1368707 -1.3310430 0.1831748 result_tbl B exp se Z p 1 -0.09159224 0.9124771 0.1073995 -0.8528183 0.3937601 2 -1.51323240 0.2201971 1.1368743 -1.3310464 0.1831737 Stratified Cox Model Data Setup data(ovarian, package = &#39;survival&#39;) Function Requires pl function above though one could extend to pl_tv. pl_strat &lt;- function(pars, preds, died, t, strata) { strat = as.factor(strata) d = data.frame(preds, died, t, strat) dlist = split(d, strata) neglls = map_dbl( dlist, function(x) pl( pars = pars, preds = x[, colnames(preds)], died = x$died, t = x$t ) ) sum(neglls) } Estimation Estimate with optim. initial_values = c(0, 0) out = optim( par = initial_values, fn = pl_strat, preds = ovarian[, c(&#39;age&#39;, &#39;ecog.ps&#39;)], died = ovarian$fustat, t = ovarian$futime, strata = ovarian$rx, method = &quot;BFGS&quot;, hessian = TRUE ) # out Comparison B = out$par se = sqrt(diag(solve(out$hessian))) Z = B/se results_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE) * 2, pnorm(Z, lower = TRUE)*2) ) cox_strata_model = coxph( Surv(futime, fustat) ~ age + ecog.ps + strata(rx), data = ovarian ) # cox_strata_model; cox_strata_model$loglik[2] summary(cox_model_tv)$coefficients coef exp(coef) se(coef) z Pr(&gt;|z|) kittyblarg -0.09159497 0.9124747 0.1074014 -0.8528288 0.3937542 kittyhappy -1.51322382 0.2201990 1.1368707 -1.3310430 0.1831748 result_tbl B exp se Z p 1 -0.09159224 0.9124771 0.1073995 -0.8528183 0.3937601 2 -1.51323240 0.2201971 1.1368743 -1.3310464 0.1831737 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/survivalCox.R "],["naive-bayes.html", "Naive Bayes Initialization Comparison Source", " Naive Bayes Initialization Demo for binary data. First we generate some data. We have several binary covariates and a binary target variable y. library(tidyverse) set.seed(123) x = matrix(sample(0:1, 50, replace = TRUE), ncol = 5) xf = map(data.frame(x), factor) y = sample(0:1, 10, prob = c(.25, .75), replace = TRUE) Comparison We can use e1071 for comparison. library(e1071) m = naiveBayes(xf, y) m Naive Bayes Classifier for Discrete Predictors Call: naiveBayes.default(x = xf, y = y) A-priori probabilities: y 0 1 0.3 0.7 Conditional probabilities: X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Using base R for our model, we can easily obtain the ‘predictions’… map(xf, function(var) t(prop.table(table(&#39; &#39; = var, y), margin = 2))) $X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 $X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 $X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/naivebayes.R "],["multinomial.html", "Multinomial Standard (Categorical) Model Alternative specific and constant variables Source", " Multinomial For more detail on these types of models, see my document at https://m-clark.github.io/docs/logregmodels.html. Standard (Categorical) Model Data Setup First, lets get some data. library(haven) library(tidyverse) library(mlogit) program = read_dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;) %&gt;% as_factor() %&gt;% mutate(prog = relevel(prog, ref = &quot;academic&quot;)) head(program[, 1:5]) # A tibble: 6 x 5 id female ses schtyp prog &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 45 female low public vocation 2 108 male middle public general 3 15 male high public vocation 4 67 male low public vocation 5 153 male middle public vocation 6 51 female high public general # convert to long form for mlogit programLong = program %&gt;% select(id, prog, ses, write) %&gt;% mlogit.data( data = , shape = &#39;wide&#39;, choice = &#39;prog&#39;, id.var = &#39;id&#39; ) head(programLong) ~~~~~~~ first 10 observations out of 600 ~~~~~~~ id prog ses write chid alt idx 1 1 FALSE low 44 11 academic 11:emic 2 1 FALSE low 44 11 general 11:eral 3 1 TRUE low 44 11 vocation 11:tion 4 2 FALSE middle 41 9 academic 9:emic 5 2 FALSE middle 41 9 general 9:eral 6 2 TRUE middle 41 9 vocation 9:tion 7 3 TRUE low 65 159 academic 159:emic 8 3 FALSE low 65 159 general 159:eral 9 3 FALSE low 65 159 vocation 159:tion 10 4 TRUE low 50 30 academic 30:emic ~~~ indexes ~~~~ chid id alt 1 11 1 academic 2 11 1 general 3 11 1 vocation 4 9 2 academic 5 9 2 general 6 9 2 vocation 7 159 3 academic 8 159 3 general 9 159 3 vocation 10 30 4 academic indexes: 1, 1, 2 We go ahead and run a model via mlogit for later comparison. mlogit_mod = mlogit(prog ~ 1| write + ses, data = programLong) mlogit_coefs = coef(mlogit_mod)[c(1,5,7,3,2,6,8,4)] # reorder Function Multinomial model via maximum likelihood multinomregML &lt;- function(par, X, y) { levs = levels(y) ref = levs[1] # reference level (category label 1) y0 = y == ref y1 = y == levs[2] # category 2 y2 = y == levs[3] # category 3 beta = matrix(par, ncol = 2) # more like mnlogit package depiction in its function # V1 = X %*% beta[ ,1] # V2 = X %*% beta[ ,2] # ll = sum(-log(1 + exp(V1) + exp(V2))) + sum(V1[y1], V2[y2]) V = X %*% beta # a vectorized approach baseProbVec = 1 / (1 + rowSums(exp(V))) # reference group probabilities loglik = sum(log(baseProbVec)) + crossprod(c(V), c(y1, y2)) loglik } out = optim( runif(8,-.1, .1), multinomregML, X = model.matrix(prog ~ ses + write, data = program), y = program$prog, control = list( maxit = 1000, reltol = 1e-12, ndeps = rep(1e-8, 8), trace = TRUE, fnscale = -1, type = 3 ), method = &#39;BFGS&#39; ) initial value 638.963532 iter 10 value 180.008322 final value 179.981726 converged # out$par Comparison An initial comparison. cbind(out$par, mlogit_coefs) %&gt;% round(4) mlogit_coefs (Intercept):general 2.8522 2.8522 sesmiddle:general -0.5333 -0.5333 seshigh:general -1.1628 -1.1628 write:general -0.0579 -0.0579 (Intercept):vocation 5.2182 5.2182 sesmiddle:vocation 0.2914 0.2914 seshigh:vocation -0.9827 -0.9827 write:vocation -0.1136 -0.1136 The following use dmultinom for the likelihood, similar to other modeling demonstrations in this document. X = model.matrix(prog ~ ses + write, data = program) y = program$prog pars = matrix(out$par, ncol = 2) V = X %*% pars acadprob = 1 / (1+rowSums(exp(V))) fitnonacad = exp(V) * matrix(rep(acadprob, 2), ncol = 2) fits = cbind(acadprob, fitnonacad) yind = model.matrix( ~ -1 + prog, data = program) # because dmultinom can&#39;t take matrix for prob ll = 0 for (i in 1:200){ ll = ll + dmultinom(yind[i, ], size = 1, prob = fits[i, ], log = TRUE) } ll [1] -179.9817 out$value [1] -179.9817 logLik(mlogit_mod) &#39;log Lik.&#39; -179.9817 (df=8) Alternative specific and constant variables Now we add alternative specific and alternative constant variables to the previous individual specific covariates.. In this example, price is alternative invariant (Z) income is individual/alternative specific (X), and catch is alternative specific (Y). We can use the fish data from the mnlogit package. library(mnlogit) data(Fish) head(Fish) mode income alt price catch chid 1.beach FALSE 7083.332 beach 157.930 0.0678 1 1.boat FALSE 7083.332 boat 157.930 0.2601 1 1.charter TRUE 7083.332 charter 182.930 0.5391 1 1.pier FALSE 7083.332 pier 157.930 0.0503 1 2.beach FALSE 1250.000 beach 15.114 0.1049 2 2.boat FALSE 1250.000 boat 10.534 0.1574 2 fm = formula(mode ~ price | income | catch) fit = mnlogit(fm, Fish) # fit = mlogit(fm, Fish) summary(fit) Call: mnlogit(formula = fm, data = Fish) Frequencies of alternatives in input data: beach boat charter pier 0.11337 0.35364 0.38240 0.15059 Number of observations in data = 1182 Number of alternatives = 4 Intercept turned: ON Number of parameters in model = 11 # individual specific variables = 2 # choice specific coeff variables = 1 # individual independent variables = 1 ------------------------------------------------------------- Maximum likelihood estimation using the Newton-Raphson method ------------------------------------------------------------- Number of iterations: 7 Number of linesearch iterations: 10 At termination: Gradient norm = 2.09e-06 Diff between last 2 loglik values = 0 Stopping reason: Succesive loglik difference &lt; ftol (1e-06). Total estimation time (sec): 0.017 Time for Hessian calculations (sec): 0.005 using 1 processors. Coefficients : Estimate Std.Error t-value Pr(&gt;|t|) (Intercept):boat 8.4184e-01 2.9996e-01 2.8065 0.0050080 ** (Intercept):charter 2.1549e+00 2.9746e-01 7.2443 4.348e-13 *** (Intercept):pier 1.0430e+00 2.9535e-01 3.5315 0.0004132 *** income:boat 5.5428e-05 5.2130e-05 1.0633 0.2876611 income:charter -7.2337e-05 5.2557e-05 -1.3764 0.1687088 income:pier -1.3550e-04 5.1172e-05 -2.6480 0.0080977 ** catch:beach 3.1177e+00 7.1305e-01 4.3724 1.229e-05 *** catch:boat 2.5425e+00 5.2274e-01 4.8638 1.152e-06 *** catch:charter 7.5949e-01 1.5420e-01 4.9254 8.417e-07 *** catch:pier 2.8512e+00 7.7464e-01 3.6807 0.0002326 *** price -2.5281e-02 1.7551e-03 -14.4046 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Log-Likelihood: -1199.1, df = 11 AIC: 2420.3 The likelihood function. multinomregML2 &lt;- function(par, X, Y, Z, respVec, choice) { # X dim nrow(Fish)/K x p + 1 (intercept) # Z, Y nrow(N); Y has alt specific coefs; then for Z ref group dropped so nrow = nrow*(K-1)/K # for ll everything through previous X the same # then calc probmat for Y and Z, add to X probmat, and add to base N = sum(choice) K = length(unique(respVec)) levs = levels(respVec) xpar = matrix(par[1:6], ncol = K-1) ypar = matrix(par[7:10], ncol = K) zpar = matrix(par[length(par)], ncol = 1) # Calc X Vx = X %*% xpar # Calc Y (mnlogit finds N x 1 results by going through 1:N, N+1:N*2 etc; then # makes 1 vector, then subtracts the first 1:N from whole vector, then makes # Nxk-1 matrix with N+1:end values (as 1:N are just zero)); creating the # vector and rebuilding the matrix is unnecessary though Vy = sapply(1:K, function(alt) Y[respVec == levs[alt], , drop = FALSE] %*% ypar[alt]) Vy = Vy[,-1] - Vy[,1] # Calc Z Vz = Z %*% zpar Vz = matrix(Vz, ncol = 3) # all Vs must fit into N x K -1 matrix where N is nobs (i.e. individuals) V = Vx + Vy + Vz ll0 = crossprod(c(V), choice[-(1:N)]) baseProbVec &lt;- 1 / (1 + rowSums(exp(V))) loglik = sum(log(baseProbVec)) + ll0 loglik # note fitted values via # fitnonref = exp(V) * matrix(rep(baseProbVec, K-1), ncol = K-1) # fitref = 1-rowSums(fitnonref) # fits = cbind(fitref, fitnonref) } inits = runif(11, -.1, .1) mdat = mnlogit(fm, Fish)$model # this data already ordered! As X has a constant value across alternatives, the coefficients regard the selection of the alternative relative to reference. X = cbind(1, mdat[mdat$`_Alt_Indx_` == &#39;beach&#39;, &#39;income&#39;]) dim(X) [1] 1182 2 head(X) [,1] [,2] [1,] 1 7083.332 [2,] 1 1250.000 [3,] 1 3750.000 [4,] 1 2083.333 [5,] 1 4583.332 [6,] 1 4583.332 Y will use the complete data to start. Coefficients will be differences from the reference alternative coefficient. Y = as.matrix(mdat[, &#39;catch&#39;, drop = FALSE]) dim(Y) [1] 4728 1 Z are difference scores from reference group. Z = as.matrix(mdat[mdat$`_Alt_Indx_` != &#39;beach&#39;, &#39;price&#39;, drop = FALSE]) Z = Z - mdat[mdat$`_Alt_Indx_` == &#39;beach&#39;, &#39;price&#39;] dim(Z) [1] 3546 1 respVec = mdat$`_Alt_Indx_` # first 10 should be 0 0 1 0 1 0 0 0 1 1 after beach dropped multinomregML2(inits, X, Y, Z, respVec, choice = mdat$mode) [,1] [1,] -162384.5 out = optim( par = rep(0, 11), multinomregML2, X = X, Y = Y, Z = Z, respVec = respVec, choice = mdat$mode, control = list( maxit = 1000, reltol = 1e-12, ndeps = rep(1e-8, 11), trace = TRUE, fnscale = -1, type = 3 ), method = &#39;BFGS&#39; ) initial value 1638.599935 iter 10 value 1253.603448 iter 20 value 1199.143447 final value 1199.143445 converged Comparison Compare fits. # out # round(out$par, 3) round(cbind(out$par, coef(fit)), 3) [,1] [,2] (Intercept):boat 0.842 0.842 income:boat 0.000 0.000 (Intercept):charter 2.155 2.155 income:charter 0.000 0.000 (Intercept):pier 1.043 1.043 income:pier 0.000 0.000 catch:beach 3.118 3.118 catch:boat 2.542 2.542 catch:charter 0.759 0.759 catch:pier 2.851 2.851 price -0.025 -0.025 cbind(logLik(fit), out$value) [,1] [,2] [1,] -1199.143 -1199.143 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/multinomial.R "],["ordinal.html", "Ordinal Data Function Estimation Comparison Source", " Ordinal The following demonstrates a standard cumulative link ordinal regression model via maximum likelihood. Default is with probit link function. Alternatively you can compare it with a logit link, which will result in values roughly 1.7*parameters estimates from the probit. Data This data generation is from the probit perspective, where the underlying continuous latent variable is normally distributed. library(tidyverse) set.seed(808) N = 1000 # Sample size x = cbind(x1 = rnorm(N), x2 = rnorm(N)) # predictor variables beta = c(1,-1) # coefficients y_star = rnorm(N, mean = x %*% beta) # the underlying latent variable y_1 = y_star &gt; -1.5 # -1.50 first cutpoint y_2 = y_star &gt; .75 # 0.75 second cutpoint y_3 = y_star &gt; 1.75 # 1.75 third cutpoint y = y_1 + y_2 + y_3 + 1 # target table(y) y 1 2 3 4 175 495 182 148 d = data.frame(x, y = factor(y)) Function ll_ord = function(par, X, y, probit = TRUE) { K = length(unique(y)) # number of classes K ncuts = K-1 # number of cutpoints/thresholds cuts = par[(1:ncuts)] # cutpoints beta = par[-(1:ncuts)] # regression coefficients lp = X %*% beta # linear predictor ll = rep(0, length(y)) # log likelihood pfun = ifelse(probit, pnorm, plogis) # which link to use for(k in 1:K){ if (k==1) { ll[y==k] = pfun((cuts[k] - lp[y==k]), log = TRUE) } else if (k &lt; K) { ll[y==k] = log(pfun(cuts[k] - lp[y==k]) - pfun(cuts[k-1] - lp[y==k])) } else { ll[y==k] = log(1 - pfun(cuts[k-1] - lp[y==k])) } } -sum(ll) } Estimation init = c(-1, 1, 2, 0, 0) # initial values result_probit = optim( init, ll_ord, y = y, X = x, probit = TRUE, control = list(reltol = 1e-10) ) result_logit = optim( init, ll_ord, y = y, X = x, probit = FALSE, control = list(reltol = 1e-10) ) Comparison We can compare our results with the ordinal package. library(ordinal) result_ordpack_probit = clm(y ~ x1 + x2, data = d, link = &#39;probit&#39;) result_ordpack_logit = clm(y ~ x1 + x2, data = d, link = &#39;logit&#39;) resprobit = data.frame(method = c(&#39;ll_ord&#39;, &#39;ordpack&#39;), rbind(coef(result_probit), coef(result_ordpack_probit))) colnames(resprobit) = c(&#39;method&#39;, paste0(&#39;cut_&#39;, 1:3), &#39;beta1&#39;, &#39;beta2&#39;) resprobit method cut_1 cut_2 cut_3 beta1 beta2 1 ll_ord -1.608561 0.7311868 1.797667 1.018545 -1.051004 2 ordpack -1.608561 0.7311868 1.797667 1.018545 -1.051004 reslogit = data.frame(method = c(&#39;ll_ord&#39;, &#39;ordpack&#39;), rbind(coef(result_logit), coef(result_ordpack_logit))) colnames(reslogit) = c(&#39;method&#39;, paste0(&#39;cut_&#39;, 1:3), &#39;beta1&#39;, &#39;beta2&#39;) reslogit method cut_1 cut_2 cut_3 beta1 beta2 1 ll_ord -2.871524 1.284054 3.168263 1.806431 -1.876804 2 ordpack -2.871524 1.284054 3.168263 1.806431 -1.876804 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ordinal_regression.R "],["markov.html", "Markov Chain Model Data Setup Function Estimation Comparison Source", " Markov Chain Model Initial demo of markovchain package. Not shown. You may want to play with it to get a feel for how it works. We will use it for comparison later. Do go ahead and load the package library(tidyverse) library(markovchain) A = matrix(c(.7, .3, .9, .1), nrow = 2, byrow = TRUE) dtmcA = new( &#39;markovchain&#39;, transitionMatrix = A, states = c(&#39;a&#39;, &#39;b&#39;), name = &#39;MarkovChain A&#39; ) dtmcA MarkovChain A A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.7 0.3 b 0.9 0.1 plot(dtmcA) transitionProbability(dtmcA, &#39;b&#39;, &#39;b&#39;) [1] 0.1 initialState = c(0, 1) steps = 4 finalState = initialState * dtmcA ^ steps #using power operator finalState a b [1,] 0.7488 0.2512 steadyStates(dtmcA) a b [1,] 0.75 0.25 observed_states = sample(c(&#39;a&#39;, &#39;b&#39;), 50, c(.7, .3), replace = TRUE) createSequenceMatrix(observed_states) a b a 24 11 b 11 3 markovchainFit(observed_states) $estimate MLE Fit A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.6857143 0.3142857 b 0.7857143 0.2142857 $standardError a b a 0.1399708 0.09476071 b 0.2369018 0.12371791 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b a 0.4113764 0.1285581 b 0.3213953 0.0000000 $upperEndpointMatrix a b a 0.9600522 0.5000133 b 1.0000000 0.4567684 $logLikelihood [1] -29.06116 Data Setup Data Functions A recursive function to take a matrix power. mat_power = function(M, N){ if (N == 1) return(M) M %*% mat_power(M, N-1) } A function to create a sequence. createSequence = function(states, len, tmat) { # states: number of states # len: length of sequence # tmat: the transition matrix states_numeric = length(unique(states)) out = numeric(len) out[1] = sample(states_numeric, 1, prob=colMeans(tmat)) # initial state for (i in 2:len){ out[i] = sample(states_numeric, 1, prob = tmat[out[i - 1], ]) } states[out] } # example result.mat = matrix(rep(2, 4), nrow = 2) mat_power(result.mat, 2) [,1] [,2] [1,] 8 8 [2,] 8 8 # transition matrix A = matrix(c(.7, .3, .4, .6), nrow = 2, byrow = TRUE) mat_power(A, 10) [,1] [,2] [1,] 0.5714311 0.4285689 [2,] 0.5714252 0.4285748 Two states Demo Note that a notably long sequence is needed to get close to recovering the true transition matrix. A = matrix(c(.7, .3, .9, .1), nrow = 2, byrow = TRUE) observed_states = createSequence(c(&#39;a&#39;, &#39;b&#39;), 5000, tmat = A) createSequenceMatrix(observed_states) a b a 2637 1124 b 1124 114 prop.table(createSequenceMatrix(observed_states), 1) a b a 0.7011433 0.29885669 b 0.9079160 0.09208401 markovchainFit(observed_states) $estimate MLE Fit A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.7011433 0.29885669 b 0.9079160 0.09208401 $standardError a b a 0.01365374 0.008914148 b 0.02708086 0.008624457 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b a 0.6743825 0.28138528 b 0.8548385 0.07518038 $upperEndpointMatrix a b a 0.7279042 0.3163281 b 0.9609935 0.1089876 $logLikelihood [1] -2674.284 res = markovchainFit(observed_states) # log likelihood sum(createSequenceMatrix(observed_states) * log(res$estimate@transitionMatrix)) [1] -2674.284 Three states demo A = matrix( c(.70, .20, .10, .20, .40, .40, .05, .05, .90), nrow = 3, byrow = TRUE ) observed_states = createSequence(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), 500, tmat = A) createSequenceMatrix(observed_states) a b c a 77 27 12 b 20 28 22 c 18 15 280 prop.table(createSequenceMatrix(observed_states), 1) a b c a 0.66379310 0.23275862 0.1034483 b 0.28571429 0.40000000 0.3142857 c 0.05750799 0.04792332 0.8945687 markovchainFit(observed_states) $estimate MLE Fit A 3 - dimensional discrete Markov Chain defined by the following states: a, b, c The transition matrix (by rows) is defined as follows: a b c a 0.66379310 0.23275862 0.1034483 b 0.28571429 0.40000000 0.3142857 c 0.05750799 0.04792332 0.8945687 $standardError a b c a 0.07564624 0.04479442 0.02986294 b 0.06388766 0.07559289 0.06700594 c 0.01355476 0.01237375 0.05346070 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b c a 0.51552916 0.14496316 0.04491797 b 0.16049675 0.25184062 0.18295646 c 0.03094114 0.02367122 0.78978761 $upperEndpointMatrix a b c a 0.81205705 0.32055408 0.1619786 b 0.41093182 0.54815938 0.4456150 c 0.08407484 0.07217543 0.9993498 $logLikelihood [1] -302.4854 Function Now we create a function to calculate the (negative) log likelihood. markov_ll = function(par, x) { # par should be the c(A) of transition probabilities A nstates = length(unique(x)) # create transition matrix par = matrix(par, ncol=nstates) par = t(apply(par, 1, function(x) x/sum(x))) # create seq matrix seqMat = table(x[-length(x)], x[-1]) # calculate log likelihood ll = sum(seqMat * log(par)) -ll } A = matrix( c(.70, .20, .10, .40, .20, .40, .10, .15, .75), nrow = 3, byrow = TRUE ) observed_states = createSequence(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), 1000, tmat = A) Estimation Note that initial state values will be transformed to rowsum to one, so the specific initial values don’t matter (i.e. they don’t have to be probabilities). With the basic optim approach, sometimes log(0) will occur and produce a warning. Can be ignored, or use LFBGS as demonstrated at the end. initpar = rep(1, 9) result = optim( par = initpar, fn = markov_ll, x = observed_states, method = &#39;BFGS&#39;, control = list(reltol = 1e-12) ) # get estimates on prob scale estmat = matrix(result$par, ncol = 3) estmat = t(apply(estmat, 1, function(x) x / sum(x))) Comparison Compare with markovchain package. compare_result = markovchainFit(observed_states) # compare log likelihood c(-result$value, compare_result$logLikelihood) [1] -844.459 -844.459 # compare estimated transition matrix list( `Estimated via optim` = estmat, `markovchain Package` = compare_result$estimate@transitionMatrix, `Analytical Solution` = prop.table( table(observed_states[-length(observed_states)], observed_states[-1]) , 1) ) %&gt;% map(round, 3) $`Estimated via optim` [,1] [,2] [,3] [1,] 0.698 0.191 0.111 [2,] 0.369 0.235 0.396 [3,] 0.113 0.162 0.725 $`markovchain Package` a b c a 0.698 0.191 0.111 b 0.369 0.235 0.396 c 0.113 0.162 0.725 $`Analytical Solution` a b c a 0.698 0.191 0.111 b 0.369 0.235 0.396 c 0.113 0.162 0.725 Visualize. plot( new( &#39;markovchain&#39;, transitionMatrix = estmat, states = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), name = &#39;Estimated Markov Chain&#39; ) ) If you don’t want warnings due to zeros use constraints (?constrOptim). result = optim( par = initpar, fn = markov_ll, x = observed_states, method = &#39;L-BFGS&#39;, lower = rep(1e-20, length(initpar)), control = list(pgtol = 1e-12) ) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/markov_model.R "],["hmm.html", "Hidden Markov Model Data Setup Function Estimation Supplemental demo Source", " Hidden Markov Model This function duplicates hmm_viterbi.py, which comes from the Viterbi algorithm wikipedia page (at least as it was when I stumbled across it). This first function is just to provide R code that is similar, in case anyone is interested in a more direct comparison, but the original used lists of tuples and thus was very inefficient R-wise, and provided output that wasn’t succinct. The second function takes a vectorized approach and returns a matrix in a much more straightforward fashion. Both will provide the same result as the Python code. See The Markov Model chapter also. Data Setup library(tidyverse) obs = c(&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) # observed state states = c(&#39;Healthy&#39;, &#39;Fever&#39;) # latent states start_p = c(&#39;Healthy&#39; = 0.6, &#39;Fever&#39; = 0.4) # starting probabilities # transition matrix trans_p = list( &#39;Healthy&#39; = c(&#39;Healthy&#39; = 0.7, &#39;Fever&#39; = 0.3), &#39;Fever&#39; = c(&#39;Healthy&#39; = 0.4, &#39;Fever&#39; = 0.6) ) # emission matrix emit_p = list( &#39;Healthy&#39; = c(&#39;normal&#39; = 0.5, &#39;cold&#39; = 0.4, &#39;dizzy&#39; = 0.1), &#39;Fever&#39; = c(&#39;normal&#39; = 0.1, &#39;cold&#39; = 0.3, &#39;dizzy&#39; = 0.6) ) Function This first function takes a python-esque approach in the manner of the original. viterbi &lt;- function(obs, states, start_p, trans_p, emit_p) { V = vector(&#39;list&#39;, length(obs)) for (st in seq_along(states)) { V[[1]][[states[st]]] = list(&quot;prob&quot; = start_p[st] * emit_p[[st]][obs[1]], &quot;prev&quot; = NULL) } for (t in 2:length(obs)) { for (st in seq_along(states)) { max_tr_prob = numeric() for (prev_st in states) { max_tr_prob[prev_st] = V[[t-1]][[prev_st]][[&quot;prob&quot;]] * trans_p[[prev_st]][[st]] } max_tr_prob = max(max_tr_prob) for (prev_st in states) { flag = V[[t-1]][[prev_st]][[&quot;prob&quot;]] * trans_p[[prev_st]][[st]] == max_tr_prob if (flag) { max_prob = max_tr_prob * emit_p[[st]][obs[t]] V[[t]][[states[st]]] = list(&#39;prob&#39; = max_prob, &#39;prev&#39; = prev_st) } } } } # I don&#39;t bother duplicating the text output code of the original df_out = rbind( Healthy = sapply(V, function(x) x$Healthy$prob), Fever = sapply(V, function(x) x$Fever$prob) ) colnames(df_out) = obs print(df_out) m = paste0( &#39;The steps of states are: &#39;, paste(rownames(df_out)[apply(df_out, 2, which.max)], collapse = &#39; &#39;), paste(&#39;\\nHighest probability: &#39;, max(df_out[, ncol(df_out)])) ) message(m) V } This approach is much more R-like. viterbi_2 &lt;- function(obs, states, start_p, trans_mat, emit_mat) { prob_mat = matrix(NA, nrow = length(states), ncol = length(obs)) colnames(prob_mat) = obs rownames(prob_mat) = states prob_mat[,1] = start_p * emit_mat[,1] for (t in 2:length(obs)) { prob_tran = prob_mat[,t-1] * trans_mat max_tr_prob = apply(prob_tran, 2, max) prob_mat[,t] = max_tr_prob * emit_mat[, obs[t]] } print(prob_mat) m = paste0( &#39;The steps of states are: &#39;, paste(states[apply(prob_mat, 2, which.max)], collapse = &#39; &#39;), paste(&#39;\\nHighest probability: &#39;, max(prob_mat[, ncol(prob_mat)])) ) message(m) } Estimation First we demo the initial function. test = viterbi( obs, states, start_p, trans_p, emit_p ) normal cold dizzy Healthy 0.30 0.084 0.00588 Fever 0.04 0.027 0.01512 # test set.seed(123) obs = sample(obs, 6, replace = TRUE) test = viterbi( obs, states, start_p, trans_p, emit_p ) dizzy dizzy dizzy cold dizzy cold Healthy 0.06 0.0096 0.003456 0.00497664 0.0003483648 0.0003224863 Fever 0.24 0.0864 0.031104 0.00559872 0.0020155392 0.0003627971 # test Now the vectorized approach. set.seed(123) obs = c(&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) obs = sample(obs, 6, replace = T) # need matrices now emit_mat = do.call(rbind, emit_p) trans_mat = do.call(rbind, trans_p) viterbi_2( obs, states, start_p, trans_mat, emit_mat ) dizzy dizzy dizzy cold dizzy cold Healthy 0.30 0.021 0.00216 0.0031104 0.000217728 0.0002015539 Fever 0.04 0.054 0.01944 0.0034992 0.001259712 0.0002267482 Supplemental demo This example comes from the hidden markov model wikipedia page. states = c(&#39;Rainy&#39;, &#39;Sunny&#39;) observations = c(&#39;walk&#39;, &#39;shop&#39;, &#39;clean&#39;) start_probability = c(&#39;Rainy&#39; = 0.6, &#39;Sunny&#39; = 0.4) transition_probability = rbind( &#39;Rainy&#39; = c(&#39;Rainy&#39; = 0.7, &#39;Sunny&#39; = 0.3), &#39;Sunny&#39; = c(&#39;Rainy&#39; = 0.4, &#39;Sunny&#39; = 0.6) ) emission_probability = rbind( &#39;Rainy&#39; = c(&#39;walk&#39; = 0.1, &#39;shop&#39; = 0.4, &#39;clean&#39; = 0.5), &#39;Sunny&#39; = c(&#39;walk&#39; = 0.6, &#39;shop&#39; = 0.3, &#39;clean&#39; = 0.1) ) viterbi_2( observations, states, start_probability, transition_probability, emission_probability ) walk shop clean Rainy 0.06 0.0384 0.013440 Sunny 0.24 0.0432 0.002592 Source Original code for R found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hmm_viterbi.R Original code for Python found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hmm_viterbi.py "],["quantile-regression.html", "Quantile Regression Function Source", " Quantile Regression Data Setup We’ll use the quantreg package for comparison, and the classic data set on Belgian household income and food expenditure. Scale income if you want a meaningful ‘centercept’. library(quantreg) data(engel) ### engel$income = scale(engel$income) Function Loss function. qreg = function(par, X, y, tau) { lp = X%*%par res = y - lp loss = ifelse(res &lt; 0 , -(1 - tau)*res, tau*res) sum(loss) } Median estimation Compare optim output with quantreg package. optim( par = c(0, 0), fn = qreg, X = cbind(1, engel$income), y = engel$foodexp, tau = .5 )$par [1] 81.4853550 0.5601706 rq(foodexp ~ income, tau = .5, data = engel) Call: rq(formula = foodexp ~ income, tau = 0.5, data = engel) Coefficients: (Intercept) income 81.4822474 0.5601806 Degrees of freedom: 235 total; 233 residual Other quantiles Now we will add additional quantiles to estimate. # quantiles qs = c(.05, .1, .25, .5, .75, .9, .95) resrq = coef(rq(foodexp ~ income, tau = qs, data = engel)) resoptim = map_df(qs, function(tau) data.frame(t( optim( par = c(0, 0), fn = qreg, X = cbind(1, engel$income), y = engel$foodexp, tau = tau )$par ))) # compare results rbind(resrq, t(resoptim)) %&gt;% round(2) tau= 0.05 tau= 0.10 tau= 0.25 tau= 0.50 tau= 0.75 tau= 0.90 tau= 0.95 (Intercept) 124.88 110.14 95.48 81.48 62.40 67.35 64.10 income 0.34 0.40 0.47 0.56 0.64 0.69 0.71 X1 124.88 110.14 95.48 81.49 62.40 67.33 64.14 X2 0.34 0.40 0.47 0.56 0.64 0.69 0.71 Visualize Let’s visualize the results. engel %&gt;% qplot(data = ., income, foodexp, color = I(scales::alpha(&#39;orange&#39;, .25))) + geom_abline(aes( intercept = X1, slope = X2, color = group ), data = data.frame(resoptim, group = factor(qs))) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/quantile_regression.Rmd "],["cubic-spline.html", "Cubic Spline Model Data Setup Functions Example 1 Example 2 Source", " Cubic Spline Model See Wood (2017) Generalized Additive Models or my document. Data Setup library(tidyverse) size = c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13, 2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98) wear = c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9, 3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7) x = size - min(size) x = x / max(x) d = data.frame(wear, x) Functions Cubic spline function. rk &lt;- function(x, z) { ((z-0.5)^2 - 1/12) * ((x-0.5)^2 - 1/12)/4 - ((abs(x-z)-0.5)^4 - (abs(x-z)-0.5)^2/2 + 7/240) / 24 } Generate the model matrix. splX &lt;- function(x, knots) { q = length(knots) + 2 # number of parameters n = length(x) # number of observations X = matrix(1, n, q) # initialized model matrix X[ ,2] = x # set second column to x X[ ,3:q] = outer(x, knots, FUN = rk) # remaining to cubic spline basis X } splS &lt;- function(knots) { q = length(knots) + 2 S = matrix(0, q, q) # initialize matrix S[3:q, 3:q] = outer(knots, knots, FUN = rk) # fill in non-zero part S } Matrix square root function. Note that there are various packages with their own. matSqrt &lt;- function(S) { d = eigen(S, symmetric = T) rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors) rS } Penalized fitting function. prsFit &lt;- function(y, x, knots, lambda) { q = length(knots) + 2 # dimension of basis n = length(x) # number of observations Xa = rbind(splX(x, knots), matSqrt(splS(knots))*sqrt(lambda)) # augmented model matrix y[(n+1):(n+q)] = 0 # augment the data vector lm(y ~ Xa - 1) # fit and return penalized regression spline } Example 1 We start with an unpenalized approach. knots = 1:4/5 X = splX(x, knots) # generate model matrix mod1 = lm(wear ~ X - 1) # fit model xp = 0:100/100 # x values for prediction Xp = splX(xp, knots) # prediction matrix Visualize ggplot(aes(x = x, y = wear), data = data.frame(x, wear)) + geom_point(color = &quot;#FF5500&quot;) + geom_line(aes(x = xp, y = Xp %*% coef(mod1)), data = data.frame(xp, Xp), color = &quot;#00AAFF&quot;) + labs(x = &#39;Scaled Engine size&#39;, y = &#39;Wear Index&#39;) + theme_minimal() Example 2 Add lambda penalty. knots = 1:7/8 d2 = data.frame(x = xp) for (i in c(.1, .01, .001, .0001, .00001, .000001)){ # fit penalized regression mod2 = prsFit( y = wear, x = x, knots = knots, lambda = i ) # spline choosing lambda Xp = splX(xp, knots) # matrix to map parameters to fitted values at xp LP = Xp %*% coef(mod2) d2[, paste0(&#39;lambda = &#39;, i)] = LP[, 1] } Visualize. d3 = d2 %&gt;% pivot_longer(cols = -x, names_to = &#39;lambda&#39;, values_to = &#39;value&#39;) %&gt;% mutate(lambda = fct_inorder(lambda)) ggplot(d3) + geom_point(aes(x = x, y = wear), col = &#39;#FF5500&#39;, data = d) + geom_line(aes(x = x, y = value), col = &quot;#00AAFF&quot;) + facet_wrap(~lambda) + theme_minimal() Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/cubicsplines.R "],["gaussian-process.html", "Gausian Processes", " Gausian Processes ‘Noise-free’ gaussian process demo. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006. See those sources for more detail. Murphy’s original Matlab code can be found here, though the relevant files are housed alongside this code in my original repo (*.m files). The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which y = sin(x). It will reproduce figure 15.2 in Murphy 2012 and 2.2 in Rasmussen and Williams 2006. Noise-Free Demonstration Data Setup library(tidyverse) l = 1 # for l, sigma_f, see note at covariance function sigma_f = 1 k_eps = 1e-8 # see note at K_starstar n_prior = 5 # number of prior draws n_post_pred = 5 # number of posterior predictive draws Generate noise-less training data. X_train = c(-4, -3, -2, -1, 1) y_train = sin(X_train) n_train = length(X_train) X_test = seq(-5, 5, .2) n_test = length(X_test) Functions The mean function. In this case the mean equals 0. gp_mu = function(x) { map_dbl(x, function(x) x = 0) } The covariance function. Here it is the squared exponential kernel. l is the horizontal scale, sigma_f is the vertical scale. gp_K = function(x, l = 1, sigma_f = 1){ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) } Visualize the prior distribution Data setup. x_prior = seq(-5, 5, .2) y_prior = MASS::mvrnorm( n = n_prior, mu = gp_mu(x_prior), Sigma = gp_K(x_prior, l = l, sigma_f = sigma_f) ) prior_data = data.frame( x = x_prior, y = t(y_prior), sd = apply(y_prior, 2, sd)) %&gt;% pivot_longer(-c(x, sd), names_to = &#39;variable&#39;) g1 = ggplot(aes(x = x, y = value), data = prior_data) + geom_line(aes(group = variable), color = &#39;#FF5500&#39;, alpha = .5) + labs(title = &#39;Prior&#39;) + theme_minimal() g1 Generate the posterior predictive distribution Create K, K*, and K** matrices as defined in the texts. K = gp_K(X_train, l = l, sigma_f = sigma_f) K_ = gp_K(c(X_train, X_test), l = l, sigma_f = sigma_f) # initial matrix K_star = K_[1:n_train, (n_train+1):ncol(K_)] # dim = N x N* tK_star = t(K_star) # dim = N* x N K_starstar = K_[(n_train+1):nrow(K_), (n_train+1):ncol(K_)] + # dim = N* x N* k_eps * diag(n_test) # the k_eps part is for positive definiteness Kinv = solve(K) Calculate posterior mean and covariance. post_mu = gp_mu(X_test) + t(K_star) %*% Kinv %*% (y_train - gp_mu(X_train)) post_K = K_starstar - t(K_star) %*% Kinv %*% K_star s2 = diag(post_K) # R = chol(post_K) # L = t(R) # L is used in alternative formulation below based on gaussSample.m Generate draws from posterior predictive. y_pp = data.frame( t(MASS::mvrnorm(n_post_pred, mu = post_mu, Sigma = post_K)) ) # y_pp = data.frame(replicate(n_post_pred, post_mu + L %*% rnorm(post_mu))) # alternative Visualize the Posterior Predictive Distribution Reshape data for plotting and create the plot. pp_data = data.frame( x = X_test, y = y_pp, se_lower = post_mu - 2 * sqrt(s2), se_upper = post_mu + 2 * sqrt(s2) ) %&gt;% pivot_longer(starts_with(&#39;y&#39;), names_to = &#39;variable&#39;) g2 = ggplot(aes(x = x, y = value), data = pp_data) + geom_ribbon(aes(ymin = se_lower, ymax = se_upper, group = variable), fill = &#39;gray92&#39;) + geom_line(aes(group = variable), color = &#39;#FF5500&#39;) + geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + labs(title = &#39;Posterior Predictive&#39;) + theme_minimal() g2 Plot prior and posterior predictive together library(patchwork) g1 + g2 Noisy Demonstration ‘Noisy’ gaussian process demo. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006. See those sources for more detail. Murphy’s Matlab code can be found here: https://github.com/probml/pmtk3, though the relevant files are housed alongside this code in my original repo (*.m files). The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which y = sin(x) + noise. It will reproduce an example akin to figure 15.3 in Murphy 2012. Data Setup l = 1 # for l, sigma_f, sigma_n, see note at covariance function sigma_f = 1 sigma_n = .25 k_eps = 1e-8 # see note at Kstarstar n_prior = 5 # number of prior draws n_post_pred = 5 # number of posterior predictive draws X_train = 15 * (runif(20) - .5) n_train = length(X_train) # kept sine function for comparison to noise free result y_train = sin(X_train) + rnorm(n = n_train, sd = .1) X_test = seq(-7.5, 7.5, length = 200) n_test = length(X_test) Functions The mean function. In this case the mean equals 0. gp_mu = function(x) { map_dbl(x, function(x) x = 0) } The covariance function. here it is the squared exponential kernel. l is the horizontal scale, sigma_f is the vertical scale, and, unlike the previous function, sigma_n the noise. gp_K = function( x, y = NULL, l = 1, sigma_f = 1, sigma_n = .5 ) { if(!is.null(y)){ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) + sigma_n*diag(length(x)) } else{ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) } } Visualize the prior distribution Data setup. x_prior = seq(-5, 5, .2) y_prior = MASS::mvrnorm( n = n_prior, mu = gp_mu(x_prior), Sigma = gp_K( x_prior, l = l, sigma_f = sigma_f, sigma_n = sigma_n ) ) Plot. prior_data = data.frame( x = x_prior, y = t(y_prior), sd = apply(y_prior, 2, sd)) %&gt;% pivot_longer(-c(x, sd), names_to = &#39;variable&#39;) g1 = ggplot(aes(x = x, y = value), data = prior_data) + geom_line(aes(group = variable), color = &#39;#FF5500&#39;, alpha = .5) + labs(title = &#39;Prior&#39;) + theme_minimal() g1 Generate the posterior predictive distribution Create Ky, K*, and K** matrices as defined in the texts. Ky = gp_K( x = X_train, y = y_train, l = l, sigma_f = sigma_f, sigma_n = sigma_n ) # initial matrix K_ = gp_K( c(X_train, X_test), l = l, sigma_f = sigma_f, sigma_n = sigma_n ) Kstar = K_[1:n_train, (n_train+1):ncol(K_)] # dim = N x N* tKstar = t(Kstar) # dim = N* x N Kstarstar = K_[(n_train+1):nrow(K_), (n_train+1):ncol(K_)] + # dim = N* x N* k_eps*diag(n_test) # the k_eps part is for positive definiteness Kyinv = solve(Ky) Calculate posterior mean and covariance. post_mu = gp_mu(X_test) + tKstar %*% Kyinv %*% (y_train - gp_mu(X_train)) post_K = Kstarstar - tKstar %*% Kyinv %*% Kstar s2 = diag(post_K) # R = chol(post_K) # L = t(R) # L is used in alternative formulation below based on gaussSample.m Generate draws from posterior predictive. y_pp = data.frame(t(MASS::mvrnorm(n_post_pred, mu = post_mu, Sigma = post_K))) # y_pp = data.frame(replicate(n_post_pred, post_mu + L %*% rnorm(post_mu))) # alternative Visualize the Posterior Predictive Distribution Reshape data for plotting and create the plot. pp_data = data.frame( x = X_test, y = y_pp, fmean = post_mu, se_lower = post_mu - 2 * sqrt(s2), se_upper = post_mu + 2 * sqrt(s2) ) %&gt;% pivot_longer(starts_with(&#39;y&#39;), names_to = &#39;variable&#39;) g2 = ggplot(aes(x = x, y = value), data = pp_data) + geom_ribbon(aes(ymin = se_lower, ymax = se_upper, group = variable), fill = &#39;gray92&#39;) + geom_line(aes(group = variable), color = &#39;#FF5500&#39;) + geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + labs(title = &#39;Posterior Predictive&#39;) + theme_minimal() g2 # # reshape data for plotting # gdat = melt(data.frame(x=X_test, y=y_pp, fmean=post_mu, selower=post_mu-2*sqrt(s2), seupper=post_mu+2*sqrt(s2)), # id=c(&#39;x&#39;, &#39;fmean&#39;,&#39;selower&#39;, &#39;seupper&#39;)) # # g2 = ggplot(aes(x = x, y = value), data = gdat) + # geom_ribbon(aes(ymin = selower, ymax = seupper, group = variable), fill = # &#39;gray90&#39;) + # geom_line(aes(group = variable), color = &#39;#FF5500&#39;, alpha = .5) + # geom_line(aes(group = variable, y = fmean), color = &#39;navy&#39;) + # geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + # labs(title = &#39;Posterior Predictive&#39;) + # theme_minimal() library(patchwork) g1 + g2 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoiseFree.R (noise-free) https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoisey.R (noisy) "],["cfa.html", "Confirmatory Factor Analysis Data Setup Functions Estimation Comparison Source", " Confirmatory Factor Analysis This mostly follows Bollen (1989) for maximum likelihood estimation of a confirmatory factor analysis. In the following example we will examine a situation where there are two underlying (correlated) latent variables for 8 observed responses. The code as is will only work with this toy data set. Setup uses the psych and mvtnorm packages, and results are checked against the lavaan package. Data Setup library(tidyverse) library(mvtnorm) library(psych) set.seed(123) # loading matrix lambda = matrix( c(1,.5,.3,.6,0,0,0,0, 0,0,0,0,1,.7,.4,.5), nrow = 2, byrow = TRUE ) # correlation of factors phi = matrix(c(1, .25, .25, 1), nrow = 2, byrow = TRUE) # factors and some noise factors = rmvnorm(1000, mean = rep(0, 2), sigma = phi, &quot;chol&quot;) e = rmvnorm(1000, sigma = diag(8)) # observed responses y = 0 + factors%*%lambda + e # Examine #dim(y) describe(y) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 1000 0.05 1.44 0.05 0.05 1.42 -5.13 4.51 9.63 0.00 0.01 0.05 X2 2 1000 0.00 1.08 -0.01 0.00 1.04 -3.34 3.25 6.59 0.00 -0.06 0.03 X3 3 1000 -0.01 1.04 0.00 -0.01 1.01 -4.40 3.56 7.96 -0.07 0.27 0.03 X4 4 1000 0.00 1.14 -0.03 -0.01 1.13 -3.85 3.98 7.83 0.10 0.16 0.04 X5 5 1000 0.04 1.43 0.10 0.05 1.39 -4.43 5.21 9.63 -0.02 0.07 0.05 X6 6 1000 -0.02 1.22 -0.01 -0.02 1.27 -3.35 4.68 8.03 0.04 -0.10 0.04 X7 7 1000 0.02 1.06 0.01 0.01 1.05 -2.94 3.33 6.28 0.11 -0.09 0.03 X8 8 1000 0.00 1.14 -0.01 0.01 1.12 -3.27 3.47 6.73 -0.05 0.07 0.04 round(cor(y), 3) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [1,] 1.000 0.291 0.233 0.352 0.104 0.078 0.049 0.050 [2,] 0.291 1.000 0.145 0.228 -0.008 0.031 0.054 0.072 [3,] 0.233 0.145 1.000 0.171 0.006 0.066 0.026 0.079 [4,] 0.352 0.228 0.171 1.000 0.014 0.057 0.054 0.079 [5,] 0.104 -0.008 0.006 0.014 1.000 0.417 0.259 0.300 [6,] 0.078 0.031 0.066 0.057 0.417 1.000 0.201 0.273 [7,] 0.049 0.054 0.026 0.054 0.259 0.201 1.000 0.142 [8,] 0.050 0.072 0.079 0.079 0.300 0.273 0.142 1.000 #see the factor structure cor.plot(cor(y)) # example exploratory fa #fa(y, nfactors=2, rotate=&quot;oblimin&quot;) Functions # measurement model, covariance approach cfa_cov &lt;- function (parms, data) { # Arguments- # parms: initial values (named) # data: raw data # Extract parameters by name require(psych) # for tr l1 = c(1, parms[grep(&#39;l1&#39;, names(parms))]) # loadings for factor 1 l2 = c(1, parms[grep(&#39;l2&#39;, names(parms))]) # loadings for factor 2 cov0 = parms[grep(&#39;cov&#39;, names(parms))] # factor covariance, variances # Covariance matrix S = cov(data)*((nrow(data)-1)/nrow(data)) # ML covariance div by N rather than N-1, the multiplier adjusts # loading estimates lambda = cbind( c(l1, rep(0,length(l2))), c(rep(0,length(l1)), l2) ) # disturbances dist_init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist_init) # factor correlation phi_init = matrix(c(cov0[1], cov0[2], cov0[2], cov0[3]), 2, 2) #factor cov/correlation matrix # other calculations and log likelihood sigtheta = lambda%*%phi_init%*%t(lambda) + disturbs # in Bollen p + q (but for the purposes of this just p) = tr(data) pq = dim(data)[2] # a reduced version; Bollen 1989 p.107 # ll = -(log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq) # should be same as Mplus H0 loglike ll = ( (-nrow(data)*pq/2) * log(2*pi) ) - (nrow(data)/2) * ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) ) ll } Correlation approach for standardized results. Lines correspond to those in cfa_cov. cfa_cor &lt;- function (parms, data) { require(psych) l1 = parms[grep(&#39;l1&#39;, names(parms))] # loadings for factor 1 l2 = parms[grep(&#39;l2&#39;, names(parms))] # loadings for factor 2 cor0 = parms[grep(&#39;cor&#39;, names(parms))] # factor correlation S = cor(data) lambda = cbind( c(l1, rep(0,length(l2))), c(rep(0,length(l1)), l2) ) dist_init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist_init) phi_init = matrix(c(1, cor0, cor0, 1), ncol=2) sigtheta = lambda%*%phi_init%*%t(lambda) + disturbs pq = dim(data)[2] #ll = ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq ) ll = ( (-nrow(data)*pq/2) * log(2*pi) ) - (nrow(data)/2) * ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) ) ll } Estimation Raw Set initial values. par_init_cov = c(rep(1, 6), rep(.05, 8), rep(.5, 3)) names(par_init_cov) = rep(c(&#39;l1&#39;,&#39;l2&#39;, &#39;dist&#39;, &#39;cov&#39;), c(3, 3, 8, 3)) Estimate and extract. result_cov = optim( par = par_init_cov, fn = cfa_cov, data = y, method = &quot;L-BFGS-B&quot;, lower = 0, control = list(fnscale = -1) ) loadings_cov = data.frame( f1 = c(1, result_cov$par[1:3], rep(0, 4)), f2 = c(rep(0, 4), 1, result_cov$par[4:6]) ) disturbances_cov = result_cov$par[7:14] Standardized par_init_cor = c(rep(1, 8), rep(.05, 8), 0) #for cor names(par_init_cor) = rep(c(&#39;l1&#39;, &#39;l2&#39;, &#39;dist&#39;, &#39;cor&#39;), c(4, 4, 8, 1)) result_cor = optim( par = par_init_cor, fn = cfa_cor, data = y, method = &quot;L-BFGS-B&quot;, lower = 0, upper = 1, control = list(fnscale = -1) ) loadings_cor = matrix( c(result_cor$par[1:4], rep(0, 4), rep(0, 4), result_cor$par[5:8]), ncol = 2 ) disturbances_cor = result_cor$par[9:16] Comparison Gather results for summary results = list( raw = list( loadings = round(data.frame(loadings_cov, Variances = disturbances_cov), 3), cov.fact = round(matrix(c(result_cov$par[c(15, 16, 16, 17)]), ncol =2) , 3) ), standardized = list( loadings = round( data.frame( loadings_cor, Variances = disturbances_cor, Rsq = (1 - disturbances_cor) ), 3), cor.fact = round(matrix(c(1, result_cor$par[c(17, 17)], 1), ncol = 2), 3) ), # note inclusion of intercepts for total number of par fit = data.frame( ll = result_cov$value, AIC = -2 * result_cov$value + 2 * (length(par_init_cov) + ncol(y)), BIC = -2 * result_cov$value + log(nrow(y)) * (length(par_init_cov) + ncol(y)) ) ) results $raw $raw$loadings f1 f2 Variances 1 1.000 0.000 1.097 2 0.465 0.000 0.955 3 0.353 0.000 0.951 4 0.588 0.000 0.948 5 0.000 1.000 1.064 6 0.000 0.744 0.942 7 0.000 0.381 0.976 8 0.000 0.507 1.038 $raw$cov.fact [,1] [,2] [1,] 0.983 0.170 [2,] 0.170 0.973 $standardized $standardized$loadings X1 X2 Variances Rsq 1 0.687 0.000 0.528 0.472 2 0.426 0.000 0.818 0.182 3 0.338 0.000 0.886 0.114 4 0.514 0.000 0.736 0.264 5 0.000 0.691 0.522 0.478 6 0.000 0.603 0.636 0.364 7 0.000 0.356 0.874 0.126 8 0.000 0.441 0.806 0.194 $standardized$cor.fact [,1] [,2] [1,] 1.000 0.174 [2,] 0.174 1.000 $fit ll AIC BIC 1 -12330.93 24711.86 24834.56 Confirm with lavaan. library(lavaan) y = data.frame(y) model = &#39; F1 =~ X1 + X2 + X3 + X4 F2 =~ X5 + X6 + X7 + X8 &#39; fit = cfa(model, data=y, mimic=&#39;Mplus&#39;, estimator=&#39;ML&#39;) fit.std = cfa(model, data=y, mimic=&#39;Mplus&#39;, estimator=&#39;ML&#39;, std.lv=T, std.ov=T) # for standardized # note that lavaan does not count the intercepts among the free params for AIC/BIC # by default, but the mimic=&#39;Mplus&#39; should have them correspond to optim&#39;s results summary(fit, fit.measures=TRUE, standardized=T) lavaan 0.6-7 ended normally after 30 iterations Estimator ML Optimization method NLMINB Number of free parameters 25 Number of observations 1000 Number of missing patterns 1 Model Test User Model: Test statistic 25.437 Degrees of freedom 19 P-value (Chi-square) 0.147 Model Test Baseline Model: Test statistic 746.093 Degrees of freedom 28 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.991 Tucker-Lewis Index (TLI) 0.987 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -12330.931 Loglikelihood unrestricted model (H1) -12318.212 Akaike (AIC) 24711.862 Bayesian (BIC) 24834.555 Sample-size adjusted Bayesian (BIC) 24755.154 Root Mean Square Error of Approximation: RMSEA 0.018 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.035 P-value RMSEA &lt;= 0.05 1.000 Standardized Root Mean Square Residual: SRMR 0.019 Parameter Estimates: Standard errors Standard Information Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all F1 =~ X1 1.000 0.991 0.687 X2 0.465 0.058 7.955 0.000 0.461 0.427 X3 0.353 0.050 7.041 0.000 0.350 0.338 X4 0.588 0.070 8.387 0.000 0.583 0.514 F2 =~ X5 1.000 0.986 0.691 X6 0.744 0.074 10.075 0.000 0.734 0.603 X7 0.381 0.047 8.160 0.000 0.376 0.356 X8 0.507 0.055 9.144 0.000 0.500 0.441 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all F1 ~~ F2 0.170 0.050 3.367 0.001 0.174 0.174 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .X1 0.054 0.046 1.173 0.241 0.054 0.037 .X2 -0.004 0.034 -0.104 0.917 -0.004 -0.003 .X3 -0.012 0.033 -0.372 0.710 -0.012 -0.012 .X4 0.004 0.036 0.113 0.910 0.004 0.004 .X5 0.044 0.045 0.964 0.335 0.044 0.031 .X6 -0.019 0.038 -0.504 0.614 -0.019 -0.016 .X7 0.019 0.033 0.555 0.579 0.019 0.018 .X8 0.002 0.036 0.042 0.967 0.002 0.001 F1 0.000 0.000 0.000 F2 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .X1 1.097 0.120 9.119 0.000 1.097 0.527 .X2 0.955 0.051 18.876 0.000 0.955 0.818 .X3 0.951 0.046 20.471 0.000 0.951 0.886 .X4 0.948 0.058 16.263 0.000 0.948 0.736 .X5 1.064 0.102 10.404 0.000 1.064 0.522 .X6 0.942 0.066 14.236 0.000 0.942 0.636 .X7 0.976 0.047 20.607 0.000 0.976 0.874 .X8 1.038 0.054 19.281 0.000 1.038 0.806 F1 0.983 0.135 7.264 0.000 1.000 1.000 F2 0.973 0.119 8.156 0.000 1.000 1.000 Mplus If you have access to Mplus you can use Mplus Automation to prepare the data. The following code is in Mplus syntax and will produce the above model. library(MplusAutomation) prepareMplusData(data.frame(y), &quot;factsim.dat&quot;) MODEL: F1 BY X1-X4; F2 BY X5-X8; results: STDYX; Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/cfa.R "],["penalized-maximum-likelihood.html", "Penalized Maximum Likelihood Data Setup Functions Estimation Comparison Source", " Penalized Maximum Likelihood A standard regression model via penalized likelihood. See the [linear regression example][Standard Linear Regression] code for comparison. Here the penalty is specified (via lambda argument) but one would typically estimate via cross-validation or some other fashion. Two penalties are possible with the function. One using the (squared) L2 norm (aka ridge regression, Tikhonov regularization), another using the L1 norm (aka lasso) which has the possibility of penalizing coefficients to zero, and thus can serve as a model selection procedure. I have a more technical approach to the lasso in the lasso.R file. Note that both L2 and L1 approaches can be seen as maximum a posteriori (MAP) estimates for a Bayesian regression with a specific prior on the coefficients. The L2 approach is akin to a normal prior with zero mean, while L1 is akin to a zero mean Laplace prior. See the Bayesian scripts for ways to implement. Data Setup library(tidyverse) set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X,y) Functions A maximum likelihood approach. penalized_ML = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = T) # log likelihood PL = switch(type, &#39;L2&#39; = -sum(L) + lambda * crossprod(beta[-1]), # the intercept is not penalized &#39;L1&#39; = -sum(L) + lambda * sum(abs(beta[-1])) ) } glmnet style approach that will put the lambda coefficient on equivalent scale. Uses a different objective function. Note that glmnet is actually elasticnet and mixes both L1 and L2 penalties. penalized_ML2 = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense obj = switch(type, &#39;L2&#39; = .5*crossprod(y - X %*% beta)/N + lambda * crossprod(beta[-1]), &#39;L1&#39; = .5*crossprod(y - X %*% beta)/N + lambda * sum(abs(beta[-1])) ) } Estimation Setup the model matrix for use with optim. X = cbind(1, X) Initial values. note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmpenalized_MLL2 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, control = list(reltol = 1e-12) ) optlmpenalized_MLL1 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, type = &#39;L1&#39;, control = list(reltol = 1e-12) ) parspenalized_MLL2 = optlmpenalized_MLL2$par parspenalized_MLL1 = optlmpenalized_MLL1$par Comparison Compare to lm. modlm = lm(y ~ ., dfXy) round( rbind( parspenalized_MLL2, parspenalized_MLL1, modlm = c(summary(modlm)$sigma ^ 2, coef(modlm)) ), digits = 4 ) sigma2 intercept b1 b2 parspenalized_MLL2 0.2195 -0.4325 0.1327 0.1113 parspenalized_MLL1 0.2195 -0.4325 0.1306 0.1094 modlm 0.2262 -0.4325 0.1334 0.1119 Compare to glmnet. Setting alpha to 0 and 1 is equivalent to L2 and L1 penalties respectively. You also wouldn’t want to specify lambda normally, and rather let it come about as part of the estimation procedure. We do so here just for demonstration. library(glmnet) glmnetL2 = glmnet( X[, -1], y, alpha = 0, lambda = .01, standardize = FALSE ) glmnetL1 = glmnet( X[, -1], y, alpha = 1, lambda = .01, standardize = FALSE ) pars_L2 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, control = list(reltol = 1e-12) )$par pars_L1 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, type = &#39;L1&#39;, control = list(reltol = 1e-12) )$par round( rbind( glmnet_L2 = t(as.matrix(coef(glmnetL2))), pars_L2 = pars_L2, glmnet_L1 = t(as.matrix(coef(glmnetL1))), pars_L1 = pars_L1 ), digits = 4 ) (Intercept) V1 V2 s0 -0.4324 0.1301 0.1094 pars_L2 -0.4324 0.1301 0.1094 s0 -0.4325 0.1207 0.1005 pars_L1 -0.4325 0.1207 0.1005 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/penalized_ML.R "],["lasso.html", "L1 (lasso) regularization Data Setup Functions Estimation Comparison Source", " L1 (lasso) regularization See Tibshirani (1996) for the source, or Murphy PML (2012) for a nice overview (watch for typos in depictions). A more conceptual depiction of the lasso can be found in penalized_ML.R. Data Setup library(tidyverse) set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N*p), ncol=p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6)) y = scale(X %*% b + rnorm(N, sd=.5)) lambda = .1 Functions Coordinate descent. lasso &lt;- function( X, # model matrix y, # target lambda = .1, # penalty parameter soft = TRUE, # soft vs. hard thresholding tol = 1e-6, # tolerance iter = 100, # number of max iterations verbose = TRUE # print out iteration number ) { # soft thresholding function soft_thresh &lt;- function(a, b) { out = rep(0, length(a)) out[a &gt; b] = a[a &gt; b] - b out[a &lt; -b] = a[a &lt; -b] + b out } w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y) tol_curr = 1 J = ncol(X) a = rep(0, J) c_ = rep(0, J) i = 1 while (tol &lt; tol_curr &amp;&amp; i &lt; iter) { w_old = w a = colSums(X^2) l = length(y)*lambda # for consistency with glmnet approach c_ = sapply(1:J, function(j) sum( X[,j] * (y - X[,-j] %*% w_old[-j]) )) if (soft) { for (j in 1:J) { w[j] = soft_thresh(c_[j]/a[j], l/a[j]) } } else { w = w_old w[c_&lt; l &amp; c_ &gt; -l] = 0 } tol_curr = crossprod(w - w_old) i = i + 1 if (verbose &amp;&amp; i%%10 == 0) message(i) } w } Estimation Note, if lambda=0, result is the same as lm.fit. result_soft = lasso( X, y, lambda = lambda, tol = 1e-12, soft = TRUE ) result_hard = lasso( X, y, lambda = lambda, tol = 1e-12, soft = FALSE ) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple values while only wanting the one set to the same lambda value as above (s). library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 1, lambda = c(10, 1, lambda), thresh = 1e-12, intercept = FALSE ), s = lambda ) library(lassoshooting) ls_res = lassoshooting( X = X, y = y, lambda = length(y) * lambda, thr = 1e-12 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), lasso_soft = result_soft, lasso_hard = result_hard, lspack = ls_res$coef, glmnet = glmnet_res[-1, 1], truth = b ) lm lasso_soft lasso_hard lspack glmnet truth X1 0.534988063 0.43542527 0.5348784 0.43542528 0.43552489 0.500 X2 -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500 X3 0.234376590 0.12436834 0.2343207 0.12436835 0.12447920 0.250 X4 -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250 X5 0.126037566 0.02036410 0.1260132 0.02036407 0.02047015 0.125 X6 -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125 X7 -0.016718534 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X8 0.009894575 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X9 -0.005441959 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X10 0.010561128 0.00000000 0.0000000 0.00000000 0.00000000 0.000 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/lasso.R "],["ridge.html", "L2 (ridge) regularization Data Setup Functions Estimation Comparison Source", " L2 (ridge) regularization Compare to lasso chapter. A more conceptual depiction of the lasso can be found in the penalized ML chapter. Data Setup library(tidyverse) set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N * p), ncol = p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4)) y = scale(X %*% b + rnorm(N, sd = .5)) Note, if lambda = 0, result is the same as lm.fit. Functions ridge &lt;- function(w, X, y, lambda = .1) { # X: model matrix; # y: target; # lambda: penalty parameter; # w: the weights/coefficients crossprod(y - X %*% w) + lambda * length(y) * crossprod(w) } Estimation result_ridge = optim( rep(0, ncol(X)), ridge, X = X, y = y, lambda = .1, method = &#39;BFGS&#39; ) Analytical result. result_ridge2 = solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y) Alternative with augmented data (note sigma is ignored as it equals 1, but otherwise X/sigma and y/sigma). X2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X))) y2 = c(y, rep(0, ncol(X))) result_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso, while alpha=0 would be ridge. library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 0, lambda = c(10, 1, .1), thresh = 1e-12, intercept = F ), s = .1 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), ridge = result_ridge$par, ridge2 = result_ridge2, ridge3 = result_ridge3, glmnet = glmnet_res[-1, 1], truth = b ) lm ridge ridge2 ridge3 glmnet truth X1 0.534988063 0.485323748 0.485323748 0.485323748 0.485368766 0.500 X2 -0.529993422 -0.480742032 -0.480742032 -0.480742032 -0.480786661 -0.500 X3 0.234376590 0.209412833 0.209412833 0.209412833 0.209435147 0.250 X4 -0.294350608 -0.268814168 -0.268814168 -0.268814168 -0.268837476 -0.250 X5 0.126037566 0.114963716 0.114963716 0.114963716 0.114973801 0.125 X6 -0.159386728 -0.145880488 -0.145880488 -0.145880488 -0.145892837 -0.125 X7 -0.016718534 -0.021658889 -0.021658889 -0.021658889 -0.021655033 0.000 X8 0.009894575 0.006956965 0.006956965 0.006956965 0.006959470 0.000 X9 -0.005441959 0.001392244 0.001392244 0.001392244 0.001386661 0.000 X10 0.010561128 0.010985385 0.010985385 0.010985385 0.010985102 0.000 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ridge.R "],["newton-irls.html", "Newton and IRLS Data Setup Functions Comparison Source", " Newton and IRLS Here we demonstrate Newton’s and Iterated Reweighted Least Squares approaches via logistic regression. For the following, I had Murphy’s PML text open and more or less followed the algorithms in chapter 8. Note that for Newton’s method, this doesn’t implement a line search to find a more optimal stepsize at a given iteration. Data Setup Predict graduate school admission based on gre, gpa, and school rank (higher=more prestige). See corresponding demo here: https://stats.idre.ucla.edu/stata/dae/logistic-regression/. The only difference is that I treat rank as numeric rather than categorical. library(tidyverse) admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) comparison_model = glm(admit ~ gre + gpa + rank, data = admit, family = binomial) summary(comparison_model) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Deviance Residuals: Min 1Q Median 3Q Max -1.5802 -0.8848 -0.6382 1.1575 2.1732 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.449549 1.132846 -3.045 0.00233 ** gre 0.002294 0.001092 2.101 0.03564 * gpa 0.777014 0.327484 2.373 0.01766 * rank -0.560031 0.127137 -4.405 1.06e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.44 on 396 degrees of freedom AIC: 467.44 Number of Fisher Scoring iterations: 4 X = model.matrix(comparison_model) y = comparison_model$y Functions Newton’s Method newton &lt;- function( X, y, tol = 1e-12, iter = 500, stepsize = .5 ) { # Args: # X: model matrix # y: target # tol: tolerance # iter: maximum number of iterations # stepsize: (0, 1) # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it +1 ll_old = ll mu = plogis(X %*% beta)[,1] g = crossprod(X, mu-y) # gradient S = diag(mu*(1-mu)) H = t(X) %*% S %*% X # hessian beta = beta - stepsize * solve(H) %*% g ll = sum(dbinom(y, prob = mu, size = 1, log = TRUE)) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll ) } Compare to base R glm. newton_result = newton( X = X, y = y, stepsize = .9, tol = 1e-8 # tol set to 1e-8 as in glm default ) newton_result $beta [,1] (Intercept) -3.449548577 gre 0.002293959 gpa 0.777013649 rank -0.560031371 $iter [1] 8 $tol [1] 2.581544e-10 $loglik [1] -229.7209 comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 rbind( newton = unlist(newton_result), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = -logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA 229.7209 IRLS Note that glm is actually using IRLS, so the results from this should be fairly spot on. irls &lt;- function(X, y, tol = 1e-12, iter = 500) { # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it + 1 ll_old = ll eta = X %*% beta mu = plogis(eta)[,1] s = mu * (1 - mu) S = diag(s) z = eta + (y-mu)/s beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z)) ll = sum( dbinom( y, prob = plogis(X %*% beta), size = 1, log = T ) ) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll, weights = plogis(X %*% beta) * (1 - plogis(X %*% beta)) ) } tol set to 1e-8 as in glm default. irls_result = irls(X = X, y = y, tol = 1e-8) str(irls_result) List of 5 $ beta : num [1:4, 1] -3.44955 0.00229 0.77701 -0.56003 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;gre&quot; &quot;gpa&quot; &quot;rank&quot; .. ..$ : NULL $ iter : num 4 $ tol : num 6e-09 $ loglik : num -230 $ weights: num [1:400, 1] 0.1536 0.2168 0.2026 0.1268 0.0884 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:400] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ : NULL comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 Comparison Compare all results. rbind( newton = unlist(newton_result), irls = unlist(irls_result[-length(irls_result)]), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 irls -3.449549 0.002293959 0.7770137 -0.5600314 4 5.996583e-09 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA -229.7209 Compare weights. head(cbind(irls_result$weights, comparison_model$weights)) [,1] [,2] 1 0.15362250 0.15362250 2 0.21679615 0.21679615 3 0.20255723 0.20255724 4 0.12676333 0.12676334 5 0.08835918 0.08835918 6 0.23528108 0.23528108 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/newton_irls.R "],["nelder-mead.html", "Nelder Mead First Version Second Version Source", " Nelder Mead This is based on the pure Python implementation by François Chollet found at https://github.com/fchollet/nelder-mead (also in the Miscellaneous R code repo at nelder_mead.py). This is mostly just an academic exercise on my part. I’m not sure how much one would use the basic NM for many situations. In my experience BFGS and other approaches would be faster, more accurate, and less sensitive to starting values for the types of problems I’ve played around with. Others who actually spend their time researching such things seem to agree. There were two issues on (GitHub)[https://github.com/fchollet/nelder-mead/issues/2] regarding the original code, and I’ve implemented the suggested corrections with notes. The initial function code is not very R-like, as the goal was to keep more similar to the original Python for comparison, which used a list approach. I also provide a more R-like/cleaner version that uses matrices instead of lists, but which still sticks the same approach for the most part. For both functions, comparisons are made using the optimx package, but feel free to use base R’s optim instead. f function to optimize, must return a scalar score and operate over an array of the same dimensions as x_start x_start initial position step look-around radius in initial step no_improve_thr See no_improv_break no_improv_break break after no_improv_break iterations with an improvement lower than no_improv_thr max_iter always break after this number of iterations. Set it to 0 to loop indefinitely. alpha parameters of the algorithm (see Wikipedia page for reference) gamma parameters of the algorithm (see Wikipedia page for reference) rho parameters of the algorithm (see Wikipedia page for reference) sigma parameters of the algorithm (see Wikipedia page for reference) verbose Print iterations? This function returns the best parameter array and best score. First Version nelder_mead = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init dim = length(x_start) prev_best = f(x_start) no_improv = 0 res = list(list(x_start = x_start, prev_best = prev_best)) for (i in 1:dim) { x = x_start x[i] = x[i] + step score = f(x) res = append(res, list(list(x_start = x, prev_best = score))) } # simplex iter iters = 0 while (TRUE) { # order idx = sapply(res, `[[`, 2) res = res[order(idx)] # ascending order best = res[[1]][[2]] # break after max_iter if (max_iter &gt; 0 &amp; iters &gt;= max_iter) return(res[[1]]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[[1]]) # centroid x0 = rep(0, dim) for (tup in 1:(length(res)-1)) { for (i in 1:dim) { x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1) } } # reflection xr = x0 + alpha * (x0 - res[[length(res)]][[1]]) rscore = f(xr) if (res[[1]][[2]] &lt;= rscore &amp; rscore &lt; res[[length(res)-1]][[2]]) { res[[length(res)]] = list(xr, rscore) next } # expansion if (rscore &lt; res[[1]][[2]]) { # xe = x0 + gamma*(x0 - res[[length(res)]][[1]]) # issue with this xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[[length(res)]] = list(xe, escore) next } else { res[[length(res)]] = list(xr, rscore) next } } # contraction # xc = x0 + rho*(x0 - res[[length(res)]][[1]]) # issue with wiki consistency for rho values (and optim) xc = x0 + rho * (res[[length(res)]][[1]] - x0) cscore = f(xc) if (cscore &lt; res[[length(res)]][[2]]) { res[[length(res)]] = list(xc, cscore) next } # reduction x1 = res[[1]][[1]] nres = list() for (tup in res) { redx = x1 + sigma * (tup[[1]] - x1) score = f(redx) nres = append(nres, list(list(redx, score))) } res = nres } } Example The function to minimize. f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } Estimate. nelder_mead( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) [[1]] [1] -1.570797e+00 -2.235577e-07 1.637460e-14 [[2]] [1] -1 Compare to optimx. You may see warnings. optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model I find a regression model to be more applicable/intuitive for my needs, so provide an example for that case. Data Setup library(tidyverse) set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm estimates. lm.fit(X, y)$coef x1 x2 x3 x4 x5 x6 -0.96214657 0.59432481 0.04864576 0.27573466 0.97525840 -0.07470287 nm_result = nelder_mead( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12, verbose = FALSE ) Comparison Compare to optimx. opt_out = optimx::optimx( runif(ncol(X)), fn = f, # model function method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, #rho maxit = 2000, reltol = 1e-12 ) ) rbind( nm_func = unlist(nm_result), nm_optimx = opt_out[1:7] ) p1 p2 p3 p4 p5 p6 value nm_func -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389 501.3155 nm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054 501.3155 Second Version This is a more natural R approach in my opinion. nelder_mead2 = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init npar = length(x_start) nc = npar + 1 prev_best = f(x_start) no_improv = 0 res = matrix(c(x_start, prev_best), ncol = nc) colnames(res) = c(paste(&#39;par&#39;, 1:npar, sep = &#39;_&#39;), &#39;score&#39;) for (i in 1:npar) { x = x_start x[i] = x[i] + step score = f(x) res = rbind(res, c(x, score)) } # simplex iter iters = 0 while (TRUE) { # order res = res[order(res[, nc]), ] # ascending order best = res[1, nc] # break after max_iter if (max_iter &amp; iters &gt;= max_iter) return(res[1, ]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[1, ]) nr = nrow(res) # centroid: more efficient than previous double loop x0 = colMeans(res[(1:npar), -nc]) # reflection xr = x0 + alpha * (x0 - res[nr, -nc]) rscore = f(xr) if (res[1, &#39;score&#39;] &lt;= rscore &amp; rscore &lt; res[npar, &#39;score&#39;]) { res[nr,] = c(xr, rscore) next } # expansion if (rscore &lt; res[1, &#39;score&#39;]) { xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[nr, ] = c(xe, escore) next } else { res[nr, ] = c(xr, rscore) next } } # contraction xc = x0 + rho * (res[nr, -nc] - x0) cscore = f(xc) if (cscore &lt; res[nr, &#39;score&#39;]) { res[nr,] = c(xc, cscore) next } # reduction x1 = res[1, -nc] nres = res for (i in 1:nr) { redx = x1 + sigma * (res[i, -nc] - x1) score = f(redx) nres[i, ] = c(redx, score) } res = nres } } Example Function f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } nelder_mead2( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) par_1 par_2 par_3 score -1.570797e+00 -2.235577e-07 1.622809e-14 -1.000000e+00 optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0 A Regression Model set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm_par = lm.fit(X, y)$coef nm_par = nelder_mead2( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12 ) Comparison Compare to optimx. opt_par = optimx::optimx( runif(ncol(X)), fn = f, method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 2000, reltol = 1e-12 ) )[1:(npreds + 1)] rbind( lm = lm_par, nm = nm_par, optimx = opt_par, truth = beta ) p1 p2 p3 p4 p5 p6 lm -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287 nm -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389 optimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054 truth -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/nelder_mead.R "],["gradient-descent.html", "Gradient Descent Data Setup Function Estimation Comparison Source", " Gradient Descent Gradient descent for a standard linear regression model. The function takes arguments starting points for the parameters to be estimated, a tolerance or maximum iteration value to provide a stopping point, stepsize (or starting stepsize for adaptive approach), whether to print out iterations, and whether to plot the loss over each iteration. Data Setup Create some basic data for standard regression. library(tidyverse) set.seed(8675309) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) # model matrix Function (Batch) Gradient Descent Algorithm. gd = function( par, X, y, tolerance = 1e-3, maxit = 1000, stepsize = 1e-3, adapt = FALSE, verbose = TRUE, plotLoss = TRUE ) { # initialize beta = par; names(beta) = colnames(X) loss = crossprod(X %*% beta - y) tol = 1 iter = 1 while(tol &gt; tolerance &amp;&amp; iter &lt; maxit){ LP = X %*% beta grad = t(X) %*% (LP - y) betaCurrent = beta - stepsize * grad tol = max(abs(betaCurrent - beta)) beta = betaCurrent loss = append(loss, crossprod(LP - y)) iter = iter + 1 if (adapt) stepsize = ifelse( loss[iter] &lt; loss[iter - 1], stepsize * 1.2, stepsize * .8 ) if (verbose &amp;&amp; iter %% 10 == 0) message(paste(&#39;Iteration:&#39;, iter)) } if (plotLoss) plot(loss, type = &#39;l&#39;, bty = &#39;n&#39;) list( par = beta, loss = loss, RSE = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), iter = iter, fitted = LP ) } Estimation Set starting values. init = rep(0, 3) For any particular data you’d have to fiddle with the stepsize, which could be assessed via cross-validation, or alternatively one can use an adaptive approach, a simple one of which is implemented in this function. gd_result = gd( init, X = X, y = y, tolerance = 1e-8, stepsize = 1e-4, adapt = TRUE ) str(gd_result) List of 5 $ par : num [1:3, 1] 0.985 0.487 0.218 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ loss : num [1:70] 2315 2315 2075 1918 1760 ... $ RSE : num [1, 1] 1.03 $ iter : num 70 $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ... Comparison We can compare to standard linear regression. rbind( gd = round(gd_result$par[, 1], 5), lm = coef(lm(y ~ x1 + x2)) ) Intercept x1 x2 gd 0.9847800 0.4867900 0.2175200 lm 0.9847803 0.4867896 0.2175169 # summary(lm(y ~ x1 + x2)) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gradient_descent.R "],["stochastic-gradient-descent.html", "Stochastic Gradient Descent Data Setup Function Estimation Comparison Visualize Estimates Data Set Shift Source", " Stochastic Gradient Descent Here we have ‘online’ learning via stochastic gradient descent. See also, standard gradient descent In the following, we have basic data for standard regression, but in this ‘online’ learning case, we can assume each observation comes to us as a stream over time rather than as a single batch, and would continue coming in. Note that there are plenty of variations of this, and it can be applied in the batch case as well. Currently no stopping point is implemented in order to trace results over all data points/iterations. On revisiting this much later, I thought it useful to add that I believe this was motivated by the example in Murphy’s Probabilistic Machine Learning. I also made some cleanup to my original code, added some comments, but mostly left it as it was. Data Setup library(tidyverse) set.seed(1234) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) Function sgd = function( par, # parameter estimates X, # model matrix y, # target variable stepsize = 1, # the learning rate stepsizeTau = 0, # if &gt; 0, a check on the LR at early iterations average = FALSE ){ # initialize beta = par names(beta) = colnames(X) betamat = matrix(0, nrow(X), ncol = length(beta)) # Collect all estimates fits = NA # fitted values s = 0 # adagrad per parameter learning rate adjustment loss = NA # Collect loss at each point for (i in 1:nrow(X)) { Xi = X[i, , drop = FALSE] yi = y[i] LP = Xi %*% beta # matrix operations not necessary, grad = t(Xi) %*% (LP - yi) # but makes consistent with the standard gd s = s + grad^2 beta = beta - stepsize * grad/(stepsizeTau + sqrt(s)) # adagrad approach if (average &amp; i &gt; 1) { beta = beta - 1/i * (betamat[i - 1, ] - beta) # a variation } betamat[i,] = beta fits[i] = LP loss[i] = (LP - yi)^2 } LP = X %*% beta lastloss = crossprod(LP - y) list( par = beta, # final estimates parvec = betamat, # all estimates loss = loss, # observation level loss RMSE = sqrt(sum(lastloss)/nrow(X)), fitted = fits ) } Estimation Set starting values. init = rep(0, 3) For any particular data you might have to fiddle with the stepsize, perhaps choosing one based on cross-validation with old data. sgd_result = sgd( init, X = X, y = y, stepsize = .1, stepsizeTau = .5, average = FALSE ) str(sgd_result) List of 5 $ par : num [1:3, 1] 1.024 0.537 0.148 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ... $ loss : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ... $ RMSE : num 1.01 $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ... sgd_result$par [,1] Intercept 1.0241049 x1 0.5368198 x2 0.1478470 Comparison We can compare to standard linear regression. # summary(lm(y ~ x1 + x2)) coef1 = coef(lm(y ~ x1 + x2)) rbind( sgd_result = sgd_result$par[, 1], lm = coef1 ) Intercept x1 x2 sgd_result 1.024105 0.5368198 0.1478470 lm 1.029957 0.5177020 0.1631026 Visualize Estimates library(tidyverse) gd = data.frame(sgd_result$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes( x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration == n), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, size = 4, data = filter(gd, Iteration == n) ) + theme_minimal() Data Set Shift This data includes a shift of the previous data. set.seed(1234) n2 = 1000 x1.2 = rnorm(n2) x2.2 = rnorm(n2) y2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2) X2 = rbind(X, cbind(1, x1.2, x2.2)) coef2 = coef(lm(y2 ~ x1.2 + x2.2)) y2 = c(y, y2) n3 = 1000 x1.3 = rnorm(n3) x2.3 = rnorm(n3) y3 = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3) coef3 = coef(lm(y3 ~ x1.3 + x2.3)) X3 = rbind(X2, cbind(1, x1.3, x2.3)) y3 = c(y2, y3) Estimation sgd_result2 = sgd( init, X = X3, y = y3, stepsize = 1, stepsizeTau = 0, average = FALSE ) str(sgd_result2) List of 5 $ par : num [1:3, 1] 0.821 -0.223 0.211 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ... $ loss : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ... $ RMSE : num 1.57 $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ... Comparison Compare with lm for each data part. sgd_result2$parvec[c(n, n + n2, n + n2 + n3), ] [,1] [,2] [,3] [1,] 1.0859378 0.5128904 0.1457697 [2,] -0.9246994 0.2945723 -0.2941759 [3,] 0.8213521 -0.2229918 0.2112883 rbind(coef1, coef2, coef3) (Intercept) x1 x2 coef1 1.0299573 0.5177020 0.1631026 coef2 -0.9700427 0.2677020 -0.2868974 coef3 1.0453166 -0.2358521 0.2418489 Visualize Estimates Visualize estimates. gd = data.frame(sgd_result2$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes(x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 4, show.legend = FALSE ) + theme_minimal() Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/stochastic_gradient_descent.R "],["em.html", "Expectation-Maximization Mixture Model Multivariate Mixture Model Probit Model PCA Probabilistic PCA State Space Model", " Expectation-Maximization Mixture Model The following code is based on algorithms noted in Murphy, 2012 Probabilistic Machine Learning, specifically, Chapter 11, section 4. Data Setup This example uses Old Faithful geyser eruptions. This is only a univariate mixture for either eruption time or wait time. The next example will be doing both variables, i.e. multivariate normal. ‘Geyser’ is supposedly more accurate, though seems to have arbitrarily assigned some duration values. See also http://www.geyserstudy.org/geyser.aspx?pGeyserNo=OLDFAITHFUL, but that only has intervals. Some July 1995 data is available. library(tidyverse) # faithful data set is in base R data(faithful) head(faithful) eruptions waiting 1 3.600 79 2 1.800 54 3 3.333 74 4 2.283 62 5 4.533 85 6 2.883 55 eruptions = as.matrix(faithful[, 1, drop = FALSE]) wait_times = as.matrix(faithful[, 2, drop = FALSE]) Function em_mixture = function( params, X, clusters = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are starting parameters (means, covariances, cluster probability), # data, number of clusters desired, tolerance, maximum iterations, and whether # to show iterations # Starting points N = nrow(X) nams = names(params) mu = params$mu var = params$var probs = params$probs # Other initializations # initialize cluster &#39;responsibilities&#39;, i.e. probability of cluster # membership for each observation i ri = matrix(0, ncol = clusters, nrow = N) it = 0 converged = FALSE if (showits) # Show iterations cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { probsOld = probs muOld = mu varOld = var riOld = ri # E # Compute responsibilities for (k in 1:clusters){ ri[, k] = probs[k] * dnorm(X, mu[k], sd = sqrt(var[k]), log = FALSE) } ri = ri/rowSums(ri) # M rk = colSums(ri) # rk is the weighted average cluster membership size probs = rk/N mu = (t(X) %*% ri) / rk var = (t(X^2) %*% ri) / rk - mu^2 # could do mu and var via log likelihood here, but this is more straightforward parmlistold = rbind(probsOld, muOld, varOld) parmlistcurrent = rbind(probs, mu, var) it = it + 1 # if showits true, &amp; it =1 or divisible by 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(parmlistold - parmlistcurrent)) &lt;= tol } clust = which(round(ri) == 1, arr.ind = TRUE) # create cluster membership clust = clust[order(clust[, 1]), 2] # order according to row rather than cluster out = list( probs = probs, mu = mu, var = var, resp = ri, cluster = clust ) out } Estimation Starting parameters, requires mean, variance and class probability. Note that starts for mean must be within the data range or it will break. params1 = list(mu = c(2, 5), var = c(1, 1), probs = c(.5, .5)) params2 = list(mu = c(50, 90), var = c(1, 15), probs = c(.5, .5)) mix_erupt = em_mixture(params1, X = eruptions, tol = 1e-8) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... mix_waiting = em_mixture(params2, X = wait_times, tol = 1e-8) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... Comparison Compare to flexmix package results. library(flexmix) flex_erupt = flexmix(eruptions ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) flex_wait = flexmix(wait_times ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) The following provides means, variances and probability of group membership. Note that the cluster label is arbitrary so cluster 1 for one model may be cluster 2 in another. Eruptions mean_var = rbind(mix_erupt$mu, sqrt(mix_erupt$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var) = c(&#39;cluster 1&#39;, &#39;cluster 2&#39;) mean_var_flex = parameters(flex_erupt) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var_flex) = c(&#39;cluster 1 flex&#39;, &#39;cluster 2 flex&#39;) prob_membership = mix_erupt$probs prob_membership_flex = flex_erupt@size / sum(flex_erupt@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) $params cluster 1 cluster 2 cluster 1 flex cluster 2 flex means 2.0186078 4.2733434 4.2733678 2.0186385 variances 0.2356218 0.4370631 0.4378355 0.2361098 $clusterpobs prob_membership prob_membership_flex 1 0.3484046 0.6507353 2 0.6515954 0.3492647 Waiting mean_var = rbind(mix_waiting$mu, sqrt(mix_waiting$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var) = c(&#39;cluster 1&#39;, &#39;cluster 2&#39;) mean_var_flex = parameters(flex_wait) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var_flex) = c(&#39;cluster 1 flex&#39;, &#39;cluster 2 flex&#39;) prob_membership = mix_waiting$probs prob_membership_flex = flex_wait@size / sum(flex_wait@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) $params cluster 1 cluster 2 cluster 1 flex cluster 2 flex means 54.614856 80.091069 54.616140 80.090678 variances 5.871219 5.867734 5.884422 5.879634 $clusterpobs prob_membership prob_membership_flex 1 0.3608861 0.3639706 2 0.6391139 0.6360294 qplot(x = eruptions, y = waiting, data = faithful) + theme_minimal() ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_waiting$cluster))) + geom_density2d() + theme_minimal() faithful %&gt;% mutate(prob_clus_1 = mix_waiting$resp[, 1]) %&gt;% ggplot(aes(x = eruptions, y = waiting)) + geom_point(aes(color = prob_clus_1)) + geom_density2d() + theme_minimal() Supplemental Example This uses the MASS version (reversed columns). These don’t look even remotely the same data on initial inspection- geyser is even more rounded and of opposite conclusion. Turns out geyser is offset by 1, such that duration 1 should be coupled with waiting 2 and on down. Still the rounding at 2 and 4 (and whatever division was done on duration) makes this fairly poor data. I’ve cleaned this up a little bit in case someone wants to play with it for additional practice, but it’s not evaluated. library(MASS) geyser = data.frame(duration = geyser$duration[-299], waiting = geyser$waiting[-1]) # compare to faithful layout(1:2) plot(faithful) plot(geyser) X3 = matrix(geyser[,1]) X4 = matrix(geyser[,2]) # MASS version test3 = em_mixture(params1, X = X3, tol = 1e-8) test4 = em_mixture(params2, X = X4, tol = 1e-8) flexmod3 = flexmix(X3 ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) flexmod4 = flexmix(X4 ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) # note variability differences compared to faithful dataset # Eruptions/Duration mean_var = rbind(test3$mu, sqrt(test3$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) mean_var_flex = parameters(flexmod3) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) prob_membership = test3$probs prob_membership_flex = flexmod3@size / sum(flexmod3@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) # Waiting mean_var = rbind(test4$mu, sqrt(test4$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) mean_var_flex = parameters(flexmod4) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) prob_membership = test4$probs prob_membership_flex = flexmod4@size / sum(flexmod4@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) # Some plots library(ggplot2) qplot(x = eruptions, y = waiting, data = faithful) + theme_minimal() ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_waiting$cluster))) + theme_minimal() ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = mix_waiting$resp[, 1])) + theme_minimal() Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20Mixture.R Multivariate Mixture Model The following code is based on algorithms noted in Murphy, 2012 Probabilistic Machine Learning. Specifically, Chapter 11, section 4. Function em_mixture = function( params, X, clusters = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are # params: starting parameters (means, covariances, cluster probability) # X: data # clusters: number of clusters desired # tol: tolerance # maxits: maximum iterations # showits: whether to show iterations require(mvtnorm) # Starting points N = nrow(X) mu = params$mu var = params$var probs = params$probs # initializations # cluster &#39;responsibilities&#39;, i.e. probability of cluster membership for each # observation i ri = matrix(0, ncol=clusters, nrow=N) ll = 0 # log likelihood it = 0 # iteration count converged = FALSE # convergence # Show iterations if showits == true if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while (!converged &amp; it &lt; maxits) { probsOld = probs # muOld = mu # Use direct values or loglike for convergence check # varOld = var llOld = ll riOld = ri ### E # Compute responsibilities for (k in 1:clusters){ ri[,k] = probs[k] * dmvnorm(X, mu[k, ], sigma = var[[k]], log = FALSE) } ri = ri/rowSums(ri) ### M rk = colSums(ri) # rk is weighted average cluster membership size probs = rk/N for (k in 1:clusters){ varmat = matrix(0, ncol = ncol(X), nrow = ncol(X)) # initialize to sum matrices for (i in 1:N){ varmat = varmat + ri[i,k] * X[i,]%*%t(X[i,]) } mu[k,] = (t(X) %*% ri[,k]) / rk[k] var[[k]] = varmat/rk[k] - mu[k,]%*%t(mu[k,]) ll[k] = -.5*sum( ri[,k] * dmvnorm(X, mu[k,], sigma = var[[k]], log = TRUE) ) } ll = sum(ll) ### compare old to current for convergence parmlistold = c(llOld, probsOld) # c(muOld, unlist(varOld), probsOld) parmlistcurrent = c(ll, probs) # c(mu, unlist(var), probs) it = it + 1 # if showits true, &amp; it =1 or modulo of 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = min(abs(parmlistold - parmlistcurrent)) &lt;= tol } clust = which(round(ri) == 1, arr.ind = TRUE) # create cluster membership clust = clust[order(clust[,1]), 2] # order accoring to row rather than cluster out = list( probs = probs, mu = mu, var = var, resp = ri, cluster = clust, ll = ll ) out } Example 1: Old Faithful eruptions This example uses Old Faithful geyser eruptions. This is can be compared to the univariate code from the other chapter. See also http://www.geyserstudy.org/geyser.aspx?pGeyserNo=OLDFAITHFUL Data Setup library(tidyverse) data(&quot;faithful&quot;) Estimation Create starting values and estimate. mustart = rbind(c(3, 60), c(3, 60.1)) # must be at least slightly different covstart = list(cov(faithful), cov(faithful)) probs = c(.01, .99) # params is a list of mu, var, and probs starts = list(mu = mustart, var = covstart, probs = probs) mix_faithful = em_mixture( params = starts, X = as.matrix(faithful), clusters = 2, tol = 1e-12, maxits = 1500, showits = TRUE ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... 60... 65... 70... 75... 80... str(mix_faithful) List of 6 $ probs : num [1:2] 0.356 0.644 $ mu : num [1:2, 1:2] 2.04 4.29 54.48 79.97 $ var :List of 2 ..$ : num [1:2, 1:2] 0.0692 0.4352 0.4352 33.6973 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : NULL .. .. ..$ : chr [1:2] &quot;eruptions&quot; &quot;waiting&quot; ..$ : num [1:2, 1:2] 0.17 0.941 0.941 36.046 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : NULL .. .. ..$ : chr [1:2] &quot;eruptions&quot; &quot;waiting&quot; $ resp : num [1:272, 1:2] 2.59e-09 1.00 8.42e-06 1.00 1.00e-21 ... $ cluster: int [1:272] 2 1 2 1 2 1 2 2 1 2 ... $ ll : num 477 Visualize. library(ggplot2) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_faithful$cluster))) + theme_minimal() faithful %&gt;% mutate(prob_clus_1 = mix_faithful$resp[, 1]) %&gt;% ggplot(aes(x = eruptions, y = waiting)) + geom_point(aes(color = prob_clus_1)) + theme_minimal() # relatively speaking, these are extremely well-separated clusters worst = apply(mix_faithful$resp, 1, function(x) max(x) &lt; .99) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = worst)) + theme_minimal() Comparison Compare to mclust results. library(mclust) # options are set to be more similar mix_mclust = Mclust( faithful[, 1:2], 2, modelNames = &#39;VVV&#39;, control = emControl(tol = 1e-12) ) str(mix_mclust, 1) List of 16 $ call : language Mclust(data = faithful[, 1:2], G = 2, modelNames = &quot;VVV&quot;, control = emControl(tol = 1e-12)) $ data : num [1:272, 1:2] 3.6 1.8 3.33 2.28 4.53 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ modelName : chr &quot;VVV&quot; $ n : int 272 $ d : int 2 $ G : int 2 $ BIC : &#39;mclustBIC&#39; num [1, 1] -2322 ..- attr(*, &quot;dimnames&quot;)=List of 2 ..- attr(*, &quot;G&quot;)= num 2 ..- attr(*, &quot;modelNames&quot;)= chr &quot;VVV&quot; ..- attr(*, &quot;control&quot;)=List of 4 ..- attr(*, &quot;initialization&quot;)=List of 3 ..- attr(*, &quot;warn&quot;)= logi FALSE ..- attr(*, &quot;n&quot;)= int 272 ..- attr(*, &quot;d&quot;)= int 2 ..- attr(*, &quot;oneD&quot;)= logi FALSE ..- attr(*, &quot;criterion&quot;)= chr &quot;BIC&quot; ..- attr(*, &quot;returnCodes&quot;)= num [1, 1] 0 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 $ loglik : num -1130 $ df : num 11 $ bic : num -2322 $ icl : num -2323 $ hypvol : num NA $ parameters :List of 4 $ z : num [1:272, 1:2] 1.00 1.91e-09 1.00 1.07e-05 1.00 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ classification: Named num [1:272] 1 2 1 2 1 2 1 1 2 1 ... ..- attr(*, &quot;names&quot;)= chr [1:272] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ uncertainty : Named num [1:272] 2.59e-09 1.91e-09 8.42e-06 1.07e-05 0.00 ... ..- attr(*, &quot;names&quot;)= chr [1:272] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... - attr(*, &quot;class&quot;)= chr &quot;Mclust&quot; # compare means t(mix_faithful$mu) [,1] [,2] [1,] 2.036388 4.289662 [2,] 54.478516 79.968115 mix_mclust$parameters$mean [,1] [,2] eruptions 4.289662 2.036388 waiting 79.968115 54.478517 # compare variances mix_faithful$var [[1]] eruptions waiting [1,] 0.06916767 0.4351676 [2,] 0.43516762 33.6972821 [[2]] eruptions waiting [1,] 0.1699684 0.9406093 [2,] 0.9406093 36.0462113 mix_mclust$parameters$variance$sigma , , 1 eruptions waiting eruptions 0.1699684 0.9406089 waiting 0.9406089 36.0462071 , , 2 eruptions waiting eruptions 0.06916769 0.4351678 waiting 0.43516784 33.6972835 # compare classifications, reverse in case arbitrary numbering of one of them is opposite table(mix_faithful$cluster, mix_mclust$classification) 1 2 1 0 97 2 175 0 table(ifelse(mix_faithful$cluster == 2, 1, 2), mix_mclust$classification) 1 2 1 175 0 2 0 97 # compare responsibilities; reverse one if arbitrary numbering of one of them is opposite # cbind(round(mix_faithful$resp[,1], 2), round(mix_mclust$z[,2], 2)) # cluster &#39;1&#39; # cbind(round(mix_faithful$resp[,2], 2), round(mix_mclust$z[,1], 2)) # cluster &#39;2&#39; Example 2: Iris data set Data Setup Set up data iris2 = iris %&gt;% select(-Species) Estimation Run and examine. We add noise to our starting value, and the function is notably sensitive to starts, but don’t want to cheat too badly. mustart = iris %&gt;% group_by(Species) %&gt;% summarise(across(.fns = function(x) mean(x) + runif(1, 0, .5))) %&gt;% select(-Species) %&gt;% as.matrix() # use purrr::map due to mclust::map covstart = iris %&gt;% split(.$Species) %&gt;% purrr::map(select, -Species) %&gt;% purrr::map(function(x) cov(x) + diag(runif(4, 0, .5))) probs = c(.1, .2, .7) starts = list(mu = mustart, var = covstart, probs = probs) mix_mclust_iris = em_mixture( params = starts, X = as.matrix(iris2), clusters = 3, tol = 1e-8, maxits = 1500, showits = T ) Iterations of EM: 1... table(mix_mclust_iris$cluster, iris$Species) setosa versicolor virginica 1 50 0 0 2 0 48 0 3 0 2 50 Comparison Compare to mclust results. mclust_iris = Mclust(iris[,1:4], 3) table(mclust_iris$classification, iris$Species) setosa versicolor virginica 1 50 0 0 2 0 45 0 3 0 5 50 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20Mixture%20MV.R Probit Model The following regards models for a binary response. See Murphy, 2012 Probabilistic Machine Learning Chapter 11.4. Data Setup library(tidyverse) admission = haven::read_dta(&quot;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&quot;) Probit via Maximum Likelihood Function We’ll start with the a basic maximum likelihood function for a standard probit. See the [logistic regression][Standard Logistic] example as comparison. probit_mle = function(params, X, y){ # Arguments are starting parameters (coefficients), model matrix, response b = params mu = X %*% b # linear predictor # compute the log likelihood either way # ll = sum(y * pnorm(mu, log.p = TRUE) + (1 - y) * pnorm(-mu, log.p = TRUE)) ll = sum(dbinom(y, 1, prob = pnorm(mu), log = TRUE)) -ll } Estimation Estimate with optim. # input data X = as.matrix(cbind(1, admission[, 2:4])) y = as.matrix(admission[, 1]) init = c(0, 0, 0, 0) # Can set tolerance really low to duplicate glm result result_mle = optim( par = init, fn = probit_mle, X = X, y = y, control = list(maxit = 1000, reltol = 1e-12) ) # extract coefficients coefs_mle = result_mle$par Comparison glm_probit = glm( admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), control = list(maxit = 500, epsilon = 1e-8), data = admission ) summary(glm_probit) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), data = admission, control = list(maxit = 500, epsilon = 1e-08)) Deviance Residuals: Min 1Q Median 3Q Max -1.5626 -0.8920 -0.6403 1.1631 2.2097 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.0915039 0.6718360 -3.113 0.00185 ** gre 0.0013982 0.0006487 2.156 0.03112 * gpa 0.4643599 0.1950263 2.381 0.01727 * rank -0.3317117 0.0745524 -4.449 8.61e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.48 on 396 degrees of freedom AIC: 467.48 Number of Fisher Scoring iterations: 4 coefs_glm = coef(glm_probit) Compare. rbind(coefs_mle, coefs_glm) (Intercept) gre gpa rank coefs_mle -2.091510 0.001398222 0.4643609 -0.3317105 coefs_glm -2.091504 0.001398222 0.4643599 -0.3317117 EM for Latent Variable Approach to Probit Function em_probit = function( params, X, y, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are starting parameters (coefficients), model matrix, response, # tolerance, maximum iterations, and whether to show iterations #starting points b = params mu = X%*%b it = 0 converged = FALSE z = rnorm(length(y)) # z is the latent variable ~N(0,1) # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) # while no convergence and we haven&#39;t reached our max iterations do this stuff while ((!converged) &amp; (it &lt; maxits)) { z_old = z # create &#39;old&#39; values for comparison # E step create a new z based on current values z = ifelse( y == 1, mu + dnorm(mu) / pnorm(mu), mu - dnorm(mu) / pnorm(-mu) ) # M step estimate b b = solve(t(X)%*%X) %*% t(X)%*%z mu = X%*%b ll = sum(y * pnorm(mu, log.p = TRUE) + (1 - y) * pnorm(-mu, log.p = TRUE)) it = it + 1 if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(z_old - z)) &lt;= tol } # Show last iteration if (showits) cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list(b = t(b), ll = ll) } Estimation Use the same setup and starting values to estimate the parameters. # can lower tolerance to duplicate glm result result_em = em_probit( params = init, X = X, y = y, tol = 1e-12, maxit = 100 ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 51... # result_em coefs_em = result_em$b Comparison Compare all results. rbind(coefs_glm, coefs_mle, coefs_em) (Intercept) gre gpa rank coefs_glm -2.091504 0.001398222 0.4643599 -0.3317117 coefs_mle -2.091510 0.001398222 0.4643609 -0.3317105 admit -2.091504 0.001398222 0.4643599 -0.3317117 rbind(logLik(glm_probit), result$value, result_em$ll) [,1] [1,] -229.7404 [2,] 844.4590 [3,] -229.7404 Visualize Show estimates over niter iterations and visualize. X2 = X X2[, 2:3] = scale(X2[, 2:3]) niter = 20 result_em = map_df(1:niter, function(x) as_tibble( em_probit( params = init, X = X2, y = y, tol = 1e-8, maxit = x, showits = F )$b) ) gdat = result_em %&gt;% rowid_to_column(&#39;iter&#39;) %&gt;% pivot_longer(-iter, names_to = &#39;coef&#39;) %&gt;% mutate( coef = factor(coef, labels = c(&#39;Intercept&#39;, &#39;gre&#39;, &#39;gpa&#39;, &#39;rank&#39;)) ) %&gt;% arrange(iter, coef) ggplot(aes(x = iter, y = value), data = gdat) + geom_line(aes(group = coef, color = coef)) + theme_minimal() Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20probit%20example.R PCA The following is an EM algorithm for principal components analysis. See Murphy, 2012 Probabilistic Machine Learning 12.2.5. Some of the constructed object is based on output from pca function used below. Data Setup state.x77 is from base R, which includes various state demographics. We will first standardize the data. library(tidyverse) X = scale(state.x77) Function em_pca = function( X, nComp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # nComp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # starting points and other initializations N = nrow(X) D = ncol(X) L = nComp Xt = t(X) Z = t(replicate(L, rnorm(N))) # latent variables W = replicate(L, rnorm(D)) # loadings it = 0 converged = FALSE if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) # while no convergence and we haven&#39;t reached our max iterations do this stuff while ((!converged) &amp; (it &lt; maxits)) { Z_old = Z # create &#39;old&#39; values for comparison Z = solve(t(W)%*%W) %*% crossprod(W, Xt) # E W = Xt%*%t(Z) %*% solve(tcrossprod(Z)) # M it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(Z_old-Z)) &lt;= tol } # calculate reconstruction error Xrecon = W %*% Z reconerr = sum((Xrecon - t(X))^2) # orthogonalize W = pracma::orth(W) # for orthonormal basis of W; pcaMethods package has also evs = eigen(cov(X %*% W)) evals = evs$values evecs = evs$vectors W = W %*% evecs Z = X %*% W if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, reconerr = reconerr, Xrecon = t(Xrecon) ) } Estimation results_pca = em_pca( X = X, nComp = 2, tol = 1e-12, maxit = 1000 ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... 60... 65... 70... 70... results_pca $scores [,1] [,2] Alabama -3.78988728 0.23477897 Alaska 1.05313550 -5.45617512 Arizona -0.86742876 -0.74506148 Arkansas -2.38177761 1.28834366 California -0.24138147 -3.50952277 Colorado 2.06218136 -0.50566387 Connecticut 1.89943583 0.24300645 Delaware 0.42478394 0.50791950 Florida -1.17212341 -1.13474136 Georgia -3.29417162 -0.10995684 Hawaii 0.48704129 -0.12526216 Idaho 1.42342916 0.61114319 Illinois -0.11896424 -1.28238783 Indiana 0.47120189 0.24520088 Iowa 2.32181208 0.53685609 Kansas 1.90151483 0.07719072 Kentucky -2.12935981 1.06425233 Louisiana -4.24100842 0.34630079 Maine 0.96019374 1.70241922 Maryland 0.20342599 -0.38881112 Massachusetts 1.19589376 0.21865625 Michigan -0.18186944 -0.84711636 Minnesota 2.43361605 0.36533543 Mississippi -4.03208863 1.05124066 Missouri -0.31125449 0.14830589 Montana 1.37887297 0.03353877 Nebraska 2.18101665 0.54774825 Nevada 1.12708455 -1.13291366 New Hampshire 1.67128925 1.31239813 New Jersey 0.64958222 -0.28146986 New Mexico -1.32244692 0.29357041 New York -1.05034998 -1.89371072 North Carolina -2.69433377 0.51713890 North Dakota 2.41766786 0.78192203 Ohio 0.26795708 -0.41685336 Oklahoma -0.07391320 0.64658337 Oregon 1.32472856 -0.22767511 Pennsylvania -0.07738173 -0.26940938 Rhode Island 0.74084731 1.46130325 South Carolina -3.71100631 0.90984427 South Dakota 2.01253414 1.31509491 Tennessee -2.21813394 0.65102504 Texas -2.41364282 -2.32744119 Utah 2.26283736 0.53433138 Vermont 1.36926611 1.50938322 Virginia -0.99354796 -0.18457034 Washington 1.34001299 -0.51154448 West Virginia -1.50662213 1.60198375 Wisconsin 1.75754046 0.63572738 Wyoming 1.48379101 -0.04225606 $loadings [,1] [,2] [1,] -0.12642809 -0.41087417 [2,] 0.29882991 -0.51897884 [3,] -0.46766917 -0.05296872 [4,] 0.41161037 0.08165611 [5,] -0.44425672 -0.30694934 [6,] 0.42468442 -0.29876662 [7,] 0.35741244 0.15358409 [8,] 0.03338461 -0.58762446 $reconerr [1] 135.6901 $Xrecon Population Income Illiteracy Life Exp Murder HS Grad Frost Area Alabama 0.38268358 -1.25437699 1.759977489 -1.54078578 1.611617628 -1.67965020 -1.31849456 -0.26448579 Alaska 2.10865553 3.14634780 -0.203512400 -0.01204852 1.206906839 2.07737324 -0.46157798 3.24134051 Arizona 0.41579388 0.12745748 0.445134639 -0.41788150 0.614057185 -0.14578398 -0.42445943 0.40885758 Arkansas -0.22822355 -1.38036948 1.045642041 -0.87516325 0.662664464 -1.39641793 -0.65340786 -0.83657698 California 1.47248966 1.74923604 0.298781592 -0.38592908 1.184481039 0.94601731 -0.62527962 2.05422301 Colorado -0.05295342 0.87867032 -0.937634273 0.80752470 -0.760924727 1.02685178 0.65938735 0.36598559 Connecticut -0.33998711 0.44149303 -0.901179312 0.80167045 -0.918427793 0.73405859 0.71620393 -0.07938460 Delaware -0.26239562 -0.13666132 -0.225562197 0.21632020 -0.344618674 0.02864973 0.22983142 -0.28428468 Florida 0.61442524 0.23864122 0.608271775 -0.57511671 0.869031809 -0.15875971 -0.59320972 0.62767090 Georgia 0.46165424 -0.92733174 1.546406772 -1.36489386 1.497209045 -1.36613193 -1.19426555 -0.04536132 Hawaii -0.01010871 0.21055091 -0.221139217 0.19024283 -0.177922225 0.24426300 0.15483634 0.08986680 Idaho -0.43106438 0.10819283 -0.698065403 0.63580178 -0.819957964 0.42191900 0.60261317 -0.31160205 Illinois 0.54194046 0.62998207 0.123562347 -0.15368171 0.446478763 0.33261242 -0.23947367 0.74959089 Indiana -0.16031986 0.01355515 -0.233354573 0.21397374 -0.284598854 0.12685427 0.20607238 -0.12835514 Iowa -0.51412256 0.41520995 -1.114276503 0.99951952 -1.196268234 0.82564274 0.91229709 -0.23795697 Kansas -0.27212055 0.52816916 -0.893368551 0.78898632 -0.868454375 0.78448171 0.69148033 0.01812218 Kentucky -0.16806291 -1.18864084 0.939463847 -0.78956388 0.619310844 -1.22226901 -0.59760746 -0.69646856 Louisiana 0.39389653 -1.44706295 1.965045766 -1.71736548 1.777799672 -1.90455332 -1.46260289 -0.34507925 Maine -0.82087554 -0.59658494 -0.539227972 0.53423863 -0.949128976 -0.10084671 0.60464971 -0.96832748 Maryland 0.13403369 0.26257452 -0.074541237 0.05198345 0.028971956 0.20255563 0.01299178 0.23526623 Massachusetts -0.24103476 0.24389086 -0.570864580 0.51009689 -0.598400225 0.44255026 0.46100943 -0.08856331 Michigan 0.37105164 0.38528744 0.129925395 -0.14403157 0.340818527 0.17585298 -0.19510600 0.49171466 Minnesota -0.45778431 0.53763591 -1.157478542 1.03153348 -1.193289744 0.92436879 0.92591437 -0.13343470 Mississippi 0.07784161 -1.75048034 1.830000664 -1.57380929 1.468604826 -2.02644084 -1.27966480 -0.75234446 Missouri -0.02158375 -0.16997977 0.137708556 -0.11600550 0.092754503 -0.17649378 -0.08846880 -0.09753928 Montana -0.18810848 0.39464258 -0.646632879 0.57029706 -0.622868279 0.57556560 0.49797738 0.02632494 Nebraska -0.50079737 0.36748326 -1.049007763 0.94245607 -1.137062259 0.76259490 0.86364791 -0.24905787 Nevada 0.32298982 0.92476479 -0.467093711 0.37141038 -0.152967781 0.81713204 0.22883653 0.70335507 New Hampshire -0.75052840 -0.18167564 -0.851126498 0.79508531 -1.145321214 0.31766975 0.79890305 -0.71540190 New Jersey 0.03352326 0.34019150 -0.288880479 0.24439105 -0.202184277 0.35996125 0.18893948 0.18708463 New Mexico 0.04657394 -0.54754352 0.602917601 -0.52036105 0.497394682 -0.64933164 -0.42757124 -0.21665853 New York 0.91087056 0.66891979 0.591523732 -0.58696799 1.047898292 0.11971028 -0.66625200 1.07772522 North Carolina 0.12816045 -1.07353166 1.232664646 -1.06678818 1.038240427 -1.29874542 -0.88356411 -0.39383276 North Dakota -0.62693269 0.31667048 -1.172086124 1.05898588 -1.314075638 0.79313367 0.98419537 -0.37876361 Ohio 0.13739698 0.29641166 -0.103235078 0.07625529 0.008911129 0.23833907 0.03174915 0.25389887 Oklahoma -0.25631970 -0.35765056 0.000318234 0.02237404 -0.165631902 -0.22456731 0.07288742 -0.38241577 Oregon -0.07393708 0.51402708 -0.607475042 0.52668095 -0.518634834 0.63061330 0.43850719 0.17801302 Pennsylvania 0.12047658 0.11669379 0.050459318 -0.05385004 0.117072383 0.04762771 -0.06903419 0.15572818 Rhode Island -0.69407567 -0.53699812 -0.423874803 0.42426477 -0.777672460 -0.12196232 0.48922098 -0.83396464 South Carolina 0.09534391 -1.58114961 1.687329949 -1.45319435 1.369363378 -1.84783766 -1.18662222 -0.65853727 South Dakota -0.79477937 -0.08110103 -1.010859058 0.93576546 -1.297749324 0.46178543 0.92128241 -0.70559427 Tennessee 0.01294506 -1.00071299 1.002868892 -0.85984677 0.785589193 -1.13651148 -0.69280158 -0.45660979 Texas 1.26143772 0.48662406 1.252067903 -1.18353020 1.786683571 -0.32967476 -1.22012392 1.28708285 Utah -0.50562916 0.39889681 -1.086562111 0.97503875 -1.169293358 0.80135139 0.89083103 -0.23844224 Vermont -0.79328027 -0.37416028 -0.720313637 0.68685449 -1.071609849 0.13055266 0.72121000 -0.84123808 Virginia 0.20144755 -0.20111375 0.474428203 -0.42402594 0.498044099 -0.36680088 -0.38345347 0.07528883 Washington 0.04076514 0.66591672 -0.599586905 0.50979252 -0.438291530 0.72191506 0.40037222 0.34533187 West Virginia -0.46773439 -1.28161942 0.619745694 -0.48932954 0.177599146 -1.11845822 -0.29244627 -0.99166284 Wisconsin -0.48340644 0.19527660 -0.855621146 0.77533290 -0.975935252 0.55646593 0.72580444 -0.31489415 Wyoming -0.17023094 0.46533114 -0.691685058 0.60729331 -0.646213653 0.64276763 0.52383551 0.07436648 Comparison Extract reconstructed values and loadings for comparison. Xrecon = results_pca$Xrecon loadings_em = results_pca$loadings scores_em = results_pca$scores Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) # install via BiocManager::install(&quot;pcaMethods&quot;) result_pcam = pca( X, nPcs = 2, method = &#39;svd&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(result_pcam) scores_pcam = scores(result_pcam) Compare loadings and scores. sum((abs(loadings_pcam) - abs(loadings_em))^2) [1] 1.520589e-24 abs(round(cbind(scores_pcam, scores_em), 2)) PC1 PC2 Alabama 3.79 0.23 3.79 0.23 Alaska 1.05 5.46 1.05 5.46 Arizona 0.87 0.75 0.87 0.75 Arkansas 2.38 1.29 2.38 1.29 California 0.24 3.51 0.24 3.51 Colorado 2.06 0.51 2.06 0.51 Connecticut 1.90 0.24 1.90 0.24 Delaware 0.42 0.51 0.42 0.51 Florida 1.17 1.13 1.17 1.13 Georgia 3.29 0.11 3.29 0.11 Hawaii 0.49 0.13 0.49 0.13 Idaho 1.42 0.61 1.42 0.61 Illinois 0.12 1.28 0.12 1.28 Indiana 0.47 0.25 0.47 0.25 Iowa 2.32 0.54 2.32 0.54 Kansas 1.90 0.08 1.90 0.08 Kentucky 2.13 1.06 2.13 1.06 Louisiana 4.24 0.35 4.24 0.35 Maine 0.96 1.70 0.96 1.70 Maryland 0.20 0.39 0.20 0.39 Massachusetts 1.20 0.22 1.20 0.22 Michigan 0.18 0.85 0.18 0.85 Minnesota 2.43 0.37 2.43 0.37 Mississippi 4.03 1.05 4.03 1.05 Missouri 0.31 0.15 0.31 0.15 Montana 1.38 0.03 1.38 0.03 Nebraska 2.18 0.55 2.18 0.55 Nevada 1.13 1.13 1.13 1.13 New Hampshire 1.67 1.31 1.67 1.31 New Jersey 0.65 0.28 0.65 0.28 New Mexico 1.32 0.29 1.32 0.29 New York 1.05 1.89 1.05 1.89 North Carolina 2.69 0.52 2.69 0.52 North Dakota 2.42 0.78 2.42 0.78 Ohio 0.27 0.42 0.27 0.42 Oklahoma 0.07 0.65 0.07 0.65 Oregon 1.32 0.23 1.32 0.23 Pennsylvania 0.08 0.27 0.08 0.27 Rhode Island 0.74 1.46 0.74 1.46 South Carolina 3.71 0.91 3.71 0.91 South Dakota 2.01 1.32 2.01 1.32 Tennessee 2.22 0.65 2.22 0.65 Texas 2.41 2.33 2.41 2.33 Utah 2.26 0.53 2.26 0.53 Vermont 1.37 1.51 1.37 1.51 Virginia 0.99 0.18 0.99 0.18 Washington 1.34 0.51 1.34 0.51 West Virginia 1.51 1.60 1.51 1.60 Wisconsin 1.76 0.64 1.76 0.64 Wyoming 1.48 0.04 1.48 0.04 Calculate mean squared reconstruction error and compare. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon - X)^2) [1] 0.3392252 mean((Xrecon_pcam - X)^2) [1] 0.3392252 mean(abs(Xrecon_pcam - Xrecon)) [1] 5.120166e-13 Visualize qplot(Xrecon_pcam[,1], X[,1]) qplot(Xrecon_pcam[,2], X[,2]) qplot(Xrecon[,1], Xrecon_pcam[,1]) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20for%20pca.R Probabilistic PCA The following is an EM algorithm for probabilistic principal components analysis. Based on Tipping and Bishop, 1999, and also Murphy 2012 Probabilistic ML, with some code snippets inspired by the ppca function used below. See also ModelFitting/EM Examples/EM for pca.R Data Setup state.x77 is from base R, which includes various state demographics. We will first standardize the data. library(tidyverse) X = scale(state.x77) Function em_ppca = function( X, nComp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # nComp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # require(pracma) tr = function(x) sum(diag(x), na.rm = TRUE) # starting points and other initializations N = nrow(X) D = ncol(X) L = nComp S = (1/N) * t(X)%*%X evals = eigen(S)$values evecs = eigen(S)$vectors V = evecs[,1:L] Lambda = diag(evals[1:L]) # latent variables Z = t(replicate(L, rnorm(N))) # variance; average variance associated with discarded dimensions sigma2 = 1/(D - L) * sum(evals[(L+1):D]) # loadings; this and sigma2 starting points will be near final estimate W = V %*% chol(Lambda - sigma2 * diag(L)) %*% diag(L) it = 0 converged = FALSE ll = 0 # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { # create &#39;old&#39; values for comparison if(exists(&#39;W_new&#39;)){ W_old = W_new } else { W_old = W } ll_old = ll Psi = sigma2*diag(L) M = t(W_old) %*% W_old + Psi # E and M W_new = S %*% W_old %*% solve( Psi + solve(M) %*% t(W_old) %*% S %*% W_old ) sigma2 = 1/D * tr(S - S %*% W_old %*% solve(M) %*% t(W_new)) Z = solve(M) %*% t(W_new) %*% t(X) ZZ = sigma2*solve(M) + Z%*%t(Z) # log likelihood as in paper # ll = .5*sigma2*D + .5*tr(ZZ) + .5*sigma2 * X%*%t(X) - # 1/sigma2 * t(Z)%*%t(W_new)%*%t(X) + .5*sigma2 * tr(t(W_new)%*%W_new%*%ZZ) # ll = -sum(ll) # more straightforward ll = dnorm(X, mean = t(W_new %*% Z), sd = sqrt(sigma2), log = TRUE) ll = -sum(ll) it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(ll_old-ll)) &lt;= tol } W = pracma::orth(W_new) # for orthonormal basis of W; pcaMethods package has also evs = eigen(cov(X %*% W)) evecs = evs$vectors W = W %*% evecs Z = X %*% W Xrecon = Z %*% t(W) reconerr = sum((Xrecon - X)^2) if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, Xrecon = Xrecon, reconerr = reconerr, ll = ll, sigma2 = sigma2 ) } Estimation results_ppca = em_ppca( X = X, nComp = 2, tol = 1e-12, maxit = 100 ) Iterations of EM: 1... 2... str(results_ppca) List of 6 $ scores : num [1:50, 1:2] 3.79 -1.053 0.867 2.382 0.241 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings: num [1:8, 1:2] 0.126 -0.299 0.468 -0.412 0.444 ... $ Xrecon : num [1:50, 1:8] 0.383 2.109 0.416 -0.228 1.472 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ reconerr: num 136 $ ll : num 369 $ sigma2 : num 0.452 Comparison Extract reconstructed values and loadings for comparison. Xrecon = results_ppca$Xrecon loadings_em = results_ppca$loadings scores_em = results_ppca$scores Compare to standard pca on full data set if desired. standard_pca = princomp(scale(state.x77)) scores_standard_pca = standard_pca$scores[,1:2] loadings_standard_pca = standard_pca$loadings[,1:2] Xrecon_standard_pca = scores_standard_pca%*%t(loadings_standard_pca) Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) results_pcam = pca( X, nPcs = 2, threshold = 1e-8, method = &#39;ppca&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(results_pcam) scores_pcam = scores(results_pcam) Compare loadings and scores. round(cbind(loadings_pcam, loadings_em, loadings_standard_pca), 3) PC1 PC2 Comp.1 Comp.2 Population 0.126 0.411 0.126 0.411 0.126 0.411 Income -0.299 0.519 -0.299 0.519 -0.299 0.519 Illiteracy 0.468 0.053 0.468 0.053 0.468 0.053 Life Exp -0.412 -0.082 -0.412 -0.082 -0.412 -0.082 Murder 0.444 0.307 0.444 0.307 0.444 0.307 HS Grad -0.425 0.299 -0.425 0.299 -0.425 0.299 Frost -0.357 -0.154 -0.357 -0.154 -0.357 -0.154 Area -0.033 0.588 -0.033 0.588 -0.033 0.588 sum((abs(loadings_pcam) - abs(loadings_em)) ^ 2) [1] 3.572549e-16 round(cbind(abs(scores_pcam), abs(scores_em)), 2) PC1 PC2 Alabama 3.79 0.23 3.79 0.23 Alaska 1.05 5.46 1.05 5.46 Arizona 0.87 0.75 0.87 0.75 Arkansas 2.38 1.29 2.38 1.29 California 0.24 3.51 0.24 3.51 Colorado 2.06 0.51 2.06 0.51 Connecticut 1.90 0.24 1.90 0.24 Delaware 0.42 0.51 0.42 0.51 Florida 1.17 1.13 1.17 1.13 Georgia 3.29 0.11 3.29 0.11 Hawaii 0.49 0.13 0.49 0.13 Idaho 1.42 0.61 1.42 0.61 Illinois 0.12 1.28 0.12 1.28 Indiana 0.47 0.25 0.47 0.25 Iowa 2.32 0.54 2.32 0.54 Kansas 1.90 0.08 1.90 0.08 Kentucky 2.13 1.06 2.13 1.06 Louisiana 4.24 0.35 4.24 0.35 Maine 0.96 1.70 0.96 1.70 Maryland 0.20 0.39 0.20 0.39 Massachusetts 1.20 0.22 1.20 0.22 Michigan 0.18 0.85 0.18 0.85 Minnesota 2.43 0.37 2.43 0.37 Mississippi 4.03 1.05 4.03 1.05 Missouri 0.31 0.15 0.31 0.15 Montana 1.38 0.03 1.38 0.03 Nebraska 2.18 0.55 2.18 0.55 Nevada 1.13 1.13 1.13 1.13 New Hampshire 1.67 1.31 1.67 1.31 New Jersey 0.65 0.28 0.65 0.28 New Mexico 1.32 0.29 1.32 0.29 New York 1.05 1.89 1.05 1.89 North Carolina 2.69 0.52 2.69 0.52 North Dakota 2.42 0.78 2.42 0.78 Ohio 0.27 0.42 0.27 0.42 Oklahoma 0.07 0.65 0.07 0.65 Oregon 1.32 0.23 1.32 0.23 Pennsylvania 0.08 0.27 0.08 0.27 Rhode Island 0.74 1.46 0.74 1.46 South Carolina 3.71 0.91 3.71 0.91 South Dakota 2.01 1.32 2.01 1.32 Tennessee 2.22 0.65 2.22 0.65 Texas 2.41 2.33 2.41 2.33 Utah 2.26 0.53 2.26 0.53 Vermont 1.37 1.51 1.37 1.51 Virginia 0.99 0.18 0.99 0.18 Washington 1.34 0.51 1.34 0.51 West Virginia 1.51 1.60 1.51 1.60 Wisconsin 1.76 0.64 1.76 0.64 Wyoming 1.48 0.04 1.48 0.04 Compare reconstructed data sets. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_pcam - X)^2) [1] 0.3392252 mean(abs(Xrecon_pcam - Xrecon)) [1] 6.414878e-09 mean(abs(Xrecon_pcam - Xrecon)) [1] 6.414878e-09 Visualize GGally::ggpairs(data.frame( data = X[, 1], custom = Xrecon[, 1], pcaMeth = Xrecon_pcam[, 1] )) GGally::ggpairs(data.frame( data = X[, 2], custom = Xrecon[, 2], pcaMeth = Xrecon_pcam[, 2] )) qplot(Xrecon[, 1], Xrecon_pcam[, 1]) GGally::ggpairs(data.frame(scores_em, scores_pcam) ) Missing Data Example A slightly revised approach can be taken in the case of missing value. Data Setup # create some missing values set.seed(123) X_miss = X NAindex = sample(length(X), 20) X_miss[NAindex] = NA Function em_ppca_miss = function(X, nComp=2, tol=.00001, maxits=100, showits=T){ # Arguments # X: numeric data # nComp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # require(pracma) # for orthonormal basis of W; pcaMethods package has also tr = function(x) sum(diag(x), na.rm = TRUE) # starting points and other initializations X_orig = X X = X N = nrow(X_orig) D = ncol(X_orig) L = nComp NAs = is.na(X_orig) X[NAs] = 0 S = (1/N) * t(X)%*%X evals = eigen(S)$values evecs = eigen(S)$vectors V = evecs[,1:L] Lambda = diag(evals[1:L]) # latent variables Z = t(replicate(L, rnorm(N))) # variance; average variance associated with discarded dimensions sigma2 = 1/(D-L) * sum(evals[(L+1):D]) # loadings W = V %*% chol(Lambda-sigma2*diag(L)) %*% diag(L) it = 0 converged = FALSE ll = 0 # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { if(exists(&#39;W_new&#39;)){ W_old = W_new } else { W_old = W } ll_old = ll # deal with missingness via projection proj = t(W_old%*%Z) X_new = X_orig X_new[NAs] = proj[NAs] X = X_new Psi = sigma2*diag(L) M = t(W_old) %*% W_old + Psi # E and M W_new = S %*% W_old %*% solve( Psi + solve(M)%*%t(W_old)%*%S%*%W_old ) sigma2 = 1/D * tr(S - S%*%W_old%*%solve(M)%*%t(W_new)) Z = solve(M)%*%t(W_new)%*%t(X) # log likelihood as in paper # ZZ = sigma2*solve(M) + Z%*%t(Z) # ll = .5*sigma2*D + .5*tr(ZZ) + .5*sigma2 * X%*%t(X) - # 1/sigma2 * t(Z)%*%t(W_new)%*%t(X) + .5*sigma2 * tr(t(W_new)%*%W_new%*%ZZ) # ll = -sum(ll) # more straightforward ll = dnorm(X, mean = t(W_new %*% Z), sd = sqrt(sigma2), log = TRUE) ll = -sum(ll) it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(ll_old-ll)) &lt;= tol } W = pracma::orth(W_new) # for orthonormal basis of W evs = eigen(cov(X %*% W)) evecs = evs$vectors W = W %*% evecs Z = X %*% W Xrecon = Z %*% t(W) reconerr = sum((Xrecon-X)^2) if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, Xrecon = Xrecon, reconerr = reconerr, ll = ll, sigma2 = sigma2 ) } Estimation Run the PCA. results_ppca_miss = em_ppca_miss( X = X_miss, nComp = 2, tol = 1e-8, maxit = 100 ) Iterations of EM: 1... 5... 10... 15... 19... str(results_ppca_miss) List of 6 $ scores : num [1:50, 1:2] 3.79 -1.07 0.86 2.35 0.24 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings: num [1:8, 1:2] 0.133 -0.299 0.422 -0.431 0.464 ... $ Xrecon : num [1:50, 1:8] 0.414 1.998 0.448 -0.2 1.521 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ reconerr: num 130 $ ll : num 368 $ sigma2 : num 0.475 Comparison Extract reconstructed values and loadings for comparison. Xrecon = results_ppca_miss$Xrecon loadings_em = results_ppca_miss$loadings scores_em = results_ppca_miss$scores Compare to standard pca on full data set if desired. standard_pca = princomp(scale(state.x77)) scores_standard_pca = standard_pca$scores[,1:2] loadings_standard_pca = standard_pca$loadings[,1:2] Xrecon_standard_pca = scores_standard_pca%*%t(loadings_standard_pca) Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) results_pcam = pca( X_miss, nPcs = 2, threshold = 1e-8, method = &#39;ppca&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(results_pcam) scores_pcam = scores(results_pcam) Compare loadings and scores. round(cbind(loadings_pcam, loadings_em, loadings_standard_pca), 3) PC1 PC2 Comp.1 Comp.2 Population -0.128 -0.416 0.133 0.396 0.126 0.411 Income 0.305 -0.492 -0.299 0.537 -0.299 0.519 Illiteracy -0.434 -0.046 0.422 0.076 0.468 0.053 Life Exp 0.432 0.077 -0.431 -0.080 -0.412 -0.082 Murder -0.464 -0.284 0.464 0.290 0.444 0.307 HS Grad 0.424 -0.288 -0.433 0.318 -0.425 0.299 Frost 0.346 0.170 -0.353 -0.185 -0.357 -0.154 Area 0.041 -0.620 -0.037 0.569 -0.033 0.588 sum((abs(loadings_pcam) - abs(loadings_em)) ^ 2) [1] 0.00738241 round(cbind(abs(scores_pcam), abs(scores_em)), 2) PC1 PC2 Alabama 3.80 0.23 3.79 0.23 Alaska 1.08 5.49 1.07 5.41 Arizona 0.87 0.78 0.86 0.84 Arkansas 2.36 1.25 2.35 1.30 California 0.19 4.02 0.24 3.77 Colorado 1.47 0.40 1.47 0.41 Connecticut 1.94 0.29 1.94 0.19 Delaware 0.40 0.55 0.40 0.47 Florida 1.16 1.12 1.18 1.18 Georgia 3.31 0.10 3.31 0.10 Hawaii 0.60 0.10 0.61 0.29 Idaho 1.41 0.59 1.42 0.62 Illinois 0.16 1.23 0.18 1.24 Indiana 0.49 0.43 0.49 0.43 Iowa 2.33 0.53 2.33 0.53 Kansas 1.91 0.07 1.91 0.06 Kentucky 2.14 1.05 2.14 1.10 Louisiana 3.63 0.39 3.59 0.41 Maine 0.93 1.68 0.94 1.73 Maryland 0.18 0.32 0.16 0.41 Massachusetts 1.22 0.23 1.23 0.17 Michigan 0.22 0.81 0.24 0.80 Minnesota 2.45 0.35 2.45 0.38 Mississippi 4.05 1.30 4.03 1.24 Missouri 0.35 0.14 0.36 0.19 Montana 1.35 0.01 1.35 0.07 Nebraska 2.19 0.53 2.20 0.54 Nevada 0.47 1.32 0.46 1.39 New Hampshire 1.81 1.34 1.81 1.32 New Jersey 0.66 0.24 0.65 0.31 New Mexico 1.29 0.28 1.26 0.29 New York 1.06 1.86 1.08 1.87 North Carolina 2.71 0.51 2.71 0.55 North Dakota 2.45 0.79 2.45 0.79 Ohio 0.24 0.40 0.23 0.38 Oklahoma 0.06 0.61 0.06 0.64 Oregon 1.12 0.29 1.10 0.33 Pennsylvania 0.07 0.63 0.09 0.51 Rhode Island 0.78 1.46 0.79 1.43 South Carolina 3.71 0.90 3.69 0.88 South Dakota 2.22 0.94 2.20 1.02 Tennessee 2.22 0.64 2.22 0.67 Texas 2.35 2.41 2.37 2.30 Utah 2.41 0.60 2.42 0.62 Vermont 1.34 1.52 1.36 1.54 Virginia 1.00 0.16 1.01 0.19 Washington 1.28 0.54 1.26 0.61 West Virginia 1.59 1.54 1.58 1.59 Wisconsin 1.96 0.46 1.96 0.47 Wyoming 1.43 0.01 1.45 0.02 Compare reconstructed data sets. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_pcam - X_miss)^2) [1] NA mean(abs(Xrecon_pcam - Xrecon)) [1] 0.02893291 mean(abs(Xrecon_pcam - Xrecon)) [1] 0.02893291 Visualize GGally::ggpairs(data.frame( data = X_miss[, 1], custom = Xrecon[, 1], pcaMeth = Xrecon_pcam[, 1] )) GGally::ggpairs(data.frame( data = X_miss[, 2], custom = Xrecon[, 2], pcaMeth = Xrecon_pcam[, 2] )) qplot(Xrecon[, 1], Xrecon_pcam[, 1]) GGally::ggpairs(data.frame(scores_em, scores_pcam) ) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20ppca.R Original code for the missing example found at (https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20ppca%20with%20missing.R State Space Model The following regards chapter 11 in Statistical Modeling and Computation, the first example for an unobserved components model. The data regards inflation based on the U.S. consumer price index (infl = 400*log(cpi_t/cpi_{t-1})), from the second quarter of 1947 to the second quarter of 2011. You can acquire the data here or in Datasets repo. Just note that it has 2 mystery columns and one mystery row presumably supplied by Excel. You can also get the CPI data yourself at the Bureau of Labor Statistics in a frustrating fashion, or in a much easier fashion here. For the following I use n instead of t or T because those are transpose and TRUE in R. The model is basically y = τ + ϵ, with ϵ ~ N(0, σ^2), and τ = τ_{n-1} + υ_n with υ ~ N(0, ω^2). Thus each y is associated with a latent variable that follows a random walk over time. ω^2 serves as a smoothing parameter, which itself may be estimated but which is fixed in the following. See the text for more details. Data Setup library(tidyverse) d = read_csv( &#39;https://raw.githubusercontent.com/m-clark/Datasets/master/us%20cpi/USCPI.csv&#39;, col_names = FALSE ) inflation = as.matrix(d$X1) summary(inflation) V1 Min. :-9.557 1st Qu.: 1.843 Median : 3.248 Mean : 3.634 3rd Qu.: 4.819 Max. :15.931 Function EM function for a state space model. em_state_space = function( params, y, omega2_0, omega2, tol = .00001, maxits = 100, showits = FALSE ) { # Arguments are # params: starting parameters (variance as &#39;sigma2&#39;), # y: data, # tol: tolerance, # omega2: latent variance (2_0) is a noisier starting variance # maxits: maximum iterations # showits: whether to show iterations # Not really needed here, but would be a good idea generally to take advantage # of sparse representation for large data # require(spam) # see usage below # Starting points n = length(y) sigma2 = params$sigma2 # Other initializations H = diag(n) for (i in 1:(ncol(H) - 1)) { H[i + 1, i] = -1 } Omega2 = spam::as.spam(diag(omega2, n)) Omega2[1, 1] = omega2_0 H = spam::as.spam(H) HinvOmega2H = t(H) %*% spam::chol2inv(spam::chol(Omega2)) %*% H # tau ~ N(0, HinvOmmega2H^-1) it = 0 converged = FALSE if (showits) # Show iterations cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { sigma2Old = sigma2[1] Sigma2invOld = diag(n)/sigma2Old K = HinvOmega2H + Sigma2invOld # E tau = solve(K, y/sigma2Old) # tau|y, sigma2_{n-1}, omega2 ~ N(0, K^-1) K_inv_tr = sum(1/eigen(K)$values) sigma2 = 1/n * (K_inv_tr + crossprod(y-tau)) # M converged = max(abs(sigma2 - sigma2Old)) &lt;= tol it = it + 1 # if showits true, &amp; it =1 or divisible by 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) } Kfinal = HinvOmega2H + diag(n) / sigma2[1] taufinal = solve(K, (y / sigma2[1])) list(sigma2 = sigma2, tau = taufinal) } Estimation ss_mod_1 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = 1^2 ) 5... 10... 15... 20... 25... 30... ss_mod_.5 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = .5^2 ) 5... 10... 15... 20... # more smooth ss_mod_.1 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = .1^2 ) 5... 10... ss_mod_1$sigma2 [,1] [1,] 2.765182 ss_mod_.5$sigma2 [,1] [1,] 4.404707 ss_mod_.1$sigma2 [,1] [1,] 7.489429 Visualization library(lubridate) series = ymd( paste0( rep(1947:2014, e = 4), &#39;-&#39;, c(&#39;01&#39;, &#39;04&#39;, &#39;07&#39;, &#39;10&#39;), &#39;-&#39;, &#39;01&#39;) ) The following corresponds to Fig. 11.1 in the text. library(tidyverse) data.frame( series = series[1:length(inflation)], inflation = inflation, Mod_1 = ss_mod_1$tau, Mod_.5 = ss_mod_.5$tau, Mod_.1 = ss_mod_.1$tau ) %&gt;% ggplot(aes(x = series, y = inflation)) + geom_point(color = &#39;gray50&#39;) + geom_line(aes(y = Mod_1), color = &#39;#ff5500&#39;) + geom_line(aes(y = Mod_.5), color = &#39;skyblue3&#39;) + geom_line(aes(y = Mod_.1), color = &#39;#00aaff&#39;) + geom_smooth(formula = y ~ s(x), # compare to generalized additive model (thicker line) se = FALSE, method = &#39;gam&#39;) + scale_x_date(date_breaks = &#39;10 years&#39;) + theme_minimal() Source Original code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20for%20state%20space%20unobserved%20components.R "],["supplemental.html", "Supplemental Other Languages", " Supplemental Other Languages When doing some of these models and algorithms, I had some other code to work with in another language, or, at the time, just wanted to try it in that language. Not much here but may be useful to some. Julia One-factor Mixed Model Two-factor Mixed Model Matlab One-factor Mixed Model Two-factor Mixed Model Algorithms Python Nelder-Mead HMM "]]
