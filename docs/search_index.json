[["index.html", "Model Estimation by Example Demonstrations with R", " Model Estimation by Example Demonstrations with R Michael Clark m-clark.github.io "],["introduction.html", "Introduction", " Introduction This document provides ‘by-hand’ demonstrations of various models and algorithms. The goal is to take away some of the mystery of them by providing clean code examples that are easy to run and compare with other tools. The code was collected over several years, so is not exactly consistent in style, but now has been cleaned up to make it more so. Within each demo, you will generally find some imported/simulated data, a primary estimating function, a comparison of results with some R package, and a link to the old code that was the initial demonstration. The document has several sections: Models: More or less standard/traditional statistical models and more Bayesian: Demonstrations of models using Stan Estimation: Algorithms used in model estimation (e.g. penalized likelihood, stochastic gradient descent) Supplemental: A handful of examples in languages other than R, possibly other miscellany Note that this code is not meant to be extensive, or used in production, and in fact, some of these demos would probably be considered of historical interest only. To be clear, almost everything here has a package/module that would do the same thing far better and efficiently. Note also, the document itself is also not intended to be an introduction to any of these methods, and in fact sometimes contains very little expository text, assuming the reader has some familiarity with the model/approach and possibly some reference text. This document is just a learning tool for those with some background in place, but who want to dive a little deeper. Note the following color coding used in this document: emphasis package function object/class link (with hover underline) Many examples require some initial data processing or visualization via ggplot2, so it’s assumed the tidyverse set of packages is loaded for all demonstrations. While I’m happy to fix any glaring errors and broken links, this is pretty much a completed document for the time being, except on the off chance I add a demo on rare occasion, or perhaps non-R code. This code has accumulated over years, and I just wanted it in a nicer format, which has been accomplished. Perhaps if others would like to add to it via pull requests, I would do so. The original code for these demonstrations may be found at their first home here: https://github.com/m-clark/Miscellaneous-R-Code. Last updated: 2020-12-16. "],["linear-regression.html", "Linear Regression Data Setup Functions Estimation Comparison Python Source", " Linear Regression We start our demonstrations with a standard regression model via maximum likelihood or least squares loss. Also included are examples for QR decomposition and normal equations. This can serve as an entry point for those starting out in the wider world of computational statistics, as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Least squares loss is not confined to the standard regression setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. You can find a Python version of the code in the supplemental section. Data Setup We will simulate some data to make our results known and easier to manipulate. library(tidyverse) set.seed(123) # ensures replication # predictors and target N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X, y) Functions A maximum likelihood approach. lm_ml &lt;- function(par, X, y) { # par: parameters to be estimated # X: predictor matrix with intercept column # y: target # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } An approach via least squares loss function. lm_ls &lt;- function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: target # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link # calculate least squares loss function L = crossprod(y - mu) } Estimation Setup for use with optim. X = cbind(1, X) Initial values. Note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach as it is the objective function. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) fit_ML = optim( par = init, fn = lm_ml, X = X, y = y, control = list(reltol = 1e-8) ) fit_LS = optim( par = init[-1], fn = lm_ls, X = X, y = y, control = list(reltol = 1e-8) ) pars_ML = fit_ML$par pars_LS = c(sigma2 = fit_LS$value / (N-k-1), fit_LS$par) # calculate sigma2 and add Comparison Compare to lm which uses QR decomposition. fit_lm = lm(y ~ ., dfXy) Example of QR. Not shown. # QRX = qr(X) # Q = qr.Q(QRX) # R = qr.R(QRX) # Bhat = solve(R) %*% crossprod(Q, y) # alternate: qr.coef(QRX, y) sigma2 intercept b1 b2 fit_ML 0.219 -0.432 0.133 0.112 fit_LS 0.226 -0.432 0.133 0.112 fit_lm 0.226 -0.432 0.133 0.112 The slight difference in sigma is roughly dividing by N vs. N-k-1 in the traditional least squares approach. It diminishes with increasing N as both tend toward whatever sd^2 you specify when creating the y target above. Compare to glm, which by default assumes gaussian family with identity link and uses lm.fit. fit_glm = glm(y ~ ., data = dfXy) summary(fit_glm) Call: glm(formula = y ~ ., data = dfXy) Deviance Residuals: Min 1Q Median 3Q Max -0.93651 -0.33037 -0.06222 0.31068 1.03991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.43247 0.04807 -8.997 1.97e-14 *** X1 0.13341 0.05243 2.544 0.0125 * X2 0.11191 0.04950 2.261 0.0260 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.2262419) Null deviance: 24.444 on 99 degrees of freedom Residual deviance: 21.945 on 97 degrees of freedom AIC: 140.13 Number of Fisher Scoring iterations: 2 Via normal equations. coefs = solve(t(X) %*% X) %*% t(X) %*% y # coefficients Compare. sqrt(crossprod(y - X %*% coefs) / (N - k - 1)) summary(fit_lm)$sigma sqrt(fit_glm$deviance / fit_glm$df.residual) c(sqrt(pars_ML[1]), sqrt(pars_LS[1])) # rerun by adding 3-4 zeros to the N [,1] [1,] 0.4756489 [1] 0.4756489 [1] 0.4756489 sigma2 sigma2 0.4684616 0.4756490 Python The above is available as a Python demo in the supplemental section. Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_lm.R "],["logistic-regression.html", "Logistic Regression Data Setup Functions Estimation Comparison Python Source", " Logistic Regression The following demo regards a standard logistic regression model via maximum likelihood or exponential loss. This can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Exponential loss is not confined to the standard GLM setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. This follows the linear regression model approach. Data Setup Predictors and target. This follows the same approach as the linear regression example, but now draws the target variable from the binomial distribution with size = 1. library(tidyverse) set.seed(1235) # ensures replication N = 10000 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) # the linear predictor lp = -.5 + .2*X[, 1] + .5*X[, 2] # increasing N will get estimated values closer to these y = rbinom(N, size = 1, prob = plogis(lp)) dfXy = data.frame(X, y) Functions A maximum likelihood approach. logreg_ml &lt;- function(par, X, y) { # Arguments # par: parameters to be estimated # X: predictor matrix with intercept column # y: target # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = plogis(LP) # logit link # calculate likelihood L = dbinom(y, size = 1, prob = mu, log = TRUE) # log likelihood # L = y*log(mu) + (1 - y)*log(1-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } Another approach via exponential loss function. logreg_exp &lt;- function(par, X, y) { # Arguments # par: parameters to be estimated # X: predictor matrix with intercept column # y: target # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor # calculate exponential loss function (convert y to -1:1 from 0:1) L = sum(exp(-ifelse(y, 1, -1) * .5 * LP)) } Estimation Setup for use with optim. X = cbind(1, X) # initial values init = rep(0, ncol(X)) names(init) = c(&#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) fit_ml = optim( par = init, fn = logreg_ml, X = X, y = y, control = list(reltol = 1e-8) ) fit_exp = optim( par = init, fn = logreg_exp, X = X, y = y, control = list(reltol = 1e-15) ) pars_ml = fit_ml$par pars_exp = fit_exp$par Comparison Compare to glm. fit_glm = glm(y ~ ., dfXy, family = binomial) intercept b1 b2 fit_ml -0.504 0.248 0.503 fit_exp -0.503 0.245 0.501 fit_glm -0.504 0.248 0.503 Python The above is available as a Python demo in the supplemental section. Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_logistic.R "],["mixed-models.html", "Mixed Models One-factor Mixed Model Two-factor Mixed Model Mixed Model via ML", " Mixed Models This chapter regards mixed models, a flexible expansion of standard regression methods to account for some covariance among the observations. One-factor Mixed Model The following is an approach for one factor random effects model via maximum likelihood in R (and Matlab and Julia in the Supplemental Section). It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. Unfortunately I did this before knowing they had both Matlab and R code on their website, though the R code here is a little cleaner and has comments. Data Setup The data regards crop yield from 10 randomly selected locations and three collections at each location. library(tidyverse) y = matrix(c(22.6, 20.5, 20.8, 22.6, 21.2, 20.5, 17.3, 16.2, 16.6, 21.4, 23.7, 23.2, 20.9, 22.2, 22.6, 14.5, 10.5, 12.3, 20.8, 19.1, 21.3, 17.4, 18.6, 18.6, 25.1, 24.8, 24.9, 14.9, 16.3, 16.6), 10, 3, byrow = TRUE) Function The estimating function. one_factor_re &lt;- function(mu, sigma2_mu, sigma2){ # Args # mu: intercept # sigma2_mu: variance of intercept # sigma2: residual variance of y # I follow their notation for consistency d = nrow(y) ni = ncol(y) # covariance matrix of observations Sigmai = sigma2 * diag(ni) + sigma2_mu * matrix(1, ni, ni) # log likelihood l = rep(NA, 10) # iterate over the rows for(i in 1:d){ l[i] = .5 * t(y[i, ] - mu) %*% chol2inv(chol(Sigmai)) %*% (y[i, ] - mu) } ll = -(ni*d) / 2*log(2*pi) - d / 2*log(det(Sigmai)) - sum(l) return(-ll) } Estimation Starting values. starts = list( mu = mean(y), sigma2_mu = var(rowMeans(y)), sigma2 = mean(apply(y, 1, var)) ) Estimate at the starting values. one_factor_re(mu = starts[[1]], sigma2_mu = starts[[2]], sigma2 = starts[[3]]) [1] 62.30661 Package bbmle has an mle2 function for maximum likelihood estimation based on underlying R functions like optim, and produces a nice summary table. LBFGS-B is used to place lower bounds on the variance estimates. library(bbmle) fit_mle = mle2( one_factor_re , start = starts, method = &#39;L-BFGS-B&#39;, lower = c( mu = -Inf, sigma2_mu = 0, sigma2 = 0 ), trace = TRUE ) Comparison We can compare to the lme4 model result. library(lme4) library(tidyverse) d = data.frame(y) %&gt;% pivot_longer(everything(), names_to = &#39;x&#39;, values_to = &#39;value&#39;) %&gt;% arrange(x) %&gt;% group_by(x) %&gt;% mutate(group = 1:n()) fit_mer = lmer(value ~ 1 | group, data = d, REML = FALSE) summary(fit_mle) Maximum likelihood estimation Call: mle2(minuslogl = one_factor_re, start = starts, method = &quot;L-BFGS-B&quot;, trace = TRUE, lower = c(mu = -Inf, sigma2_mu = 0, sigma2 = 0)) Coefficients: Estimate Std. Error z value Pr(z) mu 19.60000 1.12173 17.4729 &lt; 2.2e-16 *** sigma2_mu 12.19400 5.62858 2.1664 0.030277 * sigma2 1.16667 0.36893 3.1623 0.001565 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: 124.5288 summary(fit_mer) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: value ~ 1 | group Data: d AIC BIC logLik deviance df.resid 130.5 134.7 -62.3 124.5 27 Scaled residuals: Min 1Q Median 3Q Max -1.9950 -0.6555 0.1782 0.4870 1.7083 Random effects: Groups Name Variance Std.Dev. group (Intercept) 12.194 3.492 Residual 1.167 1.080 Number of obs: 30, groups: group, 10 Fixed effects: Estimate Std. Error t value (Intercept) 19.600 1.122 17.47 -2 * logLik(fit_mer) &#39;log Lik.&#39; 124.5288 (df=3) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/one_factor_RE.R Two-factor Mixed Model An approach for two factor random effects model via maximum likelihood in R Matlab and Julia. It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. See the previous chapter for a one factor model, and the Supplemental Section for the Matlab and Julia versions of this example. Note that the text has a typo on the sigma2 variance estimate (value should be .0023 not .023). Data Setup The data regards the breeding value of a set of five sires in raising pigs. Each sire is mated to a random group of dams, with the response being the average daily weight gain in pounds of two piglets in each litter. library(tidyverse) y = c(1.39,1.29,1.12,1.16,1.52,1.62,1.88,1.87,1.24,1.18, .95,.96,.82,.92,1.18,1.20,1.47,1.41,1.57,1.65) # for use in lme4, but also a more conceptual representation of the data d = expand.grid(sire = rep(1:5, 2), dam = 1:2) d = data.frame(d[order(d$sire), ], y) Function The function takes the log variances eta as input to keep positive. two_factor_re &lt;- function(mu, eta_alpha, eta_gamma, eta) { # Args # mu: intercept # eta_alpha: random effect one # eta_gamma: random effect two # eta: residual variance of y sigma2_alpha = exp(eta_alpha) sigma2_gamma = exp(eta_gamma) sigma2 = exp(eta) n = length(y) # covariance matrix of observations Sigma = sigma2 * diag(n) + sigma2_alpha * tcrossprod(Xalpha) + sigma2_gamma * tcrossprod(Xgamma) # log likelihood ll = -n / 2 * log(2 * pi) - sum(log(diag(chol(Sigma)))) - .5 * t(y - mu) %*% chol2inv(chol(Sigma)) %*% (y - mu) return(-ll) } Estimation Starting values and test. starts = list( mu = mean(y), eta_alpha = var(tapply(y, d$sire, mean)), eta_gamma = var(y) / 3, eta = var(y) / 3 ) Xalpha = diag(5) %x% rep(1, 4) Xgamma = diag(10) %x% rep(1, 2) Estimate at starting values. two_factor_re( mu = starts[[1]], eta_alpha = starts[[2]], eta_gamma = starts[[3]], eta = starts[[4]] ) [,1] [1,] 26.53887 Package bbmle has an mle2 function for maximum likelihood estimation based on underlying R functions like optim, and produces a nice summary table. LBFGS-B is used to place lower bounds on the variance estimates. library(bbmle) fit_mle = mle2(two_factor_re, start = starts, method = &#39;BFGS&#39;) Comparison We can compare to the lme4 model result. ### lme4 comparison library(lme4) fit_mer = lmer(y ~ (1 | sire) + (1 | dam:sire), d, REML = FALSE) summary(fit_mle) Maximum likelihood estimation Call: mle2(minuslogl = two_factor_re, start = starts, method = &quot;BFGS&quot;) Coefficients: Estimate Std. Error z value Pr(z) mu 1.32000 0.11848 11.1410 &lt; 2.2e-16 *** eta_alpha -2.92393 0.84877 -3.4449 0.0005712 *** eta_gamma -3.44860 0.65543 -5.2616 1.428e-07 *** eta -6.07920 0.44721 -13.5935 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: -23.98631 exp(coef(fit_mle)[-1]) eta_alpha eta_gamma eta 0.05372198 0.03178996 0.00229000 summary(fit_mer) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: y ~ (1 | sire) + (1 | dam:sire) Data: d AIC BIC logLik deviance df.resid -16 -12 12 -24 16 Scaled residuals: Min 1Q Median 3Q Max -1.21052 -0.59450 0.02314 0.61984 1.10386 Random effects: Groups Name Variance Std.Dev. dam:sire (Intercept) 0.03179 0.17830 sire (Intercept) 0.05372 0.23178 Residual 0.00229 0.04785 Number of obs: 20, groups: dam:sire, 10; sire, 5 Fixed effects: Estimate Std. Error t value (Intercept) 1.3200 0.1185 11.14 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/two_factor_RE.R Mixed Model via ML Introduction The following is based on the Wood text (Generalized Additive Models, 1st ed.) on additive models, chapter 6 in particular. It assumes familiarity with standard regression from a matrix perspective and at least passing familiarity with mixed models. The full document this chapter is based on can be found here, and contains more detail and exposition. See also the comparison of additive and mixed models here. For this we’ll use the sleepstudy data from the lme4 package. The data has reaction times for 18 individuals over 10 days each (see the help file for the sleepstudy object for more details). Data Setup library(tidyverse) data(sleepstudy, package = &#39;lme4&#39;) X = model.matrix(~Days, sleepstudy) Z = model.matrix(~factor(sleepstudy$Subject) - 1) colnames(Z) = paste0(&#39;Subject_&#39;, unique(sleepstudy$Subject)) # for cleaner presentation later rownames(Z) = paste0(&#39;Subject_&#39;, sleepstudy$Subject) y = sleepstudy$Reaction Function The following is based on the code in Wood (6.2.2), with a couple modifications for consistent nomenclature and presentation. We use optim and a minimizing function, in this case the negative log likelihood, to estimate the parameters of interest, collectively \\(\\theta\\), in the code below. The (square root of the) variances will be estimated on the log scale. In Wood, he simply extracts the ‘fixed effects’ for the intercept and days effects using lm (6.2.3), and we’ll do the same. mixed_ll &lt;- function(y, X, Z, theta) { tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 L = chol(e) # L&#39;L = e # transform dependent linear model to independent y = backsolve(L, y, transpose = TRUE) X = backsolve(L, X, transpose = TRUE) b = coef(lm(y~X-1)) LP = X %*% b ll = -n/2*log(2*pi) -sum(log(diag(L))) - crossprod(y - LP)/2 -ll } Here is an alternative function using a multivariate normal approach that doesn’t use the transformation to independence, and might provide additional perspective. In this case, y is just one large multivariate draw with mu mean vector and N x N covariance matrix e. mixed_ll_mv &lt;- function(y, X, Z, theta) { tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 b = coef(lm.fit(X, y)) mu = X %*% b ll = mvtnorm::dmvnorm(y, mu, e, log = TRUE) -ll } Estimation Now we’re ready to use the optim function for estimation. A slight change to tolerance is included to get closer estimates to lme4, which we will compare shortly. param_init = c(0, 0) names(param_init) = c(&#39;tau&#39;, &#39;sigma&#39;) fit = optim( fn = mixed_ll, X = X, y = y, Z = Z, par = param_init, control = list(reltol = 1e-10) ) fit_mv = optim( fn = mixed_ll_mv, X = X, y = y, Z = Z, par = param_init, control = list(reltol = 1e-10) ) Comparison func tau sigma negLogLik X(Intercept) XDays mixed_ll 36.016 30.899 897.039 251.405 10.467 mixed_ll_mv 36.016 30.899 897.039 251.405 10.467 As we can see, both formulations produce identical results. We can now compare those results to the lme4 output for the same model, and see that we’re getting what we should. library(lme4) fit_mer = lmer(Reaction ~ Days + (1 | Subject), sleepstudy, REML = FALSE) fit_mer Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: Reaction ~ Days + (1 | Subject) Data: sleepstudy AIC BIC logLik deviance df.resid 1802.0786 1814.8505 -897.0393 1794.0786 176 Random effects: Groups Name Std.Dev. Subject (Intercept) 36.01 Residual 30.90 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 We can also predict the random effects (Wood, 6.2.4), sometimes called BLUPs (Best Linear Unbiased Predictions) and after doing so again compare the results to the lme4 estimates. tau = exp(fit$par)[1] tausq = tau^2 sigma = exp(fit$par)[2] sigmasq = sigma^2 Sigma = tcrossprod(Z)*tausq/sigmasq + diag(length(y)) ranef_est = tausq * t(Z) %*% solve(Sigma) %*% resid(lm(y ~ X - 1))/sigmasq ranef_est lme4 Subject_308 40.64 40.64 Subject_309 -77.57 -77.57 Subject_310 -62.88 -62.88 Subject_330 4.39 4.39 Subject_331 10.18 10.18 Subject_332 8.19 8.19 Subject_333 16.44 16.44 Subject_334 -2.99 -2.99 Subject_335 -45.12 -45.12 Subject_337 71.92 71.92 Subject_349 -21.12 -21.12 Subject_350 14.06 14.06 Subject_351 -7.83 -7.83 Subject_352 36.25 36.25 Subject_369 7.01 7.01 Subject_370 -6.34 -6.34 Subject_371 -3.28 -3.28 Subject_372 18.05 18.05 Issues with ML estimation Situations arise in which using maximum likelihood for mixed models would result in notably biased estimates (e.g. small N, lots of fixed effects), and so it is typically not used. Standard software usually defaults to restricted maximum likelihood. However, our purpose here has been served. Link with penalized regression A link exists between mixed models and a penalized likelihood approach. For a penalized approach with the the standard linear model, the objective function we want to minimize can be expressed as follows: \\[ \\lVert y- X\\beta \\rVert^2 + \\beta^\\intercal\\beta \\] The added component to the sum of the squared residuals is the penalty. By adding the sum of the squared coefficients, we end up keeping them from getting too big, and this helps to avoid overfitting. Another interesting aspect of this approach is that it is comparable to using a specific prior on the coefficients in a Bayesian framework. We can now see mixed models as a penalized technique. If we knew \\(\\sigma\\) and \\(\\psi_\\theta\\), then the predicted random effects \\(g\\) and estimates for the fixed effects \\(\\beta\\) are those that minimize the following objective function: \\[ \\frac{1}{\\sigma^2}\\lVert y - X\\beta - Zg \\rVert^2 + g^\\intercal\\psi_\\theta^{-1}g \\] Source Main doc found at https://m-clark.github.io/docs/mixedModels/mixedModelML.html "],["probit.html", "Probit &amp; Bivariate Probit Standard Probit Bivariate Probit Source", " Probit &amp; Bivariate Probit Stata users seem to be the primary audience concerned with probit models, but I thought I’d play around with one even though I’ve never had reason to use it. Stata examples come from the UCLA ATS website and the Stata manual, so one can investigate the Stata result for comparison. Standard Probit The standard probit model is identical to the logistic model but using a different link function. Function probit_ll &lt;- function(beta, X, y) { mu = X %*% beta # these produce identical results, but the second is the typical depiction ll = sum(dbinom( y, size = 1, prob = pnorm(mu), log = T )) # ll = sum(y * pnorm(mu, log = T) + (1 - y) * log(1 - pnorm(mu))) -ll } Examples Example 1 detail available here. library(tidyverse) admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) head(admit) # A tibble: 6 x 4 admit gre gpa rank &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 380 3.61 3 2 1 660 3.67 3 3 1 800 4 1 4 1 640 3.19 4 5 0 520 2.93 4 6 1 760 3 2 X = model.matrix(admit~ gre + gpa + factor(rank), admit) y = admit$admit init = rep(0, ncol(X)) fit = optim( fn = probit_ll, par = init, X = X, y = y, method = &#39;BFGS&#39; ) fit $par [1] -2.2518260555 0.0007915268 0.5453357468 -0.4211611887 -0.8285356685 -0.9457812896 $value [1] 229.6227 $counts function gradient 140 11 $convergence [1] 0 $message NULL Example 2 from Stata manual on standard probit. We have data on the make, weight, and mileage rating of 22 foreign and 52 domestic automobiles. We wish to fit a probit model explaining whether a car is foreign based on its weight and mileage.\" auto = haven::read_dta(&#39;http://www.stata-press.com/data/r13/auto.dta&#39;) head(auto) # A tibble: 6 x 12 make price mpg rep78 headroom trunk weight length turn displacement gear_ratio foreign &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; 1 AMC Concord 4099 22 3 2.5 11 2930 186 40 121 3.58 0 [Domestic] 2 AMC Pacer 4749 17 3 3 11 3350 173 40 258 2.53 0 [Domestic] 3 AMC Spirit 3799 22 NA 3 12 2640 168 35 121 3.08 0 [Domestic] 4 Buick Century 4816 20 3 4.5 16 3250 196 40 196 2.93 0 [Domestic] 5 Buick Electra 7827 15 4 4 20 4080 222 43 350 2.41 0 [Domestic] 6 Buick LeSabre 5788 18 3 4 21 3670 218 43 231 2.73 0 [Domestic] X = model.matrix(foreign~ weight + mpg, auto) y = auto$foreign init = rep(0, ncol(X)) fit = optim( fn = probit_ll, par = init, X = X, y = y ) fit $par [1] 8.277015097 -0.002335939 -0.103973147 $value [1] 26.84419 $counts function gradient 380 NA $convergence [1] 0 $message NULL Bivariate Probit For the bivariate model, we are dealing with two binary outcomes and their correlation. Here is the main function. bivariate_probit_ll &lt;- function(pars, X, y1, y2) { rho = pars[1] mu1 = X %*% pars[2:(ncol(X) + 1)] mu2 = X %*% pars[(ncol(X) + 2):length(pars)] q1 = ifelse(y1 == 1, 1,-1) q2 = ifelse(y2 == 1, 1,-1) require(mnormt) eta1 = q1 * mu1 eta2 = q2 * mu2 ll = matrix(NA, nrow = nrow(X)) for (i in 1:length(ll)) { corr = q1[i] * q2[i] * rho corr = matrix(c(1, corr, corr, 1), 2) ll[i] = log( pmnorm( x = c(eta1[i], eta2[i]), mean = c(0, 0), varcov = corr ) ) } # the loop is probably clearer, and there is no difference in time, but here&#39;s # a oneliner ll = mapply(function(e1, e2, q1, q2) log(pmnorm(x = c(e1, e2), # varcov = matrix(c(1,q1*q2*rho,q1*q2*rho,1),2))), eta1, eta2, q1, q2) -sum(ll) } Example From the Stata manual on bivariate probit: We wish to model the bivariate outcomes of whether children attend private school and whether the head of the household voted for an increase in property tax based on the other covariates. school = haven::read_dta(&#39;http://www.stata-press.com/data/r13/school.dta&#39;) head(school) # A tibble: 6 x 11 obs pub12 pub34 pub5 private years school loginc logptax vote logeduc &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0 1 0 0 10 1 9.77 7.05 1 7.21 2 2 0 1 0 0 8 0 10.0 7.05 0 7.61 3 3 1 0 0 0 4 0 10.0 7.05 0 8.28 4 4 0 1 0 0 13 0 9.43 6.40 0 6.82 5 5 0 1 0 0 3 1 10.0 7.28 1 7.69 6 6 1 0 0 0 5 0 10.5 7.05 0 6.91 X = model.matrix(private ~ years + logptax + loginc, school) y1 = school$private y2 = school$vote init = c(0, rep(0, ncol(X)*2)) # you&#39;ll probably get a warning or two, ignore; takes a couple seconds fit = optim( fn = bivariate_probit_ll, par = init, X = X, y1 = y1, y2 = y2, method = &#39;BFGS&#39; ) loglik = fit$value rho = fit$par[1] coefs_private = fit$par[2:(ncol(X) + 1)] coefs_vote = fit$par[(ncol(X) + 2):length(init)] names(coefs_private) = names(coefs_vote) = c(&#39;Int&#39;, &#39;years&#39;, &#39;logptax&#39;, &#39;loginc&#39;) list( loglik = loglik, rho = rho, Private = coefs_private, Vote = coefs_vote ) $loglik [1] 89.25407 $rho [1] -0.2695802 $Private Int years logptax loginc -4.14327955 -0.01193699 -0.11030513 0.37459892 $Vote Int years logptax loginc -0.52933721 -0.01686685 -1.28983223 0.99840976 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/bivariateProbit.R "],["heckman-selection.html", "Heckman Selection Data Setup Two step approach Maximum Likelihood Source", " Heckman Selection This demonstration of the Heckman selection model is based on Bleven’s example here, but which is more or less the ‘classic’ example regarding women’s wages, variations of which you’ll find all over. Data Setup Description of the data: Draw 10,000 obs at random educ uniform over [0,16] age uniform over [18,64] wearnl = 4.49 + 0.08 * educ + 0.012 * age + ε Generate missing data for wearnl drawn z from standard normal [0,1]. z is actually never explained in the slides, I think it’s left out on slide 3 and just represents an additional covariate. d*=-1.5+0.15*educ+0.01*age+0.15*z+v wearnl missing if d*≤0 wearn reported if d*&gt;0 wearnl_all = wearnl with non-missing obs library(tidyverse) set.seed(123456) N = 10000 educ = sample(1:16, N, replace = TRUE) age = sample(18:64, N, replace = TRUE) covmat = matrix(c(.46^2, .25*.46, .25*.46, 1), ncol = 2) errors = mvtnorm::rmvnorm(N, sigma = covmat) z = rnorm(N) e = errors[, 1] v = errors[, 2] wearnl = 4.49 + .08 * educ + .012 * age + e d_star = -1.5 + 0.15 * educ + 0.01 * age + 0.15 * z + v observed_index = d_star &gt; 0 d = data.frame(wearnl, educ, age, z, observed_index) Examine linear regression approaches if desired. # lm based on full data lm_all = lm(wearnl ~ educ + age, data=d) # lm based on observed data lm_obs = lm(wearnl ~ educ + age, data=d[observed_index,]) summary(lm_all) Call: lm(formula = wearnl ~ educ + age, data = d) Residuals: Min 1Q Median 3Q Max -2.03286 -0.31240 0.00248 0.31578 1.50828 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.4691266 0.0171413 260.72 &lt;2e-16 *** educ 0.0798814 0.0010005 79.84 &lt;2e-16 *** age 0.0124381 0.0003398 36.60 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4611 on 9997 degrees of freedom Multiple R-squared: 0.4331, Adjusted R-squared: 0.433 F-statistic: 3820 on 2 and 9997 DF, p-value: &lt; 2.2e-16 summary(lm_obs) # smaller coefs, resid standard error Call: lm(formula = wearnl ~ educ + age, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75741 -0.30289 -0.00133 0.30918 1.50032 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.6713823 0.0258865 180.46 &lt;2e-16 *** educ 0.0705849 0.0014760 47.82 &lt;2e-16 *** age 0.0114758 0.0004496 25.52 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4507 on 5517 degrees of freedom Multiple R-squared: 0.3374, Adjusted R-squared: 0.3372 F-statistic: 1405 on 2 and 5517 DF, p-value: &lt; 2.2e-16 Two step approach The two-step approach first conducts a probit model regarding whether the individual is observed or not, in order to calculate the inverse mills ratio, or ‘nonselection hazard’. The second step is a standard linear model. Step 1: Probit Model probit = glm(observed_index ~ educ + age + z, data = d, family = binomial(link = &#39;probit&#39;)) summary(probit) Call: glm(formula = observed_index ~ educ + age + z, family = binomial(link = &quot;probit&quot;), data = d) Deviance Residuals: Min 1Q Median 3Q Max -2.4674 -0.9062 0.4628 0.8800 2.2674 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.519248 0.052819 -28.763 &lt;2e-16 *** educ 0.150027 0.003220 46.588 &lt;2e-16 *** age 0.010072 0.001015 9.926 &lt;2e-16 *** z 0.159292 0.013889 11.469 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 13755 on 9999 degrees of freedom Residual deviance: 11119 on 9996 degrees of freedom AIC: 11127 Number of Fisher Scoring iterations: 4 # http://www.stata.com/support/faqs/statistics/inverse-mills-ratio/ probit_lp = predict(probit) mills0 = dnorm(probit_lp)/pnorm(probit_lp) summary(mills0) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.38632 0.70027 0.75664 1.09246 1.96602 # identical formulation # probit_lp = -predict(probit) # imr = dnorm(probit_lp)/(1-pnorm(probit_lp)) imr = mills0[observed_index] summary(imr) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.28739 0.48466 0.57015 0.77617 1.87858 Take a look at the distribution. ggplot2::qplot(imr, geom = &#39;histogram&#39;) Step 2: Estimate via Linear Regression Standard regression model using the inverse mills ratio as covariate lm_select = lm(wearnl ~ educ + age + imr, data = d[observed_index, ]) summary(lm_select) Call: lm(formula = wearnl ~ educ + age + imr, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75994 -0.30293 -0.00186 0.31049 1.48179 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1063144 42.477 &lt;2e-16 *** educ 0.0782580 0.0052989 14.769 &lt;2e-16 *** age 0.0119700 0.0005564 21.513 &lt;2e-16 *** imr 0.0955209 0.0633557 1.508 0.132 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4506 on 5516 degrees of freedom Multiple R-squared: 0.3377, Adjusted R-squared: 0.3373 F-statistic: 937.4 on 3 and 5516 DF, p-value: &lt; 2.2e-16 Compare to sampleSelection package. library(sampleSelection) selection_2step = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;2step&#39;) summary(selection_2step) -------------------------------------------- Tobit 2 model (sample selection model) 2-step Heckman / heckit estimation 10000 observations (4480 censored and 5520 observed) 10 free parameters (df = 9991) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.519248 0.052725 -28.815 &lt;2e-16 *** educ 0.150027 0.003221 46.577 &lt;2e-16 *** age 0.010072 0.001014 9.934 &lt;2e-16 *** z 0.159292 0.013937 11.430 &lt;2e-16 *** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1066914 42.33 &lt;2e-16 *** educ 0.0782580 0.0053181 14.71 &lt;2e-16 *** age 0.0119700 0.0005592 21.41 &lt;2e-16 *** Multiple R-Squared:0.3377, Adjusted R-Squared:0.3373 Error terms: Estimate Std. Error t value Pr(&gt;|t|) invMillsRatio 0.09552 0.06354 1.503 0.133 sigma 0.45550 NA NA NA rho 0.20970 NA NA NA -------------------------------------------- coef(lm_select)[&#39;imr&#39;] / summary(lm_select)$sigma # slightly off imr 0.2119813 coef(lm_select)[&#39;imr&#39;] / summary(selection_2step)$estimate[&#39;sigma&#39;, &#39;Estimate&#39;] imr 0.2097041 Maximum Likelihood The following likelihood function takes arguments as follows: par: the regression coefficients pertaining to the two models, the residual standard error sigma and rho for the correlation estimate X: observed data model matrix for the linear regression model Z: complete data model matrix for the probit model y: the target variable observed_index: an index denoting whether y is observed select_ll &lt;- function(par, X, Z, y, observed_index) { gamma = par[1:4] lp_probit = Z %*% gamma beta = par[5:7] lp_lm = X %*% beta sigma = par[8] rho = par[9] ll = sum(log(1-pnorm(lp_probit[!observed_index]))) + - log(sigma) + sum(dnorm(y, mean = lp_lm, sd = sigma, log = TRUE)) + sum( pnorm((lp_probit[observed_index] + rho/sigma * (y-lp_lm)) / sqrt(1-rho^2), log.p = TRUE) ) -ll } X = model.matrix(lm_select) Z = model.matrix(probit) # initial values init = c(coef(probit), coef(lm_select)[-4], 1, 0) Estimate via optim. Without bounds for sigma and rho you’ll get warnings, but does fine anyway fit_unbounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;BFGS&#39;, control = list(maxit = 1000, reltol = 1e-15), hessian = T ) fit_bounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;L-BFGS&#39;, lower = c(rep(-Inf, 7), 1e-10,-1), upper = c(rep(Inf, 8), 1), control = list(maxit = 1000, factr = 1e-15), hessian = T ) Comparison Comparison model. selection_ml = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;ml&#39;) # summary(selection_ml) We now compare the results of the different estimation approaches. model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step probit (Intercept) -1.52026540 -1.521 -1.523 -1.519 -1.519 probit educ 0.15020205 0.150 0.150 0.150 0.150 probit age 0.01006608 0.010 0.010 0.010 0.010 probit z 0.15747033 0.158 0.158 0.159 0.159 lm (Intercept) 4.47798502 4.480 4.481 4.516 4.516 lm educ 0.08012367 0.080 0.080 0.078 0.078 lm age 0.01209129 0.012 0.012 0.012 0.012 lm sigma 0.45820605 0.458 0.458 0.451 0.456 both rho 0.25945397 0.257 0.255 0.212 0.210 model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step probit (Intercept) 0.053 0.053 0.053 0.053 0.053 probit educ 0.003 0.003 0.003 0.003 0.003 probit age 0.001 0.001 0.001 0.001 0.001 probit z 0.014 0.014 0.014 0.014 0.014 lm (Intercept) 0.090 0.090 0.090 0.106 0.107 lm educ 0.004 0.004 0.005 0.005 0.005 lm age 0.001 0.001 0.001 0.001 0.001 lm sigma 0.008 0.008 0.008 NA NA both rho 0.112 0.112 0.112 NA NA Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/heckman_selection.R "],["marginal-structural.html", "Marginal Structural Model Data Setup Function Estimation Comparison Source", " Marginal Structural Model This is a demonstration of a simple marginal structural model for estimation of so-called ‘causal’ effects using inverse probability weighting. Example data is from, and comparison made to, the ipw package. See more here. Data Setup This example is from the helpfile at ?ipwpoint. library(tidyverse) library(ipw) set.seed(16) n = 1000 simdat = data.frame(l = rnorm(n, 10, 5)) a_lin = simdat$l - 10 pa = plogis(a_lin) simdat = simdat %&gt;% mutate( a = rbinom(n, 1, prob = pa), y = 10 * a + 0.5 * l + rnorm(n, -10, 5) ) ipw_result = ipwpoint( exposure = a, family = &quot;binomial&quot;, link = &quot;logit&quot;, numerator = ~ 1, denominator = ~ l, data = simdat ) summary(ipw_result$ipw.weights) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.4810 0.5127 0.5285 0.9095 0.6318 74.6994 ipwplot(ipw_result$ipw.weights) We create the weights as follows using the probabilities from a logistic regression. ps_num = fitted(glm(a ~ 1, data = simdat, family = &#39;binomial&#39;)) ps_num[simdat$a == 0] = 1 - ps_num[simdat$a == 0] ps_den = fitted(glm(a ~ l, data = simdat, family = &#39;binomial&#39;)) ps_den[simdat$a == 0] = 1 - ps_den[simdat$a == 0] wts = ps_num / ps_den Compare the weights. rbind(summary(wts), summary(ipw_result$ipw.weights)) Min. 1st Qu. Median Mean 3rd Qu. Max. [1,] 0.481 0.5127181 0.5284768 0.9094652 0.631794 74.6994 [2,] 0.481 0.5127181 0.5284768 0.9094652 0.631794 74.6994 Add inverse probability weights to the data if desired. simdat = simdat %&gt;% mutate(sw = ipw_result$ipw.weights) Function Create the likelihood function for using the weights. msm_ll &lt;- function( par, # parameters to be estimated; first is taken to be sigma X, # model matrix y, # target variable wts # estimated weights ) { beta = par[-1] lp = X %*% beta sigma = exp(par[1]) # exponentiated value to stay positive ll = dnorm(y, mean = lp, sd = sigma, log = TRUE) -sum(ll * wts) # weighted likelihood # same as # ll = dnorm(y, mean = lp, sd = sigma)^wts # -sum(log(ll)) } Estimation We want to estimate the marginal structural model for the causal effect of a on y corrected for confounding by l, using inverse probability weighting with robust standard error from the survey package. Create the matrices for estimation, estimate the model, and extract results. X = cbind(1, simdat$a) y = simdat$y fit = optim( par = c(sigma = 0, intercept = 0, b = 0), fn = msm_ll, X = X, y = y, wts = wts, hessian = TRUE, method = &#39;BFGS&#39;, control = list(abstol = 1e-12) ) dispersion = exp(fit$par[1])^2 beta = fit$par[-1] Now we compute the standard errors. The following uses the survey package raw version to get the appropriate standard errors, which the ipw approach uses. glm_basic = glm(y ~ a, data = simdat, weights = wts) # to get unscaled cov res = resid(glm_basic, type = &#39;working&#39;) # residuals glm_vcov_unsc = summary(glm_basic)$cov.unscaled # weighted vcov unscaled by dispersion solve(crossprod(qr(X))) estfun = X * res * wts x = estfun %*% glm_vcov_unsc Comparison library(&quot;survey&quot;) fit_msm = svyglm( y ~ a, design = svydesign(~ 1, weights = ~ sw, data = simdat) ) summary(fit_msm) Call: svyglm(formula = y ~ a, design = svydesign(~1, weights = ~sw, data = simdat)) Survey design: svydesign(~1, weights = ~sw, data = simdat) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -4.375 1.142 -3.832 0.000135 *** a 10.647 1.190 8.948 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 29.58889) Number of Fisher Scoring iterations: 2 Now get the standard errors. se = sqrt(diag(crossprod(x) * n/(n-1))) # a robust standard error se_robust = sqrt(diag(sandwich::sandwich(glm_basic))) # an easier way to get it se_msm = sqrt(diag(vcov(fit_msm))) # extract from msm model Compare standard errors. tibble(se, se_robust, se_msm) # A tibble: 2 x 3 se se_robust se_msm &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.14 1.14 1.14 2 1.19 1.19 1.19 Inspect the general fit and compare with the other. (#tab:msm-comparison- show)msm_ll Estimate init_se se_robust t p dispersion -4.378 0.247 1.141 -3.834 0 29.563 10.649 0.361 1.189 8.950 0 29.563 (#tab:msm-comparison- show)svyglm term estimate std.error statistic p.value (Intercept) -4.375 1.142 -3.832 0 a 10.647 1.190 8.948 0 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ipw.R "],["tobit.html", "Tobit Regression Censoring with an Upper Limit Censoring with a Lower Limit Source", " Tobit Regression The following is a simple demonstration of tobit regression via maximum likelihood. The issue is one where data is censored such that while we observe the value, it is not the true value, which would extend beyond the range of the observed data. This is very commonly seen in cases where the dependent variable has been given some arbitrary cutoff at the lower or upper end of the range, often resulting in floor or ceiling effects respectively. The conceptual idea is that we are interested in modeling the underlying latent variable that would not have such restriction if it was actually observed. Censoring with an Upper Limit Data Setup Data regards academic aptitude (GRE scores) with will be modeled using reading and math test scores, as well as the type of program the student is enrolled in (academic, general, or vocational). See this for an applied example and more detail. library(tidyverse) acad_apt = read_csv(&quot;https://stats.idre.ucla.edu/stat/data/tobit.csv&quot;) %&gt;% mutate(prog = factor(prog, labels = c(&#39;acad&#39;, &#39;general&#39;, &#39;vocational&#39;))) Setup data and initial values. initmod = lm(apt ~ read + math + prog, data = acad_apt) X = model.matrix(initmod) init = c(coef(initmod), log_sigma = log(summary(initmod)$sigma)) Function tobit_ll &lt;- function(par, X, y, ul = -Inf, ll = Inf) { # this function only takes a lower OR upper limit # parameters sigma = exp(par[length(par)]) beta = par[-length(par)] # create indicator depending on chosen limit if (!is.infinite(ll)) { limit = ll indicator = y &gt; ll } else { limit = ul indicator = y &lt; ul } # linear predictor lp = X %*% beta # log likelihood ll = sum(indicator * log((1/sigma)*dnorm((y-lp)/sigma)) ) + sum((1-indicator) * log(pnorm((lp-limit)/sigma, lower = is.infinite(ll)))) -ll } Estimation Estimate via optim. fit_tobit = optim( par = init, tobit_ll, y = acad_apt$apt, X = X, ul = 800, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) # this would be more akin to the default Stata default approach # optim( # par = init, # tobit_ll, # y = acad_apt$apt, # X = X, # ul = 800, # control = list(maxit = 16000, reltol = 1e-15) # ) Comparison Compare to AER package tobit function. library(survival) fit_aer = AER::tobit( apt ~ read + math + prog, data = acad_apt, left = -Inf, right = 800 ) (Intercept) read math proggeneral progvocational sigma.log_sigma logLike fit_tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 library(survival) fit_aer = AER::tobit( apt ~ read + math + prog, data = acad_apt, left = -Inf, right = 800 ) AER is actually just using survreg from the survival package. Survival models are usually for modeling time to some event, e.g. death in medical studies, and the censoring comes from the fact that the observed event does not occur for some people. Like our tobit function, an indicator is needed to denote who is or isn’t censored. In survival models, the indicator is for the event itself, and means they are NOT censored. So we’ll reverse the indicator used in the tobit function for survreg. fit_surv = survreg(Surv(apt, apt &lt; 800, type = &#39;right&#39;) ~ read + math + prog, data = acad_apt, dist = &#39;gaussian&#39;) Compare all results. (Intercept) read math proggeneral progvocational sigma.log_sigma logLike fit_tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 survival 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 Censoring with a Lower Limit Create a censored data situation for the low end. The scale itself would be censored for anyone scoring a 200, but that basically doesn’t happen. In this data, 15 are less than a score of 500, so we’ll do that. acad_apt = acad_apt %&gt;% mutate(apt2 = apt, apt2 = if_else(apt2 &lt; 500, 500, apt2)) Estimate and use AER for comparison. fit_tobit = optim( par = init, tobit_ll, y = acad_apt$apt2, X = X, ll = 400, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) fit_aer = AER::tobit(apt2 ~ read + math + prog, data = acad_apt, left = 400) Comparison (Intercept) read math proggeneral progvocational sigma.log_sigma logLike fit_tobit 270.408 2.328 5.086 -11.331 -38.606 57.024 -1092.483 AER 270.409 2.328 5.085 -11.331 -38.606 57.024 -1092.483 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/tobit.R "],["cox.html", "Cox Survival Standard Proportional Hazards Time-varying coefficients Stratified Cox Model Source", " Cox Survival Some simple demonstrations of a standard Cox, Cox with time-varying covariates and a stratified Cox. Standard Proportional Hazards Data Setup set.seed(12) dur = 1:10 kittyblarg = rnorm(10) # something happened to kitty! kittyhappy = rep(0:1, times = 5) # is kitty happy? kittydied = sample(0:1, 10, replace = TRUE) # kitty died! oh no! d = data.frame(kittyblarg, kittyhappy, dur, kittydied) # Inspect d kittyblarg kittyhappy dur kittydied 1 -1.4805676 0 1 0 2 1.5771695 1 2 1 3 -0.9567445 0 3 0 4 -0.9200052 1 4 1 5 -1.9976421 0 5 1 6 -0.2722960 1 6 1 7 -0.3153487 0 7 0 8 -0.6282552 1 8 1 9 -0.1064639 0 9 0 10 0.4280148 1 10 1 Function Create a the (partial) likelihood function to feed to optim. cox_pl &lt;- function(pars, preds, died, t) { # Arguments- # pars: coefficients of interest # preds: predictor matrix # died: death # t: time b = pars X = as.matrix(preds[order(t), ]) died2 = died[order(t)] LP = X%*%b # Linear predictor # initialize log likelihood due to looping, not necessary ll = numeric(nrow(X)) rows = 1:nrow(preds) for (i in rows){ riskset = ifelse(rows &lt; i, FALSE, TRUE) # identify risk set ll[i] = died2[i]*(LP[i] - log(sum(exp(LP[riskset]))) ) # log likelihood } -sum(ll) } Estimation Estimate with optim. initial_values = c(0, 0) fit = optim( par = initial_values, fn = cox_pl, preds = d[, c(&#39;kittyblarg&#39;, &#39;kittyhappy&#39;)], died = d[, &#39;kittydied&#39;], t = dur, method = &quot;BFGS&quot;, hessian = T ) fit $par [1] -0.5827125 1.3803731 $value [1] 7.783878 $counts function gradient 14 5 $convergence [1] 0 $message NULL $hessian [,1] [,2] [1,] 1.9913282 0.4735968 [2,] 0.4735968 0.7780126 Comparison Extract results. B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se # create a summary table result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE)*2, pnorm(Z, lower = TRUE)*2) ) Compare to survival package. library(survival) cox_model = coxph(Surv(dur, kittydied) ~ kittyblarg + kittyhappy) B exp se Z p coxph.kittyblarg -0.583 0.558 0.766 -0.760 0.447 coxph.kittyhappy 1.380 3.976 1.226 1.126 0.260 cox_pl.kittyblarg -0.583 0.558 0.766 -0.760 0.447 cox_pl.kittyhappy 1.380 3.976 1.226 1.126 0.260 Time-varying coefficients Note that technically nothing new is going on here relative to the previous model. See the vignette for the survival package for further details. Data Setup In the following we’ll first create some noisy time points. set.seed(123) t1 = rep(NA, 20) t2 = rep(NA, 20) t1[seq(1, 20, by = 2)] = 1:10 t2[seq(1, 20, by = 2)] = t1[seq(1, 20, by = 2)] + sample(1:5, 10, replace = TRUE) + abs(rnorm(10)) t1[seq(2, 20, by = 2)] = t2[seq(1, 20, by = 2)] t2[seq(2, 20, by = 2)] = t1[seq(2, 20, by = 2)] + sample(1:5) + abs(rnorm(10)) kitty = rep(1:10, e = 2) kittyblarg = t2 + rnorm(20, sd = 5) kittyhappy = rep(0:1, times = 5, e = 2) die = 0:1 cens = c(0, 0) kittydied = ifelse(runif(20)&gt;=.5, die, cens) d = data.frame(kitty, kittyblarg, kittyhappy, t1, t2, kittydied) # Inspect the Surv object if desired # Surv(t1,t2, kittydied) # Inspect the data d kitty kittyblarg kittyhappy t1 t2 kittydied 1 1 -1.870759 0 1.000000 4.686853 0 2 1 4.904677 0 4.686853 7.902718 0 3 2 4.798609 1 2.000000 5.445662 0 4 2 15.214255 1 5.445662 10.780575 0 5 3 5.467102 0 3.000000 6.224082 0 6 3 9.958737 0 6.224082 8.309781 1 7 4 -9.776800 1 4.000000 6.359814 0 8 4 4.586278 1 6.359814 8.445237 0 9 5 9.833514 0 5.000000 8.400771 0 10 5 7.368822 0 8.400771 13.471382 1 11 6 13.283435 1 6.000000 11.110683 0 12 6 18.256961 1 11.110683 14.256076 0 13 7 10.736186 0 7.000000 11.555841 0 14 7 23.935980 0 11.555841 17.721386 1 15 8 6.114988 1 8.000000 10.786913 0 16 8 14.573972 1 10.786913 12.605429 1 17 9 13.516008 0 9.000000 11.497850 0 18 9 9.750603 0 11.497850 14.182787 1 19 10 8.371929 1 10.000000 14.966617 0 20 10 19.430893 1 14.966617 19.286674 1 Function cox_pl_tv &lt;- function(pars, preds, died, t1, t2, data) { # Same arguments as before though will take a data object # plus variable names via string input. Also requires beginning # and end time point (t1, t2) dat = data[,c(preds, died, t1, t2)] dat = dat[order(dat$t2), ] b = pars X = as.matrix(dat[, preds]) died2 = dat[, died] # linear predictor LP = X%*%b # log likelihood ll = numeric(nrow(X)) rows = 1:nrow(dat) for (i in rows){ st_i = dat$t2[i] # if they have already died/censored (row &lt; i) or if the initial time is # greater than current end time (t1 &gt; st_i), they are not in the risk set, # else they are. riskset = ifelse(rows &lt; i | dat$t1 &gt; st_i, FALSE, TRUE) ll[i] = died2[i]*(LP[i] - log(sum(exp(LP[riskset]))) ) } -sum(ll) } Estimation Estimate with optim. initial_values = c(0, 0) fit = optim( par = initial_values, fn = cox_pl_tv, preds = c(&#39;kittyblarg&#39;, &#39;kittyhappy&#39;), died = &#39;kittydied&#39;, data = d, t1 = &#39;t1&#39;, t2 = &#39;t2&#39;, method = &quot;BFGS&quot;, hessian = TRUE ) # fit Comparison Extract results. B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE) * 2, pnorm(Z, lower = TRUE) * 2) ) Compare to survival package. cox_model_tv = coxph( Surv(t1, t2, kittydied) ~ kittyblarg + kittyhappy, method = &#39;breslow&#39;, control = coxph.control(iter.max = 1000) ) # cox_model_tv # cox_model_tv$loglik[2] B exp se Z p coxph.kittyblarg -0.092 0.912 0.107 -0.853 0.394 coxph.kittyhappy -1.513 0.220 1.137 -1.331 0.183 cox_pl.kittyblarg -0.092 0.912 0.107 -0.853 0.394 cox_pl.kittyhappy -1.513 0.220 1.137 -1.331 0.183 Stratified Cox Model Data Setup data(ovarian, package = &#39;survival&#39;) Function Requires cox_pl function above though one could extend to cox_pl_tv. cox_pl_strat &lt;- function(pars, preds, died, t, strata) { strat = as.factor(strata) d = data.frame(preds, died, t, strat) dlist = split(d, strata) neglls = map_dbl( dlist, function(x) cox_pl( pars = pars, preds = x[, colnames(preds)], died = x$died, t = x$t ) ) sum(neglls) } Estimation Estimate with optim. initial_values = c(0, 0) fit = optim( par = initial_values, fn = cox_pl_strat, preds = ovarian[, c(&#39;age&#39;, &#39;ecog.ps&#39;)], died = ovarian$fustat, t = ovarian$futime, strata = ovarian$rx, method = &quot;BFGS&quot;, hessian = TRUE ) # fit Comparison B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE) * 2, pnorm(Z, lower = TRUE)*2) ) cox_strata_model = coxph( Surv(futime, fustat) ~ age + ecog.ps + strata(rx), data = ovarian ) # cox_strata_model # cox_strata_model$loglik[2] B exp se Z p coxph.age 0.139 1.149 0.048 2.885 0.004 coxph.ecog.ps -0.097 0.908 0.630 -0.154 0.878 cox_pl.age 0.139 1.149 0.048 2.885 0.004 cox_pl.ecog.ps -0.097 0.908 0.630 -0.154 0.878 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/survivalCox.R "],["hurdle.html", "Hurdle Model Poisson Negative Binomial Source", " Hurdle Model Hurdle models are applied to situations in which target data has relatively many of one value, usually zero, to go along with the other observed values. They are two-part models, a logistic model for whether an observation is zero or not, and a count model for the other part. The key distinction from the usual ‘zero-inflated’ count models, is that the count distribution does not contribute to the excess zeros. While the typical application is count data, the approach can be applied to any distribution in theory. Poisson Data Setup Here we import a simple data set. The example comes from the Stata help file for zinb command. One can compare results with hnblogit command in Stata. library(tidyverse) fish = haven::read_dta(&quot;http://www.stata-press.com/data/r11/fish.dta&quot;) Function The likelihood function is of two parts, one a logistic model, the other, a poisson count model. hurdle_poisson_ll &lt;- function(y, X, par) { # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] poispars = par[grep(&#39;pois&#39;, names(par))] # Logit model part Xlogit = X ylogit = ifelse(y == 0, 0, 1) LPlogit = Xlogit %*% logitpars mulogit = plogis(LPlogit) # Calculate the likelihood logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) ) # Poisson part Xpois = X[y &gt; 0, ] ypois = y[y &gt; 0] mupois = exp(Xpois %*% poispars) # Calculate the likelihood loglik0 = -mupois loglikpois = -sum(dpois(ypois, lambda = mupois, log = TRUE)) + sum(log(1 - exp(loglik0))) # combine likelihoods loglik = loglikpois + logliklogit loglik } Get some starting values from glm For these functions, and create a named vector for them. init_mod = glm( count ~ persons + livebait, data = fish, family = poisson, x = TRUE, y = TRUE ) starts = c(logit = coef(init_mod), pois = coef(init_mod)) Estimation Use optim. to estimate parameters. I fiddle with some options to reproduce the hurdle function as much as possible. fit = optim( par = starts, fn = hurdle_poisson_ll, X = init_mod$x, y = init_mod$y, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit Extract the elements from the output to create a summary table. B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se p = ifelse(Z &gt;= 0, pnorm(Z, lower = FALSE)*2, pnorm(Z)*2) summary_table = round(data.frame(B, se, Z, p), 3) list(summary = summary_table, ll = fit$value) $summary B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.761 0.078 logit.livebait 0.711 0.403 1.766 0.077 pois.(Intercept) -2.057 0.341 -6.035 0.000 pois.persons 0.750 0.043 17.378 0.000 pois.livebait 1.851 0.307 6.023 0.000 $ll [1] 882.2514 Comparison Compare to hurdle from pscl package. library(pscl) fit_pscl = hurdle( count ~ persons + livebait, data = fish, zero.dist = &quot;binomial&quot;, dist = &quot;poisson&quot; ) coef B se Z p pscl X.Intercept. -2.057 0.341 -6.035 0.000 pscl persons 0.750 0.043 17.378 0.000 pscl livebait 1.851 0.307 6.023 0.000 pscl X.Intercept..1 -1.417 0.491 -2.888 0.004 pscl persons.1 0.206 0.117 1.762 0.078 pscl livebait.1 0.711 0.403 1.765 0.077 hurdle_poisson_ll logit.(Intercept) -1.417 0.491 -2.888 0.004 hurdle_poisson_ll logit.persons 0.206 0.117 1.761 0.078 hurdle_poisson_ll logit.livebait 0.711 0.403 1.766 0.077 hurdle_poisson_ll pois.(Intercept) -2.057 0.341 -6.035 0.000 hurdle_poisson_ll pois.persons 0.750 0.043 17.378 0.000 hurdle_poisson_ll pois.livebait 1.851 0.307 6.023 0.000 Negative Binomial Function The likelihood function. hurdle_nb_ll &lt;- function(y, X, par) { # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] NegBinpars = par[grep(&#39;NegBin&#39;, names(par))] theta = exp(par[grep(&#39;theta&#39;, names(par))]) # Logit model part Xlogit = X ylogit = ifelse(y == 0, 0, 1) LPlogit = Xlogit%*%logitpars mulogit = plogis(LPlogit) # Calculate the likelihood logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) ) #NB part XNB = X[y &gt; 0, ] yNB = y[y &gt; 0] muNB = exp(XNB %*% NegBinpars) # Calculate the likelihood loglik0 = dnbinom(0, mu = muNB, size = theta, log = TRUE) loglik1 = dnbinom(yNB, mu = muNB, size = theta, log = TRUE) loglikNB = -( sum(loglik1) - sum(log(1 - exp(loglik0))) ) # combine likelihoods loglik = loglikNB + logliklogit loglik } Estimation starts = c( logit = coef(init_mod), NegBin = coef(init_mod), theta = 1 ) fit_nb = optim( par = starts, fn = hurdle_nb_ll, X = init_mod$x, y = init_mod$y, control = list(maxit = 5000, reltol = 1e-12), method = &quot;BFGS&quot;, hessian = TRUE ) # fit_nb B = fit_nb$par se = sqrt(diag(solve(fit_nb$hessian))) Z = B/se p = ifelse(Z &gt;= 0, pnorm(Z, lower = FALSE)*2, pnorm(Z)*2) summary_table = round(data.frame(B, se, Z, p), 3) list(summary = summary_table, ll = fit_nb$value) $summary B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.762 0.078 logit.livebait 0.711 0.403 1.765 0.077 NegBin.(Intercept) -3.461 0.869 -3.984 0.000 NegBin.persons 0.941 0.153 6.154 0.000 NegBin.livebait 1.985 0.639 3.109 0.002 theta -1.301 0.576 -2.257 0.024 $ll [1] 439.3686 Comparison fit_pscl = hurdle( count ~ persons + livebait, data = fish, zero.dist = &quot;binomial&quot;, dist = &quot;negbin&quot; ) # summary(fit_pscl)$coefficients # summary_table coef B se Z p pscl X.Intercept. -3.461 0.869 -3.984 0.000 pscl persons 0.941 0.153 6.154 0.000 pscl livebait 1.985 0.639 3.109 0.002 pscl Log.theta. -1.301 0.576 -2.257 0.024 pscl X.Intercept..1 -1.417 0.491 -2.888 0.004 pscl persons.1 0.206 0.117 1.762 0.078 pscl livebait.1 0.711 0.403 1.765 0.077 hurdle_nb_ll logit.(Intercept) -1.417 0.491 -2.888 0.004 hurdle_nb_ll logit.persons 0.206 0.117 1.762 0.078 hurdle_nb_ll logit.livebait 0.711 0.403 1.765 0.077 hurdle_nb_ll NegBin.(Intercept) -3.461 0.869 -3.984 0.000 hurdle_nb_ll NegBin.persons 0.941 0.153 6.154 0.000 hurdle_nb_ll NegBin.livebait 1.985 0.639 3.109 0.002 hurdle_nb_ll theta -1.301 0.576 -2.257 0.024 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hurdle.R "],["zi.html", "Zero-Inflated Model Poisson Negative Binomial Supplemental Example Source", " Zero-Inflated Model Log likelihood function to estimate parameters for a Zero-inflated Poisson model. With examples and comparison to pscl package output. Also includes approach based on Hilbe GLM text. Zero-inflated models are applied to situations in which target data has relatively many of one value, usually zero, to go along with the other observed values. They are two-part models, a logistic model for whether an observation is zero or not, and a count model for the other part. The key distinction from hurdle count models is that the count distribution contributes to the excess zeros. While the typical application is count data, the approach can be applied to any distribution in theory. Poisson Data Setup Get the data. library(tidyverse) fish = haven::read_dta(&quot;http://www.stata-press.com/data/r11/fish.dta&quot;) Function The log likelihood function. zip_ll &lt;- function(y, X, par) { # arguments are response y, predictor matrix X, and parameter named starting points of &#39;logit&#39; and &#39;pois&#39; # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] poispars = par[grep(&#39;pois&#39;, names(par))] # Logit part; in this function Xlogit = Xpois but one could split X argument into Xlogi and Xpois for example Xlogit = X LPlogit = Xlogit %*% logitpars logi0 = plogis(LPlogit) # alternative 1/(1+exp(-LPlogit)) # Poisson part Xpois = X mupois = exp(Xpois %*% poispars) # LLs logliklogit = log( logi0 + exp(log(1 - logi0) - mupois) ) loglikpois = log(1 - logi0) + dpois(y, lambda = mupois, log = TRUE) # Hilbe formulation # logliklogit = log(logi0 + (1 - logi0)*exp(- mupois) ) # loglikpois = log(1-logi0) -mupois + log(mupois)*y #not necessary: - log(gamma(y+1)) y0 = y == 0 # 0 values yc = y &gt; 0 # Count part loglik = sum(logliklogit[y0]) + sum(loglikpois[yc]) -loglik } Estimation Get starting values or simply do zeros. # for zip: need &#39;logit&#39;, &#39;pois&#39; initial_model = glm( count ~ persons + livebait, data = fish, x = TRUE, y = TRUE, &quot;poisson&quot; ) # starts = c(logit = coef(initial_model), pois = coef(initial_model)) starts = c(rep(0, 3), rep(0, 3)) names(starts) = c(paste0(&#39;pois.&#39;, names(coef(initial_model))), paste0(&#39;logit.&#39;, names(coef(initial_model)))) Estimate with optim. fit = optim( par = starts , fn = zip_ll, X = initial_model$x, y = initial_model$y, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit Comparison Extract for clean display. B = fit$par se = sqrt(diag(solve((fit$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 Results from pscl. library(pscl) fit_pscl = zeroinfl(count ~ persons + livebait, data = fish, dist = &quot;poisson&quot;) Compare. coef B se Z p pscl X.Intercept. -2.006 0.324 -6.196 0.000 pscl persons 0.747 0.043 17.516 0.000 pscl livebait 1.809 0.292 6.195 0.000 pscl X.Intercept..1 0.303 0.674 0.449 0.654 pscl persons.1 -0.069 0.129 -0.537 0.591 pscl livebait.1 -0.031 0.558 -0.056 0.956 zip_poisson_ll pois.(Intercept) -2.006 0.324 -6.196 0.000 zip_poisson_ll pois.persons 0.747 0.043 17.516 0.000 zip_poisson_ll pois.livebait 1.809 0.292 6.195 0.000 zip_poisson_ll logit.(Intercept) 0.303 0.674 0.449 0.654 zip_poisson_ll logit.persons -0.069 0.129 -0.537 0.591 zip_poisson_ll logit.livebait -0.031 0.558 -0.056 0.956 Negative Binomial Function zinb_ll &lt;- function(y, X, par) { # arguments are response y, predictor matrix X, and parameter named starting points of &#39;logit&#39;, &#39;negbin&#39;, and &#39;theta&#39; # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] negbinpars = par[grep(&#39;negbin&#39;, names(par))] theta = exp(par[grep(&#39;theta&#39;, names(par))]) # Logit part; in this function Xlogit = Xnegbin but one could split X argument into Xlogit and Xnegbin for example Xlogit = X LPlogit = Xlogit %*% logitpars logi0 = plogis(LPlogit) # Negbin part Xnegbin = X munb = exp(Xnegbin %*% negbinpars) # LLs logliklogit = log( logi0 + exp( log(1 - logi0) + suppressWarnings(dnbinom(0, size = theta, mu = munb, log = TRUE)) ) ) logliknegbin = log(1 - logi0) + suppressWarnings(dnbinom(y, size = theta, mu = munb, log = TRUE)) # Hilbe formulation # theta part # alpha = 1/theta # m = 1/alpha # p = 1/(1 + alpha*munb) # logliklogit = log( logi0 + (1 - logi0)*(p^m) ) # logliknegbin = log(1-logi0) + log(gamma(m+y)) - log(gamma(m)) + m*log(p) + y*log(1-p) # gamma(y+1) not needed y0 = y == 0 # 0 values yc = y &gt; 0 # Count part loglik = sum(logliklogit[y0]) + sum(logliknegbin[yc]) -loglik } Estimation Get starting values or simply do zeros. # for zinb: &#39;logit&#39;, &#39;negbin&#39;, &#39;theta&#39; initial_model = model.matrix(count ~ persons + livebait, data = fish) # to get X matrix startlogi = glm(count == 0 ~ persons + livebait, data = fish, family = &quot;binomial&quot;) startcount = glm(count ~ persons + livebait, data = fish, family = &quot;poisson&quot;) starts = c( negbin = coef(startcount), logit = coef(startlogi), theta = 1 ) # starts = c(negbin = rep(0, 3), # logit = rep(0, 3), # theta = log(1)) Estimate with optim. fit_nb = optim( par = starts , fn = zinb_ll, X = initial_model, y = fish$count, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit_nb Comparison Extract for clean display. B = fit_nb$par se = sqrt(diag(solve((fit_nb$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 Results from pscl. # pscl results library(pscl) fit_pscl = zeroinfl(count ~ persons + livebait, data = fish, dist = &quot;negbin&quot;) Compare. coef B se Z p pscl X.Intercept. -2.803 0.558 -5.026 0.000 pscl persons 0.849 0.124 6.833 0.000 pscl livebait 1.791 0.511 3.504 0.000 pscl Log.theta. -0.969 0.302 -3.206 0.001 pscl X.Intercept..1 -4.276 4.278 -1.000 0.318 pscl persons.1 0.560 0.517 1.084 0.279 pscl livebait.1 1.168 3.661 0.319 0.750 zinb_ll negbin.(Intercept) -2.803 0.558 -5.026 0.000 zinb_ll negbin.persons 0.849 0.124 6.834 0.000 zinb_ll negbin.livebait 1.791 0.511 3.504 0.000 zinb_ll logit.(Intercept) -4.276 4.278 -1.000 0.318 zinb_ll logit.persons 0.560 0.517 1.084 0.279 zinb_ll logit.livebait 1.168 3.661 0.319 0.750 zinb_ll theta -0.969 0.302 -3.206 0.001 Supplemental Example This supplemental example uses the bioChemists data. It contains a sample of 915 biochemistry graduate students with the following information: art: count of articles produced during last 3 years of Ph.D. fem: factor indicating gender of student, with levels Men and Women mar: factor indicating marital status of student, with levels Single and Married kid5: number of children aged 5 or younger phd: prestige of Ph.D. department ment: count of articles produced by Ph.D. mentor during last 3 years data(&quot;bioChemists&quot;, package = &quot;pscl&quot;) initial_model = model.matrix(art ~ fem + mar + kid5 + phd + ment, data = bioChemists) # to get X matrix startlogi = glm(art==0 ~ fem + mar + kid5 + phd + ment, data = bioChemists, family = &quot;binomial&quot;) startcount = glm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists, family = &quot;quasipoisson&quot;) starts = c( negbin = coef(startcount), logit = coef(startlogi), theta = summary(startcount)$dispersion ) # starts = c(negbin = rep(0, 6), # logit = rep(0, 6), # theta = 1) fit_nb_pub = optim( par = starts , fn = zinb_ll, X = initial_model, y = bioChemists$art, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit_nb_pub B = fit_nb_pub$par se = sqrt(diag(solve((fit_nb_pub$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 library(pscl) fit_pscl = zeroinfl(art ~ . | ., data = bioChemists, dist = &quot;negbin&quot;) summary(fit_pscl)$coefficients $count Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.4167465901 0.143596450 2.90220678 3.705439e-03 femWomen -0.1955076374 0.075592558 -2.58633447 9.700275e-03 marMarried 0.0975826042 0.084451953 1.15548073 2.478936e-01 kid5 -0.1517320709 0.054206071 -2.79917119 5.123397e-03 phd -0.0006997593 0.036269674 -0.01929323 9.846072e-01 ment 0.0247861500 0.003492672 7.09661548 1.278491e-12 Log(theta) 0.9763577454 0.135469554 7.20721163 5.710921e-13 $zero Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.19160645 1.3227962 -0.1448496 0.884829645 femWomen 0.63587048 0.8488959 0.7490559 0.453823498 marMarried -1.49943716 0.9386562 -1.5974296 0.110169987 kid5 0.62840922 0.4427746 1.4192531 0.155825245 phd -0.03773288 0.3080059 -0.1225070 0.902497523 ment -0.88227364 0.3162186 -2.7900755 0.005269575 round(data.frame(B, se, Z, p), 4) B se Z p negbin.(Intercept) 0.4167 0.1436 2.9021 0.0037 negbin.femWomen -0.1955 0.0756 -2.5863 0.0097 negbin.marMarried 0.0976 0.0845 1.1555 0.2479 negbin.kid5 -0.1517 0.0542 -2.7992 0.0051 negbin.phd -0.0007 0.0363 -0.0195 0.9845 negbin.ment 0.0248 0.0035 7.0967 0.0000 logit.(Intercept) -0.1916 1.3229 -0.1449 0.8848 logit.femWomen 0.6359 0.8490 0.7491 0.4538 logit.marMarried -1.4995 0.9387 -1.5974 0.1102 logit.kid5 0.6284 0.4428 1.4192 0.1558 logit.phd -0.0377 0.3080 -0.1225 0.9025 logit.ment -0.8823 0.3162 -2.7901 0.0053 theta 0.9763 0.1355 7.2071 0.0000 Source Original code for zip_ll found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/poiszeroinfl.R Original code for zinb_ll found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/NBzeroinfl.R "],["naive-bayes.html", "Naive Bayes Initialization Comparison Source", " Naive Bayes Initialization Demo for binary data. First we generate some data. We have several binary covariates and a binary target variable y. library(tidyverse) set.seed(123) x = matrix(sample(0:1, 50, replace = TRUE), ncol = 5) xf = map(data.frame(x), factor) y = sample(0:1, 10, prob = c(.25, .75), replace = TRUE) Comparison We can use e1071 for comparison. library(e1071) m = naiveBayes(xf, y) m Naive Bayes Classifier for Discrete Predictors Call: naiveBayes.default(x = xf, y = y) A-priori probabilities: y 0 1 0.3 0.7 Conditional probabilities: X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Using base R for our model, we can easily obtain the ‘predictions’… map(xf, function(var) t(prop.table(table(&#39; &#39; = var, y), margin = 2))) $X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 $X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 $X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/naivebayes.R "],["multinomial.html", "Multinomial Regression Standard (Categorical) Model Alternative specific and constant variables Source", " Multinomial Regression For more detail on these types of models, see my document. In general we can use multinomial models for multi-category target variables, or more generally, multi-count data. Standard (Categorical) Model Data Setup First, lets get some data. 200 entering high school students make program choices: general program, vocational program, and academic program. We will be interested in their choice, using their writing score as a proxy for scholastic ability and their socioeconomic status, a categorical variable of low, middle, and high values. library(haven) library(tidyverse) library(mlogit) program = read_dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;) %&gt;% as_factor() %&gt;% mutate(prog = relevel(prog, ref = &quot;academic&quot;)) head(program[, 1:5]) # A tibble: 6 x 5 id female ses schtyp prog &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 45 female low public vocation 2 108 male middle public general 3 15 male high public vocation 4 67 male low public vocation 5 153 male middle public vocation 6 51 female high public general # convert to long form for mlogit program_long = program %&gt;% select(id, prog, ses, write) %&gt;% mlogit.data( data = , shape = &#39;wide&#39;, choice = &#39;prog&#39;, id.var = &#39;id&#39; ) head(program_long) ~~~~~~~ first 10 observations out of 600 ~~~~~~~ id prog ses write chid alt idx 1 1 FALSE low 44 11 academic 11:emic 2 1 FALSE low 44 11 general 11:eral 3 1 TRUE low 44 11 vocation 11:tion 4 2 FALSE middle 41 9 academic 9:emic 5 2 FALSE middle 41 9 general 9:eral 6 2 TRUE middle 41 9 vocation 9:tion 7 3 TRUE low 65 159 academic 159:emic 8 3 FALSE low 65 159 general 159:eral 9 3 FALSE low 65 159 vocation 159:tion 10 4 TRUE low 50 30 academic 30:emic ~~~ indexes ~~~~ chid id alt 1 11 1 academic 2 11 1 general 3 11 1 vocation 4 9 2 academic 5 9 2 general 6 9 2 vocation 7 159 3 academic 8 159 3 general 9 159 3 vocation 10 30 4 academic indexes: 1, 1, 2 We go ahead and run a model via mlogit for later comparison. fit_mlogit = mlogit(prog ~ 1| write + ses, data = program_long) mlogit_coefs = coef(fit_mlogit)[c(1,5,7,3,2,6,8,4)] # reorder Function Multinomial model via maximum likelihood multinom_ml &lt;- function(par, X, y) { levs = levels(y) ref = levs[1] # reference level (category label 1) y0 = y == ref y1 = y == levs[2] # category 2 y2 = y == levs[3] # category 3 beta = matrix(par, ncol = 2) # more like mnlogit package depiction in its function # V1 = X %*% beta[ ,1] # V2 = X %*% beta[ ,2] # ll = sum(-log(1 + exp(V1) + exp(V2))) + sum(V1[y1], V2[y2]) V = X %*% beta # a vectorized approach baseProbVec = 1 / (1 + rowSums(exp(V))) # reference group probabilities loglik = sum(log(baseProbVec)) + crossprod(c(V), c(y1, y2)) loglik } fit = optim( runif(8,-.1, .1), multinom_ml, X = model.matrix(prog ~ ses + write, data = program), y = program$prog, control = list( maxit = 1000, reltol = 1e-12, ndeps = rep(1e-8, 8), trace = TRUE, fnscale = -1, type = 3 ), method = &#39;BFGS&#39; ) initial value 638.963532 iter 10 value 180.008322 final value 179.981726 converged # fit$par Comparison An initial comparison. fit_coefs mlogit_coefs (Intercept):general 2.8522 2.8522 sesmiddle:general -0.5333 -0.5333 seshigh:general -1.1628 -1.1628 write:general -0.0579 -0.0579 (Intercept):vocation 5.2182 5.2182 sesmiddle:vocation 0.2914 0.2914 seshigh:vocation -0.9827 -0.9827 write:vocation -0.1136 -0.1136 The following uses dmultinom for the likelihood, similar to other modeling demonstrations in this document. X = model.matrix(prog ~ ses + write, data = program) y = program$prog pars = matrix(fit$par, ncol = 2) V = X %*% pars acadprob = 1 / (1+rowSums(exp(V))) fitnonacad = exp(V) * matrix(rep(acadprob, 2), ncol = 2) fits = cbind(acadprob, fitnonacad) yind = model.matrix( ~ -1 + prog, data = program) # because dmultinom can&#39;t take matrix for prob ll = 0 for (i in 1:200){ ll = ll + dmultinom(yind[i, ], size = 1, prob = fits[i, ], log = TRUE) } ll [1] -179.9817 fit$value [1] -179.9817 logLik(fit_mlogit) &#39;log Lik.&#39; -179.9817 (df=8) Alternative specific and constant variables Now we add alternative specific and alternative constant variables to the previous individual specific covariates.. In this example, price is alternative invariant (Z) income is individual/alternative specific (X), and catch is alternative specific (Y). We can use the fish data from the mnlogit package. library(mnlogit) # note we are now using mnlogit data(Fish) head(Fish) mode income alt price catch chid 1.beach FALSE 7083.332 beach 157.930 0.0678 1 1.boat FALSE 7083.332 boat 157.930 0.2601 1 1.charter TRUE 7083.332 charter 182.930 0.5391 1 1.pier FALSE 7083.332 pier 157.930 0.0503 1 2.beach FALSE 1250.000 beach 15.114 0.1049 2 2.boat FALSE 1250.000 boat 10.534 0.1574 2 fm = formula(mode ~ price | income | catch) fit_mnlogit = mnlogit(fm, Fish) # fit_mnlogit = mlogit(fm, Fish) # summary(fit_mnlogit) The likelihood function. multinom_ml &lt;- function(par, X, Y, Z, respVec, choice) { # Args- # X dim nrow(Fish)/K x p + 1 (intercept) # Z, Y nrow(N); Y has alt specific coefs; then for Z ref group dropped so nrow = nrow*(K-1)/K # for ll everything through previous X the same # then calc probmat for Y and Z, add to X probmat, and add to base N = sum(choice) K = length(unique(respVec)) levs = levels(respVec) xpar = matrix(par[1:6], ncol = K-1) ypar = matrix(par[7:10], ncol = K) zpar = matrix(par[length(par)], ncol = 1) # Calc X Vx = X %*% xpar # Calc Y (mnlogit finds N x 1 results by going through 1:N, N+1:N*2 etc; then # makes 1 vector, then subtracts the first 1:N from whole vector, then makes # Nxk-1 matrix with N+1:end values (as 1:N are just zero)); creating the # vector and rebuilding the matrix is unnecessary though Vy = sapply(1:K, function(alt) Y[respVec == levs[alt], , drop = FALSE] %*% ypar[alt]) Vy = Vy[,-1] - Vy[,1] # Calc Z Vz = Z %*% zpar Vz = matrix(Vz, ncol = 3) # all Vs must fit into N x K -1 matrix where N is nobs (i.e. individuals) V = Vx + Vy + Vz ll0 = crossprod(c(V), choice[-(1:N)]) baseProbVec &lt;- 1 / (1 + rowSums(exp(V))) loglik = sum(log(baseProbVec)) + ll0 loglik # note fitted values via # fitnonref = exp(V) * matrix(rep(baseProbVec, K-1), ncol = K-1) # fitref = 1-rowSums(fitnonref) # fits = cbind(fitref, fitnonref) } inits = runif(11, -.1, .1) mdat = mnlogit(fm, Fish)$model # this data already ordered! As X has a constant value across alternatives, the coefficients regard the selection of the alternative relative to reference. X = cbind(1, mdat[mdat$`_Alt_Indx_` == &#39;beach&#39;, &#39;income&#39;]) dim(X) [1] 1182 2 head(X) [,1] [,2] [1,] 1 7083.332 [2,] 1 1250.000 [3,] 1 3750.000 [4,] 1 2083.333 [5,] 1 4583.332 [6,] 1 4583.332 Y will use the complete data to start. Coefficients will be differences from the reference alternative coefficient. Y = as.matrix(mdat[, &#39;catch&#39;, drop = FALSE]) dim(Y) [1] 4728 1 Z are difference scores from reference group. Z = as.matrix(mdat[mdat$`_Alt_Indx_` != &#39;beach&#39;, &#39;price&#39;, drop = FALSE]) Z = Z - mdat[mdat$`_Alt_Indx_` == &#39;beach&#39;, &#39;price&#39;] dim(Z) [1] 3546 1 respVec = mdat$`_Alt_Indx_` # first 10 should be 0 0 1 0 1 0 0 0 1 1 after beach dropped multinom_ml(inits, X, Y, Z, respVec, choice = mdat$mode) [,1] [1,] -162384.5 fit = optim( par = rep(0, 11), multinom_ml, X = X, Y = Y, Z = Z, respVec = respVec, choice = mdat$mode, control = list( maxit = 1000, reltol = 1e-12, ndeps = rep(1e-8, 11), trace = TRUE, fnscale = -1, type = 3 ), method = &#39;BFGS&#39; ) initial value 1638.599935 iter 10 value 1253.603448 iter 20 value 1199.143447 final value 1199.143445 converged Comparison Compare fits. fit_coefs mnlogit_coefs 0.842 0.842 0.000 0.000 2.155 2.155 0.000 0.000 1.043 1.043 0.000 0.000 3.118 3.118 2.542 2.542 0.759 0.759 2.851 2.851 -0.025 -0.025 fit_ll mnlogit_ll -1199.143 -1199.143 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/multinomial.R "],["ordinal.html", "Ordinal Regression Data Function Estimation Comparison Source", " Ordinal Regression The following demonstrates a standard cumulative link ordinal regression model via maximum likelihood. Default is with probit link function. Alternatively you can compare it with a logit link, which will result in values roughly 1.7*parameters estimates from the probit. Data This data generation is from the probit perspective, where the underlying continuous latent variable is normally distributed. library(tidyverse) set.seed(808) N = 1000 # Sample size x = cbind(x1 = rnorm(N), x2 = rnorm(N)) # predictor variables beta = c(1,-1) # coefficients y_star = rnorm(N, mean = x %*% beta) # the underlying latent variable y_1 = y_star &gt; -1.5 # -1.50 first cutpoint y_2 = y_star &gt; .75 # 0.75 second cutpoint y_3 = y_star &gt; 1.75 # 1.75 third cutpoint y = y_1 + y_2 + y_3 + 1 # target table(y) y 1 2 3 4 175 495 182 148 d = data.frame(x, y = factor(y)) Function ordinal_ll &lt;- function(par, X, y, probit = TRUE) { K = length(unique(y)) # number of classes K ncuts = K-1 # number of cutpoints/thresholds cuts = par[(1:ncuts)] # cutpoints beta = par[-(1:ncuts)] # regression coefficients lp = X %*% beta # linear predictor ll = rep(0, length(y)) # log likelihood pfun = ifelse(probit, pnorm, plogis) # which link to use for(k in 1:K){ if (k==1) { ll[y==k] = pfun((cuts[k] - lp[y==k]), log = TRUE) } else if (k &lt; K) { ll[y==k] = log(pfun(cuts[k] - lp[y==k]) - pfun(cuts[k-1] - lp[y==k])) } else { ll[y==k] = log(1 - pfun(cuts[k-1] - lp[y==k])) } } -sum(ll) } Estimation init = c(-1, 1, 2, 0, 0) # initial values fit_probit = optim( init, ordinal_ll, y = y, X = x, probit = TRUE, control = list(reltol = 1e-10) ) fit_logit = optim( init, ordinal_ll, y = y, X = x, probit = FALSE, control = list(reltol = 1e-10) ) Comparison We can compare our results with the ordinal package. library(ordinal) fit_ordpack_probit = clm(y ~ x1 + x2, data = d, link = &#39;probit&#39;) fit_ordpack_logit = clm(y ~ x1 + x2, data = d, link = &#39;logit&#39;) method cut_1 cut_2 cut_3 beta1 beta2 probit ordinal_ll -1.609 0.731 1.798 1.019 -1.051 probit ordpack -1.609 0.731 1.798 1.019 -1.051 logit ordinal_ll -2.872 1.284 3.168 1.806 -1.877 logit ordpack -2.872 1.284 3.168 1.806 -1.877 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ordinal_regression.R "],["markov.html", "Markov Model Data Setup Function Estimation Comparison Source", " Markov Model Here we demonstrate a Markov model. We start by showing how to create some data and estimate such a model via the markovchain package. You may want to play with it to get a better feel for how it works, as we will use it for comparison later. library(tidyverse) library(markovchain) A = matrix(c(.7, .3, .9, .1), nrow = 2, byrow = TRUE) dtmcA = new( &#39;markovchain&#39;, transitionMatrix = A, states = c(&#39;a&#39;, &#39;b&#39;), name = &#39;MarkovChain A&#39; ) dtmcA MarkovChain A A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.7 0.3 b 0.9 0.1 plot(dtmcA) transitionProbability(dtmcA, &#39;b&#39;, &#39;b&#39;) [1] 0.1 initialState = c(0, 1) steps = 4 finalState = initialState * dtmcA^steps # using power operator finalState a b [1,] 0.7488 0.2512 steadyStates(dtmcA) a b [1,] 0.75 0.25 observed_states = sample(c(&#39;a&#39;, &#39;b&#39;), 50, c(.7, .3), replace = TRUE) createSequenceMatrix(observed_states) a b a 24 11 b 11 3 markovchainFit(observed_states) $estimate MLE Fit A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.6857143 0.3142857 b 0.7857143 0.2142857 $standardError a b a 0.1399708 0.09476071 b 0.2369018 0.12371791 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b a 0.4113764 0.1285581 b 0.3213953 0.0000000 $upperEndpointMatrix a b a 0.9600522 0.5000133 b 1.0000000 0.4567684 $logLikelihood [1] -29.06116 Data Setup Data Functions A recursive function to take a matrix power. mat_power &lt;- function(M, N) { if (N == 1) return(M) M %*% mat_power(M, N - 1) } A function to create a sequence. create_sequence &lt;- function(states, len, tmat) { # states: number of states # len: length of sequence # tmat: the transition matrix states_numeric = length(unique(states)) out = numeric(len) out[1] = sample(states_numeric, 1, prob = colMeans(tmat)) # initial state for (i in 2:len){ out[i] = sample(states_numeric, 1, prob = tmat[out[i - 1], ]) } states[out] } # example test_matrix = matrix(rep(2, 4), nrow = 2) test_matrix [,1] [,2] [1,] 2 2 [2,] 2 2 mat_power(test_matrix, 2) [,1] [,2] [1,] 8 8 [2,] 8 8 # transition matrix A = matrix(c(.7, .3, .4, .6), nrow = 2, byrow = TRUE) mat_power(A, 10) [,1] [,2] [1,] 0.5714311 0.4285689 [2,] 0.5714252 0.4285748 Two states Demo Note that a notably long sequence is needed to get close to recovering the true transition matrix. A = matrix(c(.7, .3, .9, .1), nrow = 2, byrow = TRUE) observed_states = create_sequence(c(&#39;a&#39;, &#39;b&#39;), 500, tmat = A) createSequenceMatrix(observed_states) a b a 288 100 b 101 10 prop.table(createSequenceMatrix(observed_states), 1) a b a 0.7422680 0.25773196 b 0.9099099 0.09009009 fit = markovchainFit(observed_states) fit $estimate MLE Fit A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.7422680 0.25773196 b 0.9099099 0.09009009 $standardError a b a 0.04373856 0.02577320 b 0.09053942 0.02848899 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b a 0.6565420 0.20721741 b 0.7324559 0.03425269 $upperEndpointMatrix a b a 0.8279941 0.3082465 b 1.0000000 0.1459275 $logLikelihood [1] -255.0253 # log likelihood sum(createSequenceMatrix(observed_states) * log(fit$estimate@transitionMatrix)) [1] -255.0253 Three states demo A = matrix( c(.70, .20, .10, .20, .40, .40, .05, .05, .90), nrow = 3, byrow = TRUE ) observed_states = create_sequence(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), 500, tmat = A) createSequenceMatrix(observed_states) a b c a 92 20 11 b 11 24 22 c 20 13 286 prop.table(createSequenceMatrix(observed_states), 1) a b c a 0.74796748 0.16260163 0.08943089 b 0.19298246 0.42105263 0.38596491 c 0.06269592 0.04075235 0.89655172 markovchainFit(observed_states) $estimate MLE Fit A 3 - dimensional discrete Markov Chain defined by the following states: a, b, c The transition matrix (by rows) is defined as follows: a b c a 0.74796748 0.16260163 0.08943089 b 0.19298246 0.42105263 0.38596491 c 0.06269592 0.04075235 0.89655172 $standardError a b c a 0.07798100 0.03635883 0.02696443 b 0.05818640 0.08594701 0.08228800 c 0.01401923 0.01130267 0.05301421 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b c a 0.59512750 0.09133962 0.03658157 b 0.07893918 0.25259956 0.22468337 c 0.03521872 0.01859952 0.79264575 $upperEndpointMatrix a b c a 0.90080746 0.23386364 0.1422802 b 0.30702573 0.58950571 0.5472465 c 0.09017313 0.06290518 1.0000000 $logLikelihood [1] -277.6268 Function Now we create a function to calculate the (negative) log likelihood. markov_ll &lt;- function(par, x) { # par should be the c(A) of transition probabilities A nstates = length(unique(x)) # create transition matrix par = matrix(par, ncol = nstates) par = t(apply(par, 1, function(x) x / sum(x))) # create seq matrix seq_mat = table(x[-length(x)], x[-1]) # calculate log likelihood ll = sum(seq_mat * log(par)) -ll } A = matrix( c(.70, .20, .10, .40, .20, .40, .10, .15, .75), nrow = 3, byrow = TRUE ) observed_states = create_sequence(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), 1000, tmat = A) Estimation Note that initial state values will be transformed to rowsum to one, so the specific initial values don’t matter (i.e. they don’t have to be probabilities). With the basic optim approach, sometimes log(0) will occur and produce a warning. Can be ignored, or use LFBGS as demonstrated at the end. initpar = rep(1, 9) fit = optim( par = initpar, fn = markov_ll, x = observed_states, method = &#39;BFGS&#39;, control = list(reltol = 1e-12) ) # get estimates on prob scale est_mat = matrix(fit$par, ncol = 3) est_mat = t(apply(est_mat, 1, function(x) x / sum(x))) Comparison Compare with markovchain package. fit_compare = markovchainFit(observed_states) # compare log likelihood c(-fit$value, fit_compare$logLikelihood) [1] -815.6466 -815.6466 # compare estimated transition matrix list( `Estimated via optim` = est_mat, `markovchain Package` = fit_compare$estimate@transitionMatrix, `Analytical Solution` = prop.table( table(observed_states[-length(observed_states)], observed_states[-1]) , 1) ) %&gt;% purrr::map(round, 3) $`Estimated via optim` [,1] [,2] [,3] [1,] 0.674 0.242 0.084 [2,] 0.462 0.126 0.412 [3,] 0.106 0.151 0.743 $`markovchain Package` a b c a 0.674 0.242 0.084 b 0.462 0.126 0.412 c 0.106 0.151 0.743 $`Analytical Solution` a b c a 0.674 0.242 0.084 b 0.462 0.126 0.412 c 0.106 0.151 0.743 Visualize. plot( new( &#39;markovchain&#39;, transitionMatrix = est_mat, states = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), name = &#39;Estimated Markov Chain&#39; ) ) If you don’t want warnings due to zeros use constraints (?constrOptim). fit = optim( par = initpar, fn = markov_ll, x = observed_states, method = &#39;L-BFGS&#39;, lower = rep(1e-20, length(initpar)), control = list(pgtol = 1e-12) ) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/markov_model.R "],["hmm.html", "Hidden Markov Model Data Setup Function Estimation Supplemental demo Source", " Hidden Markov Model This function duplicates hmm_viterbi.py, which comes from the Viterbi algorithm wikipedia page (at least as it was when I stumbled across it, see it in the supplemental section). This first function is just to provide R code that is similar, in case anyone is interested in a more direct comparison, but the original used lists of tuples and thus was very inefficient R-wise, and provided output that wasn’t succinct. The second function takes a vectorized approach and returns a matrix in a much more straightforward fashion. Both will provide the same result as the Python code. See The Markov Model chapter also. Data Setup library(tidyverse) obs = c(&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) # observed state states = c(&#39;Healthy&#39;, &#39;Fever&#39;) # latent states start_p = c(&#39;Healthy&#39; = 0.6, &#39;Fever&#39; = 0.4) # starting probabilities # transition matrix trans_p = list( &#39;Healthy&#39; = c(&#39;Healthy&#39; = 0.7, &#39;Fever&#39; = 0.3), &#39;Fever&#39; = c(&#39;Healthy&#39; = 0.4, &#39;Fever&#39; = 0.6) ) # emission matrix emit_p = list( &#39;Healthy&#39; = c(&#39;normal&#39; = 0.5, &#39;cold&#39; = 0.4, &#39;dizzy&#39; = 0.1), &#39;Fever&#39; = c(&#39;normal&#39; = 0.1, &#39;cold&#39; = 0.3, &#39;dizzy&#39; = 0.6) ) Function This first function takes a Python-esque approach in the manner of the original, allowing for easier comparison. viterbi &lt;- function(obs, states, start_p, trans_p, emit_p) { V = vector(&#39;list&#39;, length(obs)) for (st in seq_along(states)) { V[[1]][[states[st]]] = list(&quot;prob&quot; = start_p[st] * emit_p[[st]][obs[1]], &quot;prev&quot; = NULL) } for (t in 2:length(obs)) { for (st in seq_along(states)) { max_tr_prob = numeric() for (prev_st in states) { max_tr_prob[prev_st] = V[[t-1]][[prev_st]][[&quot;prob&quot;]] * trans_p[[prev_st]][[st]] } max_tr_prob = max(max_tr_prob) for (prev_st in states) { flag = V[[t-1]][[prev_st]][[&quot;prob&quot;]] * trans_p[[prev_st]][[st]] == max_tr_prob if (flag) { max_prob = max_tr_prob * emit_p[[st]][obs[t]] V[[t]][[states[st]]] = list(&#39;prob&#39; = max_prob, &#39;prev&#39; = prev_st) } } } } # I don&#39;t bother duplicating the text output code of the original df_out = rbind( Healthy = sapply(V, function(x) x$Healthy$prob), Fever = sapply(V, function(x) x$Fever$prob) ) colnames(df_out) = obs print(df_out) m = paste0( &#39;The steps of states are: &#39;, paste(rownames(df_out)[apply(df_out, 2, which.max)], collapse = &#39; &#39;), paste(&#39;\\nHighest probability: &#39;, max(df_out[, ncol(df_out)])) ) message(m) V } This approach is much more R-like. viterbi_2 &lt;- function(obs, states, start_p, trans_mat, emit_mat) { prob_mat = matrix(NA, nrow = length(states), ncol = length(obs)) colnames(prob_mat) = obs rownames(prob_mat) = states prob_mat[,1] = start_p * emit_mat[,1] for (t in 2:length(obs)) { prob_tran = prob_mat[,t-1] * trans_mat max_tr_prob = apply(prob_tran, 2, max) prob_mat[,t] = max_tr_prob * emit_mat[, obs[t]] } print(prob_mat) m = paste0( &#39;The steps of states are: &#39;, paste(states[apply(prob_mat, 2, which.max)], collapse = &#39; &#39;), paste(&#39;\\nHighest probability: &#39;, max(prob_mat[, ncol(prob_mat)])) ) message(m) } Estimation First we demo the initial function. test = viterbi( obs, states, start_p, trans_p, emit_p ) normal cold dizzy Healthy 0.30 0.084 0.00588 Fever 0.04 0.027 0.01512 # test set.seed(123) obs = sample(obs, 6, replace = TRUE) test = viterbi( obs, states, start_p, trans_p, emit_p ) dizzy dizzy dizzy cold dizzy cold Healthy 0.06 0.0096 0.003456 0.00497664 0.0003483648 0.0003224863 Fever 0.24 0.0864 0.031104 0.00559872 0.0020155392 0.0003627971 # test Now the vectorized approach. set.seed(123) obs = c(&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) obs = sample(obs, 6, replace = T) # need matrices now emit_mat = do.call(rbind, emit_p) trans_mat = do.call(rbind, trans_p) viterbi_2( obs, states, start_p, trans_mat, emit_mat ) dizzy dizzy dizzy cold dizzy cold Healthy 0.30 0.021 0.00216 0.0031104 0.000217728 0.0002015539 Fever 0.04 0.054 0.01944 0.0034992 0.001259712 0.0002267482 Supplemental demo This example comes from the hidden markov model wikipedia page. states = c(&#39;Rainy&#39;, &#39;Sunny&#39;) observations = c(&#39;walk&#39;, &#39;shop&#39;, &#39;clean&#39;) start_probability = c(&#39;Rainy&#39; = 0.6, &#39;Sunny&#39; = 0.4) transition_probability = rbind( &#39;Rainy&#39; = c(&#39;Rainy&#39; = 0.7, &#39;Sunny&#39; = 0.3), &#39;Sunny&#39; = c(&#39;Rainy&#39; = 0.4, &#39;Sunny&#39; = 0.6) ) emission_probability = rbind( &#39;Rainy&#39; = c(&#39;walk&#39; = 0.1, &#39;shop&#39; = 0.4, &#39;clean&#39; = 0.5), &#39;Sunny&#39; = c(&#39;walk&#39; = 0.6, &#39;shop&#39; = 0.3, &#39;clean&#39; = 0.1) ) viterbi_2( observations, states, start_probability, transition_probability, emission_probability ) walk shop clean Rainy 0.06 0.0384 0.013440 Sunny 0.24 0.0432 0.002592 Source Original code for R found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hmm_viterbi.R Original code for Python found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hmm_viterbi.py "],["quantile-regression.html", "Quantile Regression Data Setup Function Estimation Comparison Visualize Python Source", " Quantile Regression Data Setup We’ll use the quantreg package for comparison, and the classic data set on Belgian household income and food expenditure. Scale income if you want a meaningful ‘centercept’. library(tidyverse) library(quantreg) data(engel) # engel$income = scale(engel$income) X = cbind(1, engel$income) colnames(X) = c(&#39;Intercept&#39;, &#39;income&#39;) Function Loss function. It really is this simple. qreg &lt;- function(par, X, y, tau) { lp = X%*%par res = y - lp loss = ifelse(res &lt; 0 , -(1 - tau)*res, tau*res) sum(loss) } Estimation We’ll estimate the median to start. Compare optim output with quantreg package. optim( par = c(intercept = 0, income = 0), fn = qreg, X = X, y = engel$foodexp, tau = .5 )$par intercept income 81.4853550 0.5601706 rq(foodexp ~ income, tau = .5, data = engel) Call: rq(formula = foodexp ~ income, tau = 0.5, data = engel) Coefficients: (Intercept) income 81.4822474 0.5601806 Degrees of freedom: 235 total; 233 residual Other quantiles Now we will add additional quantiles to estimate. # quantiles qs = c(.05, .1, .25, .5, .75, .9, .95) fit_rq = coef(rq(foodexp ~ income, tau = qs, data = engel)) fit_qreg = map_df(qs, function(tau) data.frame(t( optim( par = c(intercept = 0, income = 0), fn = qreg, X = X, y = engel$foodexp, tau = tau )$par ))) Comparison Compare results. coef tau= 0.05 tau= 0.10 tau= 0.25 tau= 0.50 tau= 0.75 tau= 0.90 tau= 0.95 fit_rq X.Intercept. 124.880 110.142 95.484 81.482 62.397 67.351 64.104 fit_rq income 0.343 0.402 0.474 0.560 0.644 0.686 0.709 fit_qreg intercept 124.881 110.142 95.484 81.485 62.400 67.332 64.143 fit_qreg income.1 0.343 0.402 0.474 0.560 0.644 0.686 0.709 Visualize Let’s visualize the results. Python The above is available as a Python demo in the supplemental section. Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/quantile_regression.Rmd "],["cubic-spline.html", "Cubic Spline Model Data Setup Functions Example 1 Example 2 Source", " Cubic Spline Model See Wood (2017) Generalized Additive Models or my document for an introduction to generalized additive models. Data Setup The data regards engine wear index versus engine capacity for 19 Volvo car engines used. The idea is that a larger car engine will wear out less quickly than a smaller one (from Wood GAM 2e chapter 4). library(tidyverse) data(engine, package = &#39;gamair&#39;) size = engine$size wear = engine$wear x = size - min(size) x = x / max(x) d = data.frame(wear, x) Functions Cubic spline function, rk refers to reproducing kernel. If I recall correctly, the function code is actually based on the first edition of Wood’s text. rk &lt;- function(x, z) { ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12)/4 - ((abs(x - z) - 0.5)^4 - (abs(x - z) - 0.5)^2 / 2 + 7/240) / 24 } Generate the model matrix. splX &lt;- function(x, knots) { q = length(knots) + 2 # number of parameters n = length(x) # number of observations X = matrix(1, n, q) # initialized model matrix X[ ,2] = x # set second column to x X[ ,3:q] = outer(x, knots, FUN = rk) # remaining to cubic spline basis X } splS &lt;- function(knots) { q = length(knots) + 2 S = matrix(0, q, q) # initialize matrix S[3:q, 3:q] = outer(knots, knots, FUN = rk) # fill in non-zero part S } Matrix square root function. Note that there are various packages with their own. mat_sqrt &lt;- function(S) { d = eigen(S, symmetric = TRUE) rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors) rS } Penalized fitting function. prs_fit &lt;- function(y, x, knots, lambda) { q = length(knots) + 2 # dimension of basis n = length(x) # number of observations Xa = rbind(splX(x, knots), mat_sqrt(splS(knots))*sqrt(lambda)) # augmented model matrix y[(n + 1):(n+q)] = 0 # augment the data vector lm(y ~ Xa - 1) # fit and return penalized regression spline } Example 1 We start with an unpenalized approach. knots = 1:4/5 X = splX(x, knots) # generate model matrix fit_lm = lm(wear ~ X - 1) # fit model xp = 0:100/100 # x values for prediction Xp = splX(xp, knots) # prediction matrix Visualize. ggplot(aes(x = x, y = wear), data = data.frame(x, wear)) + geom_point(color = &quot;#FF5500&quot;) + geom_line(aes(x = xp, y = Xp %*% coef(fit_lm)), data = data.frame(xp, Xp), color = &quot;#00AAFF&quot;) + labs(x = &#39;Scaled Engine size&#39;, y = &#39;Wear Index&#39;) Example 2 Now we add the lambda penalty and compare fits at different values of lambda. knots = 1:7/8 d2 = data.frame(x = xp) lambda = c(.1, .01, .001, .0001, .00001, .000001) rmse = vector(&#39;numeric&#39;, length(lambda)) idx = 0 for (i in lambda) { # fit penalized regression fit_penalized = prs_fit( y = wear, x = x, knots = knots, lambda = i ) # spline choosing lambda Xp = splX(xp, knots) # matrix to map parameters to fitted values at xp LP = Xp %*% coef(fit_penalized) d2[, paste0(&#39;lambda = &#39;, i)] = LP[, 1] r = resid(fit_penalized) idx = 1 + idx rmse[idx] = sqrt(mean(r^2)) } Visualize. I add the root mean square error for model comparison. d3 = d2 %&gt;% pivot_longer(cols = -x, names_to = &#39;lambda&#39;, values_to = &#39;value&#39;) %&gt;% mutate(lambda = fct_inorder(lambda), rmse = round(rmse[lambda], 3)) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/cubicsplines.R "],["gaussian-process.html", "Gaussian Processes Noise-Free Demonstration Noisy Demonstration Source", " Gaussian Processes Noise-Free Demonstration We’ll start with the ‘Noise-free’ gaussian process. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006. See those sources for more detail. Murphy’s original Matlab code can be found here, though the relevant files are housed alongside this code in my original repo (*.m files) and the supplemental section. The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which y = sin(x). It will reproduce figure 15.2 in Murphy 2012 and 2.2 in Rasmussen and Williams 2006. Data Setup library(tidyverse) l = 1 # for l, sigma_f, see note at covariance function sigma_f = 1 k_eps = 1e-8 # see note at K_starstar n_prior = 5 # number of prior draws n_post_pred = 5 # number of posterior predictive draws Generate noise-less training data. X_train = c(-4, -3, -2, -1, 1) y_train = sin(X_train) n_train = length(X_train) X_test = seq(-5, 5, .2) n_test = length(X_test) Functions The mean function. In this case the mean equals 0. gp_mu &lt;- function(x) { map_dbl(x, function(x) x = 0) } The covariance function. Here it is the squared exponential kernel. l is the horizontal scale, sigma_f is the vertical scale. gp_K &lt;- function(x, l = 1, sigma_f = 1){ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) } Visualize the prior distribution Data setup for the prior and plot. x_prior = seq(-5, 5, .2) y_prior = MASS::mvrnorm( n = n_prior, mu = gp_mu(x_prior), Sigma = gp_K(x_prior, l = l, sigma_f = sigma_f) ) prior_data = data.frame( x = x_prior, y = t(y_prior), sd = apply(y_prior, 2, sd)) %&gt;% pivot_longer(-c(x, sd), names_to = &#39;variable&#39;) g1 = ggplot(aes(x = x, y = value), data = prior_data) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;, alpha = .5) + labs(title = &#39;Prior&#39;) g1 Generate the posterior predictive distribution Create K, K*, and K** matrices as defined in the texts. K = gp_K(X_train, l = l, sigma_f = sigma_f) K_ = gp_K(c(X_train, X_test), l = l, sigma_f = sigma_f) # initial matrix K_star = K_[1:n_train, (n_train+1):ncol(K_)] # dim = N x N* tK_star = t(K_star) # dim = N* x N K_starstar = K_[(n_train+1):nrow(K_), (n_train+1):ncol(K_)] + # dim = N* x N* k_eps * diag(n_test) # the k_eps part is for positive definiteness Kinv = solve(K) Calculate posterior mean and covariance. post_mu = gp_mu(X_test) + t(K_star) %*% Kinv %*% (y_train - gp_mu(X_train)) post_K = K_starstar - t(K_star) %*% Kinv %*% K_star s2 = diag(post_K) # R = chol(post_K) # L = t(R) # L is used in alternative formulation below based on gaussSample.m Generate draws from posterior predictive. y_pp = data.frame( t(MASS::mvrnorm(n_post_pred, mu = post_mu, Sigma = post_K)) ) # alternative if using R and L above # y_pp = data.frame(replicate(n_post_pred, post_mu + L %*% rnorm(post_mu))) Visualize the Posterior Predictive Distribution Reshape data for plotting and create the plot. pp_data = data.frame( x = X_test, y = y_pp, se_lower = post_mu - 2 * sqrt(s2), se_upper = post_mu + 2 * sqrt(s2) ) %&gt;% pivot_longer(starts_with(&#39;y&#39;), names_to = &#39;variable&#39;) g2 = ggplot(aes(x = x, y = value), data = pp_data) + geom_ribbon(aes(ymin = se_lower, ymax = se_upper, group = variable), fill = &#39;gray92&#39;) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;) + geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + labs(title = &#39;Posterior Predictive&#39;) g2 Plot prior and posterior predictive together. library(patchwork) g1 + g2 Noisy Demonstration ‘Noisy’ gaussian process demo. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006. See those sources for more detail. Murphy’s original Matlab code can be found here, though the relevant files are housed alongside this code in my original repo (*.m files). The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which y = sin(x) + noise. It will reproduce an example akin to figure 15.3 in Murphy 2012. Data Setup l = 1 # for l, sigma_f, sigma_n, see note at covariance function sigma_f = 1 sigma_n = .25 k_eps = 1e-8 # see note at Kstarstar n_prior = 5 # number of prior draws n_post_pred = 5 # number of posterior predictive draws X_train = 15 * (runif(20) - .5) n_train = length(X_train) # kept sine function for comparison to noise free result y_train = sin(X_train) + rnorm(n = n_train, sd = .1) X_test = seq(-7.5, 7.5, length = 200) n_test = length(X_test) Functions The mean function. In this case the mean equals 0. gp_mu &lt;- function(x) { map_dbl(x, function(x) x = 0) } The covariance function. Here it is the squared exponential kernel. l is the horizontal scale, sigma_f is the vertical scale, and, unlike the previous function, sigma_n the noise. gp_K &lt;- function( x, y = NULL, l = 1, sigma_f = 1, sigma_n = .5 ) { if(!is.null(y)){ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) + sigma_n*diag(length(x)) } else{ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) } } Visualize the prior distribution Data setup. x_prior = seq(-5, 5, .2) y_prior = MASS::mvrnorm( n = n_prior, mu = gp_mu(x_prior), Sigma = gp_K( x_prior, l = l, sigma_f = sigma_f, sigma_n = sigma_n ) ) Plot. prior_data = data.frame( x = x_prior, y = t(y_prior), sd = apply(y_prior, 2, sd)) %&gt;% pivot_longer(-c(x, sd), names_to = &#39;variable&#39;) g1 = ggplot(aes(x = x, y = value), data = prior_data) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;, alpha = .5) + labs(title = &#39;Prior&#39;) g1 Generate the posterior predictive distribution Create Ky, K*, and K** matrices as defined in the texts. Ky = gp_K( x = X_train, y = y_train, l = l, sigma_f = sigma_f, sigma_n = sigma_n ) # initial matrix K_ = gp_K( c(X_train, X_test), l = l, sigma_f = sigma_f, sigma_n = sigma_n ) Kstar = K_[1:n_train, (n_train+1):ncol(K_)] # dim = N x N* tKstar = t(Kstar) # dim = N* x N Kstarstar = K_[(n_train+1):nrow(K_), (n_train+1):ncol(K_)] + # dim = N* x N* k_eps*diag(n_test) # the k_eps part is for positive definiteness Kyinv = solve(Ky) Calculate posterior mean and covariance. post_mu = gp_mu(X_test) + tKstar %*% Kyinv %*% (y_train - gp_mu(X_train)) post_K = Kstarstar - tKstar %*% Kyinv %*% Kstar s2 = diag(post_K) # R = chol(post_K) # L = t(R) # L is used in alternative formulation below based on gaussSample.m Generate draws from posterior predictive. y_pp = data.frame(t(MASS::mvrnorm(n_post_pred, mu = post_mu, Sigma = post_K))) # alternative # y_pp = data.frame(replicate(n_post_pred, post_mu + L %*% rnorm(post_mu))) Visualize the Posterior Predictive Distribution Reshape data for plotting and create the plot. pp_data = data.frame( x = X_test, y = y_pp, fmean = post_mu, se_lower = post_mu - 2 * sqrt(s2), se_upper = post_mu + 2 * sqrt(s2) ) %&gt;% pivot_longer(starts_with(&#39;y&#39;), names_to = &#39;variable&#39;) g2 = ggplot(aes(x = x, y = value), data = pp_data) + geom_ribbon(aes(ymin = se_lower, ymax = se_upper, group = variable), fill = &#39;gray92&#39;) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;) + geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + labs(title = &#39;Posterior Predictive&#39;) g2 Plot prior and posterior predictive together. library(patchwork) g1 + g2 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoiseFree.R (noise-free) https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoisey.R (noisy) "],["neural-network.html", "Neural Network Example 1 Example 2 Example 3 Source", " Neural Network The following example follows Andrew Trask’s old blog post, which is nice because it tries to demonstrate a neural net in very few lines of code, much like this document’s goal. The data setup is very simple (only 4 observations!), and I keep the Python code essentially identical outside of very slight cosmetic (mostly name/space) changes. For more detail I suggest following the original posts, but I’ll add some context here and there. In addition, see the logistic regression chapter and gradient descent chapter. https://iamtrask.github.io/2015/07/12/basic-python-network/ https://iamtrask.github.io/2015/07/27/python-network-part2/ Example 1 Python In this initial example, while it can serve as instructive starting point for backpropagation, we’re not really using what most would call a neural net, but rather just an alternative way to estimate a logistic regression. layer_1 in this case is just the linear predictor after a nonlinear transformation (sigmoid). Note that in this particular example however, that the first column is perfectly correlated with the target y, which would cause a problem if no regularization were applied (a.k.a. separation). Description of the necessary objects from the blog. These will be consistent throughout the demo. X Input dataset matrix where each row is a training example y Output dataset matrix where each row is a training example layer_0 First Layer of the Network, specified by the input data layer_1 Second Layer of the Network, otherwise known as the hidden layer synapse_0 First layer of weights, Synapse 0, connecting layer_0 to layer_1. * Elementwise multiplication, so two vectors of equal size are multiplying corresponding values 1-to-1 to generate a final vector of identical size. - Elementwise subtraction, so two vectors of equal size are subtracting corresponding values 1-to-1 to generate a final vector of identical size. x.dot(y) If x and y are vectors, this is a dot product. If both are matrices, it’s a matrix-matrix multiplication. If only one is a matrix, then it’s vector matrix multiplication. import numpy as np # sigmoid function def nonlin(x, deriv = False): if(deriv == True): return x*(1 - x) return 1/(1 + np.exp(-x)) # input dataset X = np.array([ [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1] ]) # output dataset y = np.array([[0, 0, 1, 1]]).T # seed random numbers to make calculation # deterministic (just a good practice) np.random.seed(1) # initialize weights randomly with mean 0 (or just use np.random.uniform) synapse_0 = 2*np.random.random((3, 1)) - 1 for iter in np.arange(10000): # forward propagation layer_0 = X layer_1 = nonlin(np.dot(layer_0, synapse_0)) # how much did we miss? layer_1_error = y - layer_1 # multiply how much we missed by the # slope of the sigmoid at the values in layer_1 l1_delta = layer_1_error * nonlin(layer_1, True) # update weights synapse_0 += np.dot(layer_0.T, l1_delta) print(&quot;Output After Training:&quot;) Output After Training: print(np.append(layer_1, y, axis = 1)) [[0.00966449 0. ] [0.00786506 0. ] [0.99358898 1. ] [0.99211957 1. ]] R For R I make a couple changes, but it should be easy to follow from the original Python, and I keep the original comments. I convert the code into a function so that settings can be altered more easily. X = matrix( c(0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1), nrow = 4, ncol = 3, byrow = TRUE ) # output dataset y = c(0, 0, 1, 1) # seed random numbers to make calculation # deterministic (just a good practice) set.seed(1) # initialize weights randomly with mean 0 synapse_0 = matrix(runif(3, min = -1, max = 1), 3, 1) # sigmoid function nonlin &lt;- function(x, deriv = FALSE) { if (deriv) x * (1 - x) else plogis(x) } nn_1 &lt;- function(X, y, synapse_0, maxiter = 10000) { for (iter in 1:maxiter) { # forward propagation layer_0 = X layer_1 = nonlin(layer_0 %*% synapse_0) # how much did we miss? layer_1_error = y - layer_1 # multiply how much we missed by the # slope of the sigmoid at the values in layer_1 l1_delta = layer_1_error * nonlin(layer_1, deriv = TRUE) # update weights synapse_0 = synapse_0 + crossprod(layer_0, l1_delta) } list(layer_1 = layer_1, layer_1_error = layer_1_error, synapse_0 = synapse_0) } fit_nn = nn_1(X, y, synapse_0) message(&quot;Output After Training: \\n&quot;, paste0(capture.output(cbind(fit_nn$layer_1, y)), collapse = &#39;\\n&#39;)) Output After Training: y [1,] 0.009670417 0 [2,] 0.007864211 0 [3,] 0.993590571 1 [4,] 0.992115835 1 A key takeaway from this demonstration regards the update step. Using the derivative, we are getting the slope of the sigmoid function at the point of interest, e.g. the three points below. If a predicted probability (layer 1) is close to zero or 1, this suggests the prediction ‘confidence’ is high, the slopes are shallow, and the result is that there is less need for updating. For the others, e.g. close to x = 0, there will be relatively more updating, as the error will be greater. The last step is to compute the weight updates for each weight for each observation, sum them, and update the weights accordingly. So our update is a function of the error and the slope. If both are small, then there will be little update, but if there is larger error and/or less confident prediction, the result will ultimately be a larger update. Example 2 In the following example, we have similarly simple data, but technically a bit harder problem to estimate. In this case, y = 1 only if we apply the XOR function to columns 1 and 2. This makes the relationship between inputs and output a nonlinear one. To account for the nonlinearity we will have two ‘hidden’ layers, the first of four ‘nodes’, and the second a single node, which relates the transformed information to the target variable. In the hidden layer, we can think of each node as a linear combination of the input, with weights represented by paths (in the original post these weights/connections are collectively called ‘synapses’). This is followed by a nonlinear transformation for each node (e.g. sigmoid). The final predictions are a linear combination of the hidden layer, followed by another nonlinear transformation (sigmoid again). The last transformation needs to put the result between 0 and 1, but you could try other transformations for the first hidden layer. In fact sigmoid is rarely used except for the final layer. Python For the following, I remove defining layer_0, as it is just the input matrix X. But otherwise, only minor cleanup and more explicit names as before. import numpy as np def nonlin(x, deriv = False): if(deriv == True): return x*(1 - x) return 1/(1 + np.exp(-x)) X = np.array([ [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1] ]) y = np.array([ [0], [1], [1], [0] ]) np.random.seed(1) # randomly initialize our weights with mean 0 synapse_0 = 2*np.random.random((3, 4)) - 1 synapse_1 = 2*np.random.random((4, 1)) - 1 for j in np.arange(30000): # Feed forward through layers 0, 1, and 2 layer_1 = nonlin(np.dot(X, synapse_0)) layer_2 = nonlin(np.dot(layer_1, synapse_1)) # how much did we miss the target value? layer_2_error = y - layer_2 if (j% 10000) == 0: print(&quot;Error:&quot; + str(np.mean(np.abs(layer_2_error)))) # in what direction is the target value? # were we really sure? if so, don&#39;t change too much. layer_2_delta = layer_2_error*nonlin(layer_2, deriv=True) # how much did each layer_1 value contribute to the layer_2 error (according to the weights)? layer_1_error = layer_2_delta.dot(synapse_1.T) # in what direction is the target layer_1? # were we really sure? if so, don&#39;t change too much. layer_1_delta = layer_1_error * nonlin(layer_1, deriv=True) synapse_1 += layer_1.T.dot(layer_2_delta) synapse_0 += X.T.dot(layer_1_delta) Error:0.4964100319027255 Error:0.008584525653247153 Error:0.005789459862507812 print(&#39;Final error: &#39; + str(np.round(np.mean(np.abs(layer_2_error)), 5))) Final error: 0.00463 np.round(layer_1, 3) array([[0.696, 0.124, 0.923, 0.996], [0.18 , 0. , 0.017, 0.893], [0.995, 0.888, 0.023, 0.822], [0.947, 0.026, 0. , 0.122]]) np.round(np.append(layer_2, y, axis = 1), 3) array([[0.004, 0. ], [0.995, 1. ], [0.996, 1. ], [0.006, 0. ]]) np.round(synapse_0, 3) array([[ 4.407, 4.028, -6.225, -4.092], [-2.342, -5.706, -6.53 , -3.506], [ 0.826, -1.959, 2.488, 5.623]]) np.round(synapse_1, 3) array([[ -6.611], [ 6.857], [-10.069], [ 7.501]]) R In general, different weights can ultimately produce similar predictions, and combinations for a particular node are arbitrary. For example, node 1 in one network might become more like node 3 on a separate run with different starting points. So things are somewhat more difficult to compare across runs, let alone across different tools. However, what matters more is the final prediction, and you should see essentially the same result for R and Python. Again we create a function for repeated use. X = matrix( c(0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1), nrow = 4, ncol = 3, byrow = TRUE ) y = matrix(as.integer(xor(X[,1], X[,2])), ncol = 1) # make the relationship explicit set.seed(1) # or do randomly in same fashion synapse_0 = matrix(runif(12, -1, 1), 3, 4) synapse_1 = matrix(runif(12, -1, 1), 4, 1) # synapse_0 # synapse_1 nn_2 &lt;- function( X, y, synapse_0_start, synapse_1_start, maxiter = 30000, verbose = TRUE ) { synapse_0 = synapse_0_start synapse_1 = synapse_1_start for (j in 1:maxiter) { layer_1 = plogis(X %*% synapse_0) # 4 x 4 layer_2 = plogis(layer_1 %*% synapse_1) # 4 x 1 # how much did we miss the target value? layer_2_error = y - layer_2 if (verbose &amp;&amp; (j %% 10000) == 0) { message(glue::glue(&quot;Error: {mean(abs(layer_2_error))}&quot;)) } # in what direction is the target value? # were we really sure? if so, don&#39;t change too much. layer_2_delta = (y - layer_2) * (layer_2 * (1 - layer_2)) # how much did each l1 value contribute to the l2 error (according to the weights)? layer_1_error = layer_2_delta %*% t(synapse_1) # in what direction is the target l1? # were we really sure? if so, don&#39;t change too much. layer_1_delta = tcrossprod(layer_2_delta, synapse_1) * (layer_1 * (1 - layer_1)) # update synapse_1 = synapse_1 + crossprod(layer_1, layer_2_delta) synapse_0 = synapse_0 + crossprod(X, layer_1_delta) } list( layer_1_error = layer_1_error, layer_2_error = layer_2_error, synapse_0 = synapse_0, synapse_1 = synapse_1, layer_1 = layer_1, layer_2 = layer_2 ) } With function in place, we’re ready to try it out. fit_nn = nn_2( X, y, synapse_0_start = synapse_0, synapse_1_start = synapse_1, maxiter = 30000 ) Error: 0.0105538166393651 Error: 0.00729252475321203 Error: 0.0058973637409426 glue::glue(&#39;Final error: {round(mean(abs(fit_nn$layer_2_error)), 5)}&#39;) Final error: 0.0059 round(fit_nn$layer_1, 3) [,1] [,2] [,3] [,4] [1,] 0.259 0.889 0.364 0.445 [2,] 0.000 0.037 0.978 0.030 [3,] 0.946 1.000 0.984 0.020 [4,] 0.016 0.984 1.000 0.001 round(cbind(fit_nn$layer_2, y), 3) [,1] [,2] [1,] 0.002 0 [2,] 0.993 1 [3,] 0.994 1 [4,] 0.008 0 round(fit_nn$synapse_0, 3) [,1] [,2] [,3] [,4] [1,] 3.915 7.364 4.705 -3.669 [2,] -6.970 -5.351 4.337 -3.242 [3,] -1.050 2.079 -0.559 -0.221 round(fit_nn$synapse_1, 3) [,1] [1,] 10.988 [2,] -10.733 [3,] 5.576 [4,] -2.987 Comparison Let’s compare our results to the nnet package that comes with the base R installation. Note that it will include intercepts (a.k.a. biases) for each node, so it’s estimating more parameters in total. fit_nnet = nnet::nnet(X, y, size = 4) # weights: 21 initial value 1.152323 iter 10 value 0.998588 iter 20 value 0.558917 final value 0.000000 converged data.frame(coef(fit_nnet)) coef.fit_nnet. b-&gt;h1 5320.330 i1-&gt;h1 -49025.055 i2-&gt;h1 -15547.859 i3-&gt;h1 5320.491 b-&gt;h2 12329.141 i1-&gt;h2 -13914.491 i2-&gt;h2 -13782.790 i3-&gt;h2 12328.763 b-&gt;h3 22292.322 i1-&gt;h3 -18601.227 i2-&gt;h3 34270.828 i3-&gt;h3 22292.567 b-&gt;h4 22269.956 i1-&gt;h4 -3504.603 i2-&gt;h4 30916.959 i3-&gt;h4 22269.420 b-&gt;o 34687.508 h1-&gt;o -37130.643 h2-&gt;o 32075.031 h3-&gt;o -36018.926 h4-&gt;o -18446.081 fitted(fit_nnet) [,1] [1,] 0 [2,] 1 [3,] 1 [4,] 0 Example 3 The next example follows the code from the second post listed in the introduction. A couple of changes are seen here. We have a notably larger hidden layer (size 32). We also split the previous nonlin function into two parts. And finally, we add an alpha parameter, which is akin to the learning rate in gradient descent (stepsize in our previous implementation). It basically puts a control on the gradient so that we hopefully don’t make too large a jump in our estimated weights on each update. However, we can show what will happen as a result of setting the parameter to small and large values. Python As before, I make names more explicit, and other very minor updates to the original code. import numpy as np alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000] hidden_size = 32 # compute sigmoid nonlinearity def sigmoid(x): output = 1/(1 + np.exp(-x)) return output # convert output of sigmoid function to its derivative def sigmoid_output_to_derivative(output): return output*(1 - output) X = np.array([ [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1] ]) y = np.array([ [0], [1], [1], [0] ]) for alpha in alphas: print(&quot;\\nTraining With Alpha:&quot; + str(alpha)) np.random.seed(1) # randomly initialize our weights with mean 0 synapse_0 = 2*np.random.random((3, hidden_size)) - 1 synapse_1 = 2*np.random.random((hidden_size, 1)) - 1 for j in np.arange(30000): # Feed forward through layers input, 1, and 2 layer_1 = sigmoid(np.dot(X, synapse_0)) layer_2 = sigmoid(np.dot(layer_1, synapse_1)) # how much did we miss the target value? layer_2_error = layer_2 - y if (j% 10000) == 0: print(&quot;Error after &quot;+ str(j) +&quot; iterations:&quot; + str(np.mean(np.abs(layer_2_error)))) # in what direction is the target value? # were we really sure? if so, don&#39;t change too much. layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2) # how much did each l1 value contribute to the l2 error (according to the weights)? layer_1_error = layer_2_delta.dot(synapse_1.T) # in what direction is the target l1? # were we really sure? if so, don&#39;t change too much. layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1) synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta)) synapse_0 -= alpha * (X.T.dot(layer_1_delta)) Training With Alpha:0.001 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.49104946812904954 Error after 20000 iterations:0.4849763070274596 Training With Alpha:0.01 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.35637906164802124 Error after 20000 iterations:0.14693984546475994 Training With Alpha:0.1 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.030540490838554993 Error after 20000 iterations:0.019063872533418427 Training With Alpha:1 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.007360522342493724 Error after 20000 iterations:0.004972517050388164 Training With Alpha:10 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.0022489756615412743 Error after 20000 iterations:0.0015326357139237015 Training With Alpha:100 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.5 Error after 20000 iterations:0.5 Training With Alpha:1000 Error after 0 iterations:0.49643992250078794 Error after 10000 iterations:0.5 Error after 20000 iterations:0.5 Since alpha = 10 seems reasonable, let’s inspect the results just for that. Note that this value being the ‘best’ likely won’t hold on another random run, and in general we would assess this hyperparameter using cross-validation or other means. import numpy as np alpha = 10 # randomly initialize our weights with mean 0 np.random.seed(1) synapse_0 = 2*np.random.random((3, hidden_size)) - 1 synapse_1 = 2*np.random.random((hidden_size, 1)) - 1 for j in np.arange(30000): # Feed forward through layers input, 1, and 2 layer_1 = sigmoid(np.dot(X, synapse_0)) layer_2 = sigmoid(np.dot(layer_1, synapse_1)) # how much did we miss the target value? layer_2_error = layer_2 - y # in what direction is the target value? # were we really sure? if so, don&#39;t change too much. layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2) # how much did each l1 value contribute to the l2 error (according to the weights)? layer_1_error = layer_2_delta.dot(synapse_1.T) # in what direction is the target l1? # were we really sure? if so, don&#39;t change too much. layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1) synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta)) synapse_0 -= alpha * (X.T.dot(layer_1_delta)) np.append(np.round(layer_2, 4), y, axis = 1) array([[0.0011, 0. ], [0.9989, 1. ], [0.9989, 1. ], [0.0015, 0. ]]) R The following has little to no modification, but again creates a function for easier manipulation of inputs. # input dataset X = matrix( c(0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1), nrow = 4, ncol = 3, byrow = TRUE ) # output dataset y = matrix(c(0, 1, 1, 0), ncol = 1) alphas = c(0.001, 0.01, 0.1, 1, 10, 100, 1000) hidden_size = 32 # compute sigmoid nonlinearity sigmoid = plogis # already part of base R, no function needed # convert output of sigmoid function to its derivative sigmoid_output_to_derivative &lt;- function(output) { output * (1 - output) } nn_3 &lt;- function( X, y, hidden_size, alpha, maxiter = 30000, show_messages = FALSE ) { for (val in alpha) { if(show_messages) message(glue::glue(&quot;Training With Alpha: {val}&quot;)) set.seed(1) # randomly initialize our weights with mean 0 synapse_0 = matrix(runif(3 * hidden_size, -1, 1), 3, hidden_size) synapse_1 = matrix(runif(hidden_size), hidden_size, 1) for (j in 1:maxiter) { # Feed forward through layers input, 1, and 2 layer_1 = sigmoid(X %*% synapse_0) layer_2 = sigmoid(layer_1 %*% synapse_1) # how much did we miss the target value? layer_2_error = layer_2 - y if ((j %% 10000) == 0 &amp; show_messages) { message(glue::glue(&quot;Error after {j} iterations: {mean(abs(layer_2_error))}&quot;)) } # in what direction is the target value? # were we really sure? if so, don&#39;t change too much. layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2) # how much did each l1 value contribute to the l2 error (according to the weights)? layer_1_error = layer_2_delta %*% t(synapse_1) # in what direction is the target l1? # were we really sure? if so, don&#39;t change too much. layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1) synapse_1 = synapse_1 - val * crossprod(layer_1, layer_2_delta) synapse_0 = synapse_0 - val * crossprod(X, layer_1_delta) } } list( layer_1_error = layer_1_error, layer_2_error = layer_2_error, synapse_0 = synapse_0, synapse_1 = synapse_1, layer_1 = layer_1, layer_2 = layer_2 ) } With function in place, we are now ready to try it out. You can change whether you show the messages or not to compare with the Python. I don’t in this case for the sake of the document, but it won’t be overwhelming if you do interactively, and is recommended. We will also see that alpha = 10 is the better option under the default settings as it was with the Python code. Note that this simple demo code will probably not work well beyond a hidden size of 50 (add more iterations even then). set.seed(1) fit_nn = nn_3( X, y, hidden_size = 32, maxiter = 30000, alpha = alphas, show_messages = FALSE ) Let’s rerun at alpha = 10. set.seed(1) fit_nn = nn_3( X, y, hidden_size = 32, alpha = 10, show_messages = TRUE ) Training With Alpha: 10 Error after 10000 iterations: 0.00483502508464005 Error after 20000 iterations: 0.00207985328313795 Error after 30000 iterations: 0.00152092933542719 cbind(round(fit_nn$layer_2, 4), y) [,1] [,2] [1,] 0.0013 0 [2,] 0.9985 1 [3,] 0.9986 1 [4,] 0.0018 0 Comparison Again we can compare to nnet. fit_nnet = nnet::nnet(X, y, size = 32) # weights: 161 initial value 1.851187 iter 10 value 0.936564 iter 20 value 0.008594 final value 0.000065 converged fitted(fit_nnet) [,1] [1,] 0.0001274475 [2,] 0.9933000550 [3,] 0.9959851931 [4,] 0.0019116939 Source This was never on the original repo but I may put it there eventually. "],["elm.html", "Extreme Learning Machine Data Setup Function Estimation Comparison Supplemental Example Source", " Extreme Learning Machine A very simple implementation of an extreme learning machine for regression, which can be seen as a quick way to estimate a ‘good enough’ neural net, one that can be nearly as performant but with a lot less computational overhead. See elmNN and ELMR for some R package implementations. I add comparison to generalized additive models (elm/neural networks and GAMs are adaptive basis function models). http://www.extreme-learning-machines.org G.B. Huang, Q.Y. Zhu and C.K. Siew, Extreme Learning Machine: Theory and Applications. Data Setup One variable, complex function. library(tidyverse) library(mgcv) set.seed(123) n = 5000 x = runif(n) # x = rnorm(n) mu = sin(2*(4*x-2)) + 2* exp(-(16^2) * ((x-.5)^2)) y = rnorm(n, mu, .3) d = data.frame(x, y) qplot(x, y, color = I(&#39;#ff55001A&#39;)) Motorcycle accident data. data(&#39;mcycle&#39;, package = &#39;MASS&#39;) times = matrix(mcycle$times, ncol = 1) accel = mcycle$accel Function elm &lt;- function(X, y, n_hidden = NULL, active_fun = tanh) { # X: an N observations x p features matrix # y: the target # n_hidden: the number of hidden nodes # active_fun: activation function pp1 = ncol(X) + 1 w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden) # random weights h = active_fun(cbind(1, scale(X)) %*% w0) # compute hidden layer B = MASS::ginv(h) %*% y # find weights for hidden layer fit = h %*% B # fitted values list( fit = fit, loss = crossprod(y - fit), B = B, w0 = w0 ) } Estimation X_mat = as.matrix(x, ncol = 1) fit_elm = elm(X_mat, y, n_hidden = 100) str(fit_elm) List of 4 $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ... $ loss: num [1, 1] 442 $ B : num [1:100, 1] 217 -608 1408 -1433 -4575 ... $ w0 : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ... ggplot(aes(x, y), data = d) + geom_point(color = &#39;#ff55001A&#39;) + geom_line(aes(y = fit_elm$fit), color = &#39;#00aaff&#39;) cor(fit_elm$fit[,1], y)^2 [1] 0.8862518 fit_elm_mcycle = elm(times, accel, n_hidden = 100) cor(fit_elm_mcycle$fit[,1], accel)^2 [1] 0.8122349 Comparison We’ll compare to a generalized additive model with gaussian process approximation. fit_gam = gam(y ~ s(x, bs = &#39;gp&#39;, k = 20), data = d) summary(fit_gam)$r.sq [1] 0.8856188 d %&gt;% mutate(fit_elm = fit_elm$fit, fit_gam = fitted(fit_gam)) %&gt;% ggplot() + geom_point(aes(x, y), color = &#39;#ff55001A&#39;) + geom_line(aes(x, y = fit_elm), color = &#39;#1e90ff&#39;) + geom_line(aes(x, y = fit_gam), color = &#39;#990024&#39;) fit_gam_mcycle = gam(accel ~ s(times), data = mcycle) summary(fit_gam_mcycle)$r.sq [1] 0.7832988 mcycle %&gt;% ggplot(aes(times, accel)) + geom_point(color = &#39;#ff55001A&#39;) + geom_line(aes(y = fit_elm_mcycle$fit), color = &#39;#1e90ff&#39;) + geom_line(aes(y = fitted(fit_gam_mcycle)), color = &#39;#990024&#39;) Supplemental Example Yet another example with additional covariates. d = gamSim(eg = 7, n = 10000) Gu &amp; Wahba 4 term additive model, correlated predictors X = as.matrix(d[, 2:5]) y = d[, 1] n_nodes = c(10, 25, 100, 250, 500, 1000) The following estimation over multiple models will take several seconds. fit_elm = purrr::map(n_nodes, function(n) elm(X, y, n_hidden = n)) Now find the best fitting model. # estimate best_loss = which.min(map_dbl(fit_elm, function(x) x$loss)) fit_best = fit_elm[[best_loss]] A quick check of the fit. # str(fit_best) # qplot(fit_best$fit[, 1], y, alpha = .2) cor(fit_best$fit[, 1], y)^2 [1] 0.7241967 And compare again to mgcv. In this case, we’re comparing fit on test data of the same form. fit_gam = gam(y ~ s(x0) + s(x1) + s(x2) + s(x3), data = d) gam.check(fit_gam) Method: GCV Optimizer: magic Smoothing parameter selection converged after 15 iterations. The RMS GCV score gradient at convergence was 9.309879e-07 . The Hessian was positive definite. Model rank = 37 / 37 Basis dimension (k) checking results. Low p-value (k-index&lt;1) may indicate that k is too low, especially if edf is close to k&#39;. k&#39; edf k-index p-value s(x0) 9.00 4.71 1.00 0.48 s(x1) 9.00 4.89 1.00 0.35 s(x2) 9.00 8.96 0.99 0.20 s(x3) 9.00 1.00 1.00 0.47 summary(fit_gam)$r.sq [1] 0.6952978 test_data0 = gamSim(eg = 7) # default n = 400 Gu &amp; Wahba 4 term additive model, correlated predictors test_data = cbind(1, scale(test_data0[, 2:5])) # remember to use your specific activation function here elm_prediction = tanh(test_data %*% fit_best$w0) %*% fit_best$B gam_prediction = predict(fit_gam, newdata = test_data0) cor(data.frame(elm_prediction, gam_prediction), test_data0$y)^2 [,1] elm_prediction 0.6873090 gam_prediction 0.7185687 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/elm.R "],["rkhs.html", "Reproducing Kernel Hilbert Space Regression Data Setup Functions Estimation Comparison Example: Cubic Spline Source", " Reproducing Kernel Hilbert Space Regression This R code is based on Reproducing Kernel Hilbert Spaces for Penalized Regression: A tutorial, Nosedal-Sanchez et al. (2010), specifically, their code in the supplemental section. The original code had several issues as far as general R programming practices, and eventually appears to have been replaced in publication at some point, as did most of the corresponding supplemental text. I can no longer locate the original, so now follow the published code. The original data I was following was also replaced by the longley and mcycle data sets. To start, we will use an example for ridge regression, followed by a spline example. Data Setup library(tidyverse) data(longley) # avaiable in base R y = longley[,1] X = as.matrix(longley[,2:7]) X = apply(X, 2, scales::rescale, to = c(0, 1)) Functions Function to find the inverse of a matrix We can use base::solve, but this function avoids a computationally singular result. inverse &lt;- function(X, eps = 1e-12) { eig.X = eigen(X, symmetric = TRUE) P = eig.X[[2]] lambda = eig.X[[1]] ind = lambda &gt; eps lambda[ind] = 1/lambda[ind] lambda[!ind] = 0 P %*% diag(lambda) %*% t(P) } Reproducing Kernel rk &lt;- function(s, t) { init_len = length(s) rk = 0 for (i in 1:init_len) rk = s[i]*t[i] + rk rk } Gram matrix For the first example involving ridge regression, the gram function just produces tcrossprod(X). I generalize it in case a different kernel is desired, and add that as an additional argument. This will avoid having to redo the function later. gram &lt;- function(X, rkfunc = rk) { apply(X, 1, function(Row) apply(X, 1, function(tRow) rkfunc(Row, tRow)) ) } Ridge regression ridge &lt;- function(X, y, lambda) { Gramm = gram(X) # Gramm matrix (nxn) n = length(y) Q = cbind(1, Gramm) # design matrix S = rbind(0, cbind(0, Gramm)) M = crossprod(Q) + lambda*S M_inv = inverse(M) # inverse of M gamma_hat = crossprod(M_inv, crossprod(Q, y)) f_hat = Q %*% gamma_hat A = Q %*% M_inv %*% t(Q) tr_A = sum(diag(A)) # trace of hat matrix rss = crossprod(y - f_hat) # residual sum of squares gcv = n*rss / (n - tr_A)^2 # obtain GCV score list( f_hat = f_hat, gamma_hat = gamma_hat, beta_hat = c(gamma_hat[1], crossprod(gamma_hat[-1], X)), gcv = gcv ) } Estimation A simple direct search for the GCV optimal smoothing parameter can be made as follows: lambda = 10^seq(-6, 0, by = .1) gcv_search = map(lambda, function(lam) ridge(X, y, lam)) V = map_dbl(gcv_search, function(x) x$gcv) ridge_coefs = map_df( gcv_search, function(x) data.frame( value = x$beta_hat[-1], coef = colnames(X) ), .id = &#39;iter&#39; ) %&gt;% mutate(lambda = lambda[as.integer(iter)]) Compare with Figure 3 in the article. gcv_plot = qplot( lambda, V, geom = &#39;line&#39;, main = &#39;GCV score&#39;, ylab = &#39;GCV&#39; ) + scale_x_log10() beta_plot = ridge_coefs %&gt;% ggplot(aes(x = lambda, y = value, color = coef)) + geom_line() + scale_x_log10() + scico::scale_color_scico_d(end = .8) + labs(title = &#39;Betas Across Lambda&#39;) Pick the best model and obtain the estimates. fit_ridge = ridge(X, y, lambda[which.min(V)]) # fit optimal model gamma_hat = fit_ridge$gamma_hat beta_0 = fit_ridge$gamma_hat[1] # intercept beta_hat = crossprod(gamma_hat[-1,], X) # slope and noise term coefficients Comparison I add a comparison to glmnet, where setting alpha = 0 is equivalent to ridge regression. c(beta_0, beta_hat) [1] 82.7840043 54.1683427 5.3640251 1.3781910 -28.7948627 5.3956341 -0.6095799 fit_glmnet = glmnet::glmnet( X, y, alpha = 0, lambda = lambda, standardize = FALSE ) (Intercept) GNP Unemployed Armed.Forces Population Year Employed fit_ridge 82.784 54.168 5.364 1.378 -28.795 5.396 -0.610 fit_glmnet 82.329 69.783 6.963 1.245 -37.323 -1.497 -1.606 Example: Cubic Spline Data Setup For this example we’ll use the MASS::mcycle data. x = as.matrix(MASS::mcycle$times) x = scales::rescale(x, to = c(0, 1)) # rescale predictor to [0,1] y = MASS::mcycle$accel Functions Reproducing Kernel rk_spline &lt;- function(s, t) { return(.5 * min(s, t)^2 * max(s, t) - (1/6) * min(s, t)^3) } No need to redo the gram function do to previous change that accepts the kernel as an argument Smoothing Spline smoothing_spline &lt;- function(X, y, lambda) { Gramm = gram(X, rkfunc = rk_spline) # Gramm matrix (nxn) n = length(y) J = cbind(1, X) # matrix with a basis for the null space of the penalty Q = cbind(J, Gramm) # design matrix m = ncol(J) # dimension of the null space of the penalty S = matrix(0, n + m, n + m) # initialize S S[(m + 1):(n + m), (m + 1):(n + m)] = Gramm # non-zero part of S M = crossprod(Q) + lambda*S M_inv = inverse(M) # inverse of M gamma_hat = crossprod(M_inv, crossprod(Q, y)) f_hat = Q %*% gamma_hat A = Q %*% M_inv %*% t(Q) tr_A = sum(diag(A)) # trace of hat matrix rss = crossprod(y - f_hat) # residual sum of squares gcv = n * rss/(n - tr_A)^2 # obtain GCV score list( f_hat = f_hat, gamma_hat = gamma_hat, gcv = gcv ) } Estimation We’ll now get fits for multiple lambda values. lambda = 10^seq(-6, 0, by = .1) gcv_search = map(lambda, function(lam) smoothing_spline(x, y, lam)) V = map_dbl(gcv_search, function(x) x$gcv) Plot of GCV across different lambdas. Comparison I’ve added comparison to an additive model using mgcv. Compare the result to Figure 2 of the Supplementary Material. fit_rk = smoothing_spline(x, y, lambda[which.min(V)]) # fit optimal model fit_gam = mgcv::gam(y ~ s(x)) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/RKHSReg Current Supplemental Code You can peruse the supplemental section that shows the R code here. Original Supplemental Code This was the old original code from the supplemental section for the article, but was changed at some point (I can’t remember, it may have been at one of the author’s website). The R code on the repo follows these examples, while this document follows the currently accessible supplementary material. I used RStudio’s default cleanup to make the code a little easier to read, and maybe added a little spacing, but otherwise it is identical to what’s in the linked paper. A.1 ###### Data ######## set.seed(3) n &lt;- 20 x1 &lt;- runif(n) x2 &lt;- runif(n) X &lt;- matrix(c(x1, x2), ncol = 2) # design matrix y &lt;- 2 + 3 * x1 + rnorm(n, sd = 0.25) ##### function to find the inverse of a matrix #### my.inv &lt;- function(X, eps = 1e-12) { eig.X &lt;- eigen(X, symmetric = T) P &lt;- eig.X[[2]] lambda &lt;- eig.X[[1]] ind &lt;- lambda &gt; eps lambda[ind] &lt;- 1 / lambda[ind] lambda[!ind] &lt;- 0 ans &lt;- P %*% diag(lambda, nrow = length(lambda)) %*% t(P) return(ans) } ###### Reproducing Kernel ######### rk &lt;- function(s, t) { p &lt;- length(s) rk &lt;- 0 for (i in 1:p) { rk &lt;- s[i] * t[i] + rk } return((rk)) } ##### Gram matrix ####### get.gramm &lt;- function(X) { n &lt;- dim(X)[1] Gramm &lt;- matrix(0, n, n) #initializes Gramm array #i=index for rows #j=index for columns Gramm&lt;-as.matrix(Gramm) # Gramm matrix for (i in 1:n) { for (j in 1:n) { Gramm[i, j] &lt;- rk(X[i,], X[j,]) } } return(Gramm) } ridge.regression &lt;- function(X, y, lambda) { Gramm &lt;- get.gramm(X) #Gramm matrix (nxn) n &lt;- dim(X)[1] # n=length of y J &lt;- matrix(1, n, 1) # vector of ones dim Q &lt;- cbind(J, Gramm) # design matrix m &lt;- 1 # dimension of the null space of the penalty S &lt;- matrix(0, n + m, n + m) #initialize S S[(m + 1):(n + m), (m + 1):(n + m)] &lt;- Gramm #non-zero part of S M &lt;- (t(Q) %*% Q + lambda * S) M.inv &lt;- my.inv(M) # inverse of M gamma.hat &lt;- crossprod(M.inv, crossprod(Q, y)) f.hat &lt;- Q %*% gamma.hat A &lt;- Q %*% M.inv %*% t(Q) tr.A &lt;- sum(diag(A)) #trace of hat matrix rss &lt;- t(y - f.hat) %*% (y - f.hat) #residual sum of squares gcv &lt;- n * rss / (n - tr.A) ^ 2 #obtain GCV score return(list( f.hat = f.hat, gamma.hat = gamma.hat, gcv = gcv )) } # Plot of GCV lambda &lt;- 1e-8 V &lt;- rep(0, 40) for (i in 1:40) { V[i] &lt;- ridge.regression(X, y, lambda)$gcv #obtain GCV score lambda &lt;- lambda * 1.5 #increase lambda } index &lt;- (1:40) plot( 1.5 ^ (index - 1) * 1e-8, V, type = &quot;l&quot;, main = &quot;GCV score&quot;, lwd = 2, xlab = &quot;lambda&quot;, ylab = &quot;GCV&quot; ) # plot score i &lt;- (1:60)[V == min(V)] # extract index of min(V) opt.mod &lt;- ridge.regression(X, y, 1.5 ^ (i - 1) * 1e-8) #fit optimal model ### finding beta.0, beta.1 and beta.2 ########## gamma.hat &lt;- opt.mod$gamma.hat beta.hat.0 &lt;- opt.mod$gamma.hat[1]#intercept beta.hat &lt;- gamma.hat[2:21,] %*% X #slope and noise term coefficients #### Fitted Line Plot for Cubic Smoothing Spline #### plot(x[,1],y,xlab=&quot;x&quot;,ylab=&quot;response&quot;,main=&quot;Cubic Smoothing Spline&quot;) ; lines(x[,1],opt.mod$f.hat,type=&quot;l&quot;,lty=1,lwd=2,col=&quot;blue&quot;) ; A.2 A.2 RKHS solution applied to Cubic Smoothing Spline We consider a sample of size n = 50, (\\(y_1, y_2, y_3, ..., y_{50}\\)), from the model \\(y_i = sin(2πx_i) + ϵ_i\\) where ϵi has a N(0, 0.22) . The following code generates x and y… A simple direct search for the GCV optimal smoothing parameter can be made as follows: Now we have to find an optimal lambda using GCV… Below we give a function to find the cubic smoothing spline using the RKHS framework we discussed in Section 4.3. We also provide a graph with our estimation along with the true function and data. ###### Data ######## set.seed(3) n &lt;- 50 x &lt;- matrix(runif(n), nrow, ncol = 1) x.star &lt;- matrix(sort(x), nrow, ncol = 1) # sorted x, used by plot y &lt;- sin(2 * pi * x.star) + rnorm(n, sd = 0.2) #### Reproducing Kernel for &lt;f,g&gt;=int_0^1 f’’(x)g’’(x)dx ##### rk.1 &lt;- function(s, t) { return((1 / 2) * min(s, t) ^ 2) * (max(s, t) + (1 / 6) * (min(s, t)) ^ 3) } get.gramm.1 &lt;- function(X) { n &lt;- dim(X)[1] Gramm &lt;- matrix(0, n, n) #initializes Gramm array #i=index for rows #j=index for columns Gramm &lt;- as.matrix(Gramm) # Gramm matrix for (i in 1:n) { for (j in 1:n) { Gramm[i, j] &lt;- rk.1(X[i, ], X[j, ]) } } return(Gramm) } smoothing.spline &lt;- function(X, y, lambda) { Gramm &lt;- get.gramm.1(X) #Gramm matrix (nxn) n &lt;- dim(X)[1] # n=length of y J &lt;- matrix(1, n, 1) # vector of ones dim T &lt;- cbind(J, X) # matrix with a basis for the null space of the penalty Q &lt;- cbind(T, Gramm) # design matrix m &lt;- dim(T)[2] # dimension of the null space of the penalty S &lt;- matrix(0, n + m, n + m) #initialize S S[(m + 1):(n + m), (m + 1):(n + m)] &lt;- Gramm #non-zero part of S M &lt;- (t(Q) %*% Q + lambda * S) M.inv &lt;- my.inv(M) # inverse of M gamma.hat &lt;- crossprod(M.inv, crossprod(Q, y)) f.hat &lt;- Q %*% gamma.hat A &lt;- Q %*% M.inv %*% t(Q) tr.A &lt;- sum(diag(A)) #trace of hat matrix rss &lt;- t(y - f.hat) %*% (y - f.hat) #residual sum of squares gcv &lt;- n * rss / (n - tr.A) ^ 2 #obtain GCV score return(list( f.hat = f.hat, gamma.hat = gamma.hat, gcv = gcv )) } ### Now we have to find an optimal lambda using GCV... ### Plot of GCV lambda &lt;- 1e-8 V &lt;- rep(0, 60) for (i in 1:60) { V[i] &lt;- smoothing.spline(x.star, y, lambda)$gcv #obtain GCV score lambda &lt;- lambda * 1.5 #increase lambda } plot(1:60, V, type = &quot;l&quot;, main = &quot;GCV score&quot;, xlab = &quot;i&quot;) # plot score i &lt;- (1:60)[V == min(V)] # extract index of min(V) fit_rk &lt;- smoothing.spline(x.star, y, 1.5 ^ (i - 1) * 1e-8) #fit optimal model #Graph (Cubic Spline) plot( x.star, fit_rk$f.hat, type = &quot;l&quot;, lty = 2, lwd = 2, col = &quot;blue&quot;, xlab = &quot;x&quot;, ylim = c(-2.5, 1.5), xlim = c(-0.1, 1.1), ylab = &quot;response&quot;, main = &quot;Cubic Spline&quot; ) #predictions lines(x.star, sin(2 * pi * x.star), lty = 1, lwd = 2) #true legend( -0.1, -1.5, c(&quot;predictions&quot;, &quot;true&quot;), lty = c(2, 1), bty = &quot;n&quot;, lwd = c(2, 2), col = c(&quot;blue&quot;, &quot;black&quot;) ) points(x.star, y) "],["cfa.html", "Confirmatory Factor Analysis Data Setup Functions Estimation Comparison Source", " Confirmatory Factor Analysis This mostly follows Bollen (1989) for maximum likelihood estimation of a confirmatory factor analysis. In the following example we will examine a situation where there are two underlying (correlated) latent variables for 8 observed responses. The code as is will only work with this toy data set. Setup uses the psych and mvtnorm packages, and results are checked against the lavaan package. Data Setup For the data we will simulate observed variables with specific loadings on two latent constructs (factors). library(tidyverse) set.seed(123) # loading matrix lambda = matrix( c(1.0, 0.5, 0.8, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7, 0.6, 0.8), nrow = 2, byrow = TRUE ) # correlation of factors phi = matrix(c(1, .25, .25, 1), nrow = 2, byrow = TRUE) # factors and some noise factors = mvtnorm::rmvnorm(1000, mean = rep(0, 2), sigma = phi, &quot;chol&quot;) e = mvtnorm::rmvnorm(1000, sigma = diag(8)) # observed responses y = 0 + factors%*%lambda + e # Examine #dim(y) psych::describe(y) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 1000 0.05 1.44 0.05 0.05 1.42 -5.13 4.51 9.63 0.00 0.01 0.05 X2 2 1000 0.00 1.08 -0.01 0.00 1.04 -3.34 3.25 6.59 0.00 -0.06 0.03 X3 3 1000 0.01 1.27 0.05 0.01 1.17 -5.33 4.09 9.42 -0.06 0.32 0.04 X4 4 1000 0.00 1.14 -0.03 -0.01 1.13 -3.85 3.98 7.83 0.10 0.16 0.04 X5 5 1000 0.04 1.43 0.10 0.05 1.39 -4.43 5.21 9.63 -0.02 0.07 0.05 X6 6 1000 -0.02 1.22 -0.01 -0.02 1.27 -3.35 4.68 8.03 0.04 -0.10 0.04 X7 7 1000 0.02 1.14 0.02 0.01 1.10 -3.08 3.66 6.74 0.11 0.00 0.04 X8 8 1000 0.01 1.29 0.02 0.01 1.24 -3.68 4.50 8.18 -0.02 0.10 0.04 # round(cor(y), 2) # see the factor structure psych::cor.plot(cor(y)) # example exploratory fa #psych::fa(y, nfactors=2, rotate=&quot;oblimin&quot;) Functions We will have two separate estimation functions, one for the covariance matrix, and another for the correlation matrix. # measurement model, covariance approach # trace function, strangely absent from base R tr &lt;- function(mat) { sum(diag(mat), na.rm = TRUE) } cfa_cov &lt;- function (parms, data) { # Arguments- # parms: initial values (named) # data: raw data # Extract parameters by name l1 = c(1, parms[grep(&#39;l1&#39;, names(parms))]) # loadings for factor 1 l2 = c(1, parms[grep(&#39;l2&#39;, names(parms))]) # loadings for factor 2 cov0 = parms[grep(&#39;cov&#39;, names(parms))] # factor covariance, variances # Covariance matrix S = cov(data)*((nrow(data)-1)/nrow(data)) # ML covariance div by N rather than N-1, the multiplier adjusts # loading estimates lambda = cbind( c(l1, rep(0,length(l2))), c(rep(0,length(l1)), l2) ) # disturbances dist_init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist_init) # factor correlation phi_init = matrix(c(cov0[1], cov0[2], cov0[2], cov0[3]), 2, 2) #factor cov/correlation matrix # other calculations and log likelihood sigtheta = lambda%*%phi_init%*%t(lambda) + disturbs # in Bollen p + q (but for the purposes of this just p) = tr(data) pq = dim(data)[2] # a reduced version; Bollen 1989 p.107 # ll = -(log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq) # this should be the same as Mplus H0 log likelihood ll = ( (-nrow(data)*pq/2) * log(2*pi) ) - (nrow(data)/2) * ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) ) -ll } We can use the correlation matrix for standardized results. Lines correspond to those in cfa_cov. cfa_cor &lt;- function (parms, data) { l1 = parms[grep(&#39;l1&#39;, names(parms))] # loadings for factor 1 l2 = parms[grep(&#39;l2&#39;, names(parms))] # loadings for factor 2 cor0 = parms[grep(&#39;cor&#39;, names(parms))] # factor correlation S = cor(data) lambda = cbind( c(l1, rep(0,length(l2))), c(rep(0,length(l1)), l2) ) dist_init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist_init) phi_init = matrix(c(1, cor0, cor0, 1), ncol=2) sigtheta = lambda%*%phi_init%*%t(lambda) + disturbs pq = dim(data)[2] #ll = ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq ) ll = ( (-nrow(data)*pq/2) * log(2*pi) ) - (nrow(data)/2) * ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) ) -ll } Estimation Corresponding to the functions, we will get results based on the covariance and correlation matrix respectively. Raw Set initial values. par_init_cov = c(rep(1, 6), rep(.05, 8), rep(.5, 3)) names(par_init_cov) = rep(c(&#39;l1&#39;,&#39;l2&#39;, &#39;dist&#39;, &#39;cov&#39;), c(3, 3, 8, 3)) Estimate and extract. fit_cov = optim( par = par_init_cov, fn = cfa_cov, data = y, method = &quot;L-BFGS-B&quot;, lower = 0 ) loadings_cov = data.frame( f1 = c(1, fit_cov$par[1:3], rep(0, 4)), f2 = c(rep(0, 4), 1, fit_cov$par[4:6]) ) disturbances_cov = fit_cov$par[7:14] Standardized par_init_cor = c(rep(1, 8), rep(.05, 8), 0) #for cor names(par_init_cor) = rep(c(&#39;l1&#39;, &#39;l2&#39;, &#39;dist&#39;, &#39;cor&#39;), c(4, 4, 8, 1)) fit_cor = optim( par = par_init_cor, fn = cfa_cor, data = y, method = &quot;L-BFGS-B&quot;, lower = 0, upper = 1 ) loadings_cor = matrix( c(fit_cor$par[1:4], rep(0, 4), rep(0, 4), fit_cor$par[5:8]), ncol = 2 ) disturbances_cor = fit_cor$par[9:16] Comparison Gather results for summarizing. results = list( raw = list( loadings = round(data.frame(loadings_cov, Variances = disturbances_cov), 3), cov.fact = round(matrix(c(fit_cov$par[c(15, 16, 16, 17)]), ncol = 2) , 3) ), standardized = list( loadings = round( data.frame( loadings_cor, Variances = disturbances_cor, Rsq = (1 - disturbances_cor) ), 3), cor.fact = round(matrix(c(1, fit_cor$par[c(17, 17)], 1), ncol = 2), 3) ), # note inclusion of intercepts for total number of par fit_lav = data.frame( ll = fit_cov$value, AIC = 2*fit_cov$value + 2 * (length(par_init_cov) + ncol(y)), BIC = 2*fit_cov$value + log(nrow(y)) * (length(par_init_cov) + ncol(y)) ) ) results $raw $raw$loadings f1 f2 Variances 1 1.000 0.000 1.073 2 0.459 0.000 0.955 3 0.836 0.000 0.908 4 0.570 0.000 0.961 5 0.000 1.000 1.047 6 0.000 0.739 0.941 7 0.000 0.575 0.972 8 0.000 0.803 1.034 $raw$cov.fact [,1] [,2] [1,] 1.006 0.185 [2,] 0.185 0.989 $standardized $standardized$loadings X1 X2 Variances Rsq 1 0.696 0.000 0.516 0.484 2 0.426 0.000 0.819 0.181 3 0.661 0.000 0.563 0.437 4 0.504 0.000 0.746 0.254 5 0.000 0.697 0.514 0.486 6 0.000 0.604 0.636 0.364 7 0.000 0.502 0.748 0.252 8 0.000 0.618 0.618 0.382 $standardized$cor.fact [,1] [,2] [1,] 1.000 0.186 [2,] 0.186 1.000 $fit_lav ll AIC BIC 1 12497.68 25045.37 25168.06 Compare with lavaan. library(lavaan) y = data.frame(y) model = &#39; F1 =~ X1 + X2 + X3 + X4 F2 =~ X5 + X6 + X7 + X8 &#39; fit_lav = cfa( model, data = y, mimic = &#39;Mplus&#39;, estimator = &#39;ML&#39; ) fit_lav_std = cfa( model, data = y, mimic = &#39;Mplus&#39;, estimator = &#39;ML&#39;, std.lv = TRUE, std.ov = TRUE ) # note that lavaan does not count the intercepts among the free params for # AIC/BIC by default, (can get its result via -2 * as.numeric(lls) + k * # attr(lls, &quot;df&quot;)), but the mimic=&#39;Mplus&#39; should have them correspond to optim&#39;s # results summary(fit_lav, fit.measures = TRUE, standardized = TRUE) lavaan 0.6-7 ended normally after 30 iterations Estimator ML Optimization method NLMINB Number of free parameters 25 Number of observations 1000 Number of missing patterns 1 Model Test User Model: Test statistic 25.586 Degrees of freedom 19 P-value (Chi-square) 0.142 Model Test Baseline Model: Test statistic 1229.322 Degrees of freedom 28 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.995 Tucker-Lewis Index (TLI) 0.992 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -12497.683 Loglikelihood unrestricted model (H1) -12484.890 Akaike (AIC) 25045.366 Bayesian (BIC) 25168.060 Sample-size adjusted Bayesian (BIC) 25088.658 Root Mean Square Error of Approximation: RMSEA 0.019 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.035 P-value RMSEA &lt;= 0.05 1.000 Standardized Root Mean Square Residual: SRMR 0.018 Parameter Estimates: Standard errors Standard Information Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all F1 =~ X1 1.000 1.003 0.696 X2 0.459 0.046 10.043 0.000 0.460 0.426 X3 0.836 0.066 12.590 0.000 0.839 0.661 X4 0.570 0.050 11.450 0.000 0.572 0.504 F2 =~ X5 1.000 0.994 0.697 X6 0.739 0.056 13.239 0.000 0.735 0.604 X7 0.575 0.048 12.071 0.000 0.572 0.502 X8 0.803 0.060 13.386 0.000 0.799 0.618 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all F1 ~~ F2 0.185 0.046 4.014 0.000 0.186 0.186 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .X1 0.054 0.046 1.173 0.241 0.054 0.037 .X2 -0.004 0.034 -0.104 0.917 -0.004 -0.003 .X3 0.007 0.040 0.182 0.855 0.007 0.006 .X4 0.004 0.036 0.113 0.910 0.004 0.004 .X5 0.044 0.045 0.964 0.335 0.044 0.031 .X6 -0.019 0.038 -0.504 0.614 -0.019 -0.016 .X7 0.024 0.036 0.674 0.500 0.024 0.021 .X8 0.010 0.041 0.247 0.805 0.010 0.008 F1 0.000 0.000 0.000 F2 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .X1 1.073 0.087 12.356 0.000 1.073 0.516 .X2 0.955 0.047 20.128 0.000 0.955 0.819 .X3 0.908 0.065 13.887 0.000 0.908 0.563 .X4 0.961 0.051 18.885 0.000 0.961 0.746 .X5 1.048 0.079 13.336 0.000 1.048 0.514 .X6 0.941 0.056 16.867 0.000 0.941 0.636 .X7 0.972 0.050 19.255 0.000 0.972 0.748 .X8 1.034 0.063 16.411 0.000 1.034 0.618 F1 1.006 0.108 9.351 0.000 1.000 1.000 F2 0.989 0.100 9.853 0.000 1.000 1.000 Mplus If you have access to Mplus you can use Mplus Automation to prepare the data. The following code is in Mplus syntax and will produce the above model. library(MplusAutomation) prepareMplusData(data.frame(y), &quot;factsim.dat&quot;) MODEL: F1 BY X1-X4; F2 BY X5-X8; results: STDYX; Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/cfa.R "],["bayesian.html", "Introduction to Bayesian Methods", " Introduction to Bayesian Methods The following demonstrations will likely assume more background knowledge, but otherwise take a similar approach. For example, the model fitting functions in the previous demonstrations are now replaced with Stan code. I start with a demo followed by the simplest of models such as a t-test and linear regression to get one comfortable. For more introduction, see my introduction to bayesian method, and in particular, themaximum likelihood section of this document, which is an update of the appendix in that introduction. Note that some of the old code is now easily accomplished with tools like rstanarm or brms (e.g. for standard linear and mixed models), or Stan even has built in functions (e.g. gaussian process covariance functions). As such, I didn’t copy all of my old efforts to this document. So you can take a look at the old repo for a few more demos I probably won’t include here, including in other languages like Bugs/Jags. "],["bayesian-basics.html", "Basics Prior, likelihood, &amp; posterior distributions Prior Likelihood Posterior Posterior predictive Source", " Basics Prior, likelihood, &amp; posterior distributions The following is an attempt to provide a small example to show the connection between prior distribution, likelihood and posterior distribution. It is taken directly from my document with mostly just cleaned up code and visualization. Let’s say we want to estimate the probability that a soccer/football player will score a penalty kick in a shootout. We will employ the binomial distribution to model this. Our goal is to estimate a parameter \\(\\theta\\), the probability that the random knucklehead from your favorite football team will score the penalty in an overtime shootout. Let’s say that for this match it takes 10 shots per team before the game is decided. We can represent the following data for your team as follows, as well as set up some other things for later. shots = c(&#39;goal&#39;,&#39;goal&#39;,&#39;goal&#39;,&#39;miss&#39;,&#39;miss&#39;, &#39;goal&#39;,&#39;goal&#39;,&#39;miss&#39;,&#39;miss&#39;,&#39;goal&#39;) # convert to numeric, arbitrarily picking goal = 1, miss = 0 shots_01 = as.numeric(shots == &#39;goal&#39;) N = length(shots) # sample size n_goals = sum(shots == &#39;goal&#39;) # number of shots made n_missed = sum(shots == &#39;miss&#39;) # number of those miss Recall the binomial distribution where we specify the number of trials for a particular observation and the probability of an event. Let’s look at the distribution for a couple values for \\(\\theta\\) equal to .5 and .85, and \\(N=10\\) observations. We will repeat this 1000 times. set.seed(1234) x1 = rbinom(1000, size = 10, p = .5) x2 = rbinom(1000, size = 10, p = .85) mean(x1) [1] 5.043 qplot(x1, geom = &#39;histogram&#39;) mean(x2) [1] 8.569 qplot(x2, geom = &#39;histogram&#39;) The histograms are not shown, but we can see the means are roughly around \\(N*p\\) as we expect with the binomial. Prior For our current situation, we don’t know \\(\\theta\\) and are trying to estimate it. We will start by supplying some possible values. To keep things simple, we’ll only consider 10 values that fall between 0 and 1. theta = seq(from = 1/(N + 1), to = N/(N + 1), length = 10) For the Bayesian approach we must choose a prior distribution representing our initial beliefs about the estimates we might potentially consider. I provide three possibilities, and note that any one of them would work just fine for this situation. We’ll go with a triangular distribution, which will put most of the weight toward values around \\(.5\\). While we will only work with one prior, do play with the others. ### prior distribution # triangular as in Kruschke text example p_theta = pmin(theta, 1 - theta) # uniform # p_theta = dunif(theta) # beta prior with mean = .5 # p_theta = dbeta(theta, 10, 10) # Normalize so that values sum to 1 p_theta = p_theta / sum(p_theta) So, given some estimate of \\(\\theta\\), we have a probability of that value based on our chosen prior. Likelihood Next we will compute the likelihood of the data given some value of \\(\\theta\\). Generally, the likelihood for some target variable \\(y\\), with observed values \\(i \\dots n\\), given some (set of) parameter(s) \\(\\theta\\), can be expressed as follows: \\[p(y|\\theta) = \\prod_{i}^{n} p(y_i|\\theta)\\] Specifically, the likelihood function for the binomial can be expressed as: \\[p(y|\\theta) = {N \\choose k}\\, \\theta^k\\, (1-\\theta)^{N-k}\\] where \\(N\\) is the total number of possible times in which the event of interest could occur, and \\(k\\) number of times the event of interest occurs. Our maximum likelihood estimate in this simple setting would simply be the proportion of events witnessed out of the total number of samples. We’ll use the formula presented above. Technically, the first term is not required, but it serves to normalize the likelihood as we did with the prior. p_data_given_theta = choose(N, n_goals) * theta^n_goals * (1-theta)^n_missed Posterior Given the prior and likelihood, we can now compute the posterior distribution via Bayes theorem. The only thing left to calculate is the denominator from Bayes theorem, then plug in the rest. p_data = p_data_given_theta*p_theta # marginal probability of the data p_theta_given_data = p_data_given_theta*p_theta / sum(p_data) # Bayes theorem Now let’s examine what all we’ve got. theta prior likelihood posterior 0.091 0.033 0.000 0.000 0.182 0.067 0.003 0.002 0.273 0.100 0.024 0.018 0.364 0.133 0.080 0.079 0.455 0.167 0.164 0.203 0.545 0.167 0.236 0.293 0.636 0.133 0.244 0.242 0.727 0.100 0.172 0.128 0.818 0.067 0.069 0.034 0.909 0.033 0.008 0.002 Starting with the prior column, we can see that with the triangular distribution, we’ve given most of our prior probability to the middle values with probability tapering off somewhat slowly towards either extreme. The likelihood, on the other hand, suggests the data is most likely for \\(\\theta\\) values .55-.64, though we know the specific maximum likelihood estimate for \\(\\theta\\) is the proportion for the sample, or .6. Our posterior estimate will therefore fall somewhere between the prior and likelihood estimates, and we can see it has shifted the bulk of the probability slightly away from the most likely values suggested by the prior distribution, and towards a \\(\\theta\\) value suggested by the data of .6. Let’s go ahead and see what the mean is: posterior_mean = sum(p_theta_given_data*theta) posterior_mean [1] 0.5623611 So, we start with a prior centered on a value of \\(\\theta=.5\\), add data whose ML estimate is \\(\\theta=.6\\), and our posterior distribution suggests we end up somewhere in between. We can perhaps understand this further via the following visualizations. In each of these the prior is represented by the blue density, the likelihood by the red, and the posterior by purple. This first is based on a different prior than just used in our example, and instead employs the beta distribution noted among the possibilities in the code above. The beta distribution is highly flexible, and with shape parameters \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) set to 10 and 10 we get a symmetric distribution centered on \\(\\theta = .5\\). This would actually be a somewhat stronger prior than we might normally want to use, but serves to illustrate a point. The mean of the beta is \\(\\frac{\\mathcal{A}}{\\mathcal{A}+\\mathcal{B}}\\), and thus has a nice interpretation as a prior based on data with sample size equal to \\(\\mathcal{A}+\\mathcal{B}\\). The posterior distribution that results would have a mean somewhere between the maximum likelihood value and that of the prior. With the stronger prior, the posterior is pulled closer to it. The second utilizes a more diffuse prior of \\(\\beta(2,2)\\). The result of using the vague prior is that the likelihood gets more weight with regard to the posterior. In fact, if we used a uniform distribution, we would essentially be doing the equivalent of maximum likelihood estimation. In that sense, many of the commonly used methods that implement maximum likelihood can be seen as a special case of a Bayesian approach. The third graph employs the initial \\(\\beta(10,10)\\) prior again, but this time we add more observations to the data. This serves to give more weight to the likelihood, which is what we want. As scientists, we’d want the evidence, i.e. data, to eventually outweigh our prior beliefs about the state of things the more we have of it. For an interactive demonstration of the above, see this. Posterior predictive At this point it is hoped you have a better understanding of the process of Bayesian estimation. Conceptually, one starts with prior beliefs about the state of the world and adds evidence to one’s understanding, ultimately coming to a conclusion that serves as a combination of evidence and prior belief. More concretely, we have a prior distribution regarding parameters, a distribution regarding the data given those parameters, and finally a posterior distribution that is the weighted combination of the two. However, there is yet another distribution of interest to us- the posterior predictive distribution. Stated simply, once we have the posterior distribution for \\(\\theta\\), we can then feed (possibly new or unobserved) data into the data generating process and get distributions for \\(\\tilde{y}\\). Where \\(\\tilde{y}\\) can regard any potential observation, we can distinguish it from the case where we use the current data to produce \\(y^{\\textrm{Rep}}\\), i.e. a replicate of \\(y\\). For example, if a regression model had predictor variables \\(X\\), the predictor variables are identical for producing \\(y^{\\textrm{Rep}}\\) as they were in modeling \\(y\\). However, \\(\\tilde{y}\\) might be based on any values \\(\\tilde{X}\\) that might be feasible or interesting, whether actually observed in the data or not. Since \\(y^{\\textrm{Rep}}\\) is an attempt to replicate the observed data based on the parameters \\(\\theta\\), we can compare our simulated data to the observed data to see how well they match. We can implement the simulation process with the data and model at hand, given a sample of values of \\(\\theta\\) drawn from the posterior. I provide the results of such a process with the following graph. Each bar graph of frequencies represents a replication of the 10 shots taken, i.e. \\(y^{\\textrm{Rep}}\\), given an estimate of \\(\\theta\\) from the posterior distribution (16 total). These are eleven plausible sets of 10 makes and misses, given \\(\\theta\\) shown against the observed. library(rstanarm) shotres = stan_glm( shots_01 ~ 1, data = data.frame(shots_01), family = &#39;binomial&#39;, iter = 500, warmup = 250, prior = student_t() ) # pp_check(shotres) With an understanding of the key elements of Bayesian inference in hand, we can proceed to more complex settings. Source Original code available at: https://m-clark.github.io/bayesian-basics/example.html "],["bayesian-t-test.html", "Bayesian t-test Data Setup Model Code Estimation Comparison Visualization Source", " Bayesian t-test The following is based on Kruschke’s 2012 JEP article ‘Bayesian estimation supersedes the t-test (BEST)’ with only minor changes to Stan model. It uses the JAGS/BUGS code in the paper’s Appendix B as the reference. Data Setup Create two groups of data for comparison. Play around with the specs if you like. library(tidyverse) set.seed(1234) N_g = 2 # N groups N_1 = 50 # N for group 1 N_2 = 50 # N for group 2 mu_1 = 1 # mean for group 1 mu_2 = -.5 # mean for group 1 sigma_1 = 1 # sd for group 1 sigma_2 = 1 # sd for group 1 y_1 = rnorm(N_1, mu_1, sigma_1) y_2 = rnorm(N_2, mu_2, sigma_2) y = c(y_1, y_2) group_id = as.numeric(gl(2, N_1)) # if unbalanced # group = 1:2 # group_id = rep(group, c(N_1,N_2)) d = data.frame(y, group_id) tidyext::num_by(d, y, group_id) # personal package, not necessary # A tibble: 2 x 11 # Groups: group_id [2] group_id Variable N Mean SD Min Q1 Median Q3 Max `% Missing` &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 y 50 0.5 0.9 -1.3 0 0.5 1 3.4 0 2 2 y 50 -0.4 1 -2.3 -1.1 -0.5 0.3 2 0 Model Code The Stan code. data { int&lt;lower = 1&gt; N; // sample size int&lt;lower = 2&gt; N_g; // number of groups vector[N] y; // response int&lt;lower = 1, upper = N_g&gt; group_id[N]; // group ID } transformed data{ real y_mean; // mean of y; see mu prior y_mean = mean(y); } parameters { vector[2] mu; // estimated group means and sd vector&lt;lower = 0&gt;[2] sigma; // Kruschke puts upper bound as well; ignored here real&lt;lower = 0, upper = 100&gt; nu; // df for t distribution } model { // priors // note that there is a faster implementation of this for stan, // and that the sd here is more informative than in Kruschke paper mu ~ normal(y_mean, 10); sigma ~ cauchy(0, 5); // Based on Kruschke; makes average nu 29 // might consider upper bound, as if too large then might as well switch to normal nu ~ exponential(1.0/29); // likelihood for (n in 1:N) { y[n] ~ student_t(nu, mu[group_id[n]], sigma[group_id[n]]); // compare to normal; remove all nu specifications if you do this; //y[n] ~ normal(mu[group_id[n]], sigma[group_id[n]]); } } generated quantities { vector[N] y_rep; // posterior predictive distribution real mu_diff; // mean difference real cohens_d; // effect size; see footnote 1 in Kruschke paper real CLES; // common language effect size real CLES2; // a more explicit approach; the mean should roughly equal CLES for (n in 1:N) { y_rep[n] = student_t_rng(nu, mu[group_id[n]], sigma[group_id[n]]); } mu_diff = mu[1] - mu[2]; cohens_d = mu_diff / sqrt(sum(sigma)/2); CLES = normal_cdf(mu_diff / sqrt(sum(sigma)), 0, 1); CLES2 = student_t_rng(nu, mu[1], sigma[1]) - student_t_rng(nu, mu[2], sigma[2]) &gt; 0; } Estimation Run the model. stan_data = list( N = length(y), N_g = N_g, group_id = group_id, y = y ) library(rstan) fit = sampling( bayes_t_test, data = stan_data, thin = 4 ) Comparison Let’s take a look. print( fit, pars = c(&#39;mu&#39;, &#39;sigma&#39;, &#39;mu_diff&#39;, &#39;cohens_d&#39;, &#39;CLES&#39;, &#39;CLES2&#39;, &#39;nu&#39;), probs = c(.025, .5, .975), digits = 3 ) Inference for Stan model: e9624a2b7528e50b8f8b9d0fb2b3c58c. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat mu[1] 0.512 0.004 0.125 0.279 0.508 0.755 1139 0.997 mu[2] -0.386 0.005 0.156 -0.680 -0.392 -0.083 899 0.999 sigma[1] 0.825 0.004 0.116 0.586 0.825 1.063 900 0.998 sigma[2] 1.017 0.004 0.123 0.795 1.010 1.275 820 1.002 mu_diff 0.898 0.006 0.199 0.500 0.910 1.270 960 0.997 cohens_d 0.939 0.007 0.209 0.513 0.949 1.318 890 0.998 CLES 0.744 0.002 0.048 0.642 0.749 0.824 905 0.998 CLES2 0.768 0.014 0.422 0.000 1.000 1.000 966 1.002 nu 27.231 0.671 21.394 3.815 21.489 83.493 1018 0.997 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:12:28 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Now we extract quantities of interest for more processing/visualization. Compare population and observed data values to estimates in summary to the observed mean difference. y_rep = extract(fit, par = &#39;y_rep&#39;)$y_rep mu_diff = extract(fit, par = &#39;mu_diff&#39;)$mu_diff init = d %&gt;% group_by(group_id) %&gt;% summarise( mean = mean(y), sd = sd(y), ) means = init$mean sds = init$sd mu_1 - mu_2 # based on population values [1] 1.5 abs(diff(means)) # observed in data [1] 0.9074175 Compare estimated Cohen’s d. cohens_d = extract(fit, par = &#39;cohens_d&#39;)$cohens_d (mu_1 - mu_2) / sqrt((sigma_1 ^ 2 + sigma_2 ^ 2)/2) # population [1] 1.5 (means[1] - means[2]) / sqrt(sum(sds^2)/2) # observed [1] 0.9411788 mean(cohens_d) # bayesian estimate [1] 0.9388044 Common language effect size is the probability that a randomly selected score from one population will be greater than a randomly sampled score from the other. CLES = extract(fit, par=&#39;CLES&#39;)$CLES pnorm((mu_1 - mu_2) / sqrt(sigma_1^2 + sigma_2^2)) # population [1] 0.8555778 pnorm((means[1] - means[2]) / sqrt(sum(sds^2))) # observed [1] 0.7471391 mean(CLES) # bayesian estimate [1] 0.7443192 Compare to Welch’s t-test that does not assume equal variances. t.test(y_1, y_2) Welch Two Sample t-test data: y_1 and y_2 t = 4.7059, df = 95.633, p-value = 8.522e-06 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.5246427 1.2901923 sample estimates: mean of x mean of y 0.5469470 -0.3604705 Compare to BEST. Note that it requires coda, whose traceplot function will mask rstan’s. library(BEST) BESTout = BESTmcmc( y_1, y_2, numSavedSteps = 10000, thinSteps = 10, burnInSteps = 2000 ) summary(BESTout) mean median mode HDI% HDIlo HDIup compVal %&gt;compVal mu1 0.513 0.512 0.530 95 0.259 0.7530 mu2 -0.381 -0.380 -0.375 95 -0.686 -0.0823 muDiff 0.894 0.894 0.886 95 0.522 1.2955 0 100.00 sigma1 0.834 0.830 0.824 95 0.615 1.0544 sigma2 1.022 1.015 1.015 95 0.804 1.2609 sigmaDiff -0.188 -0.184 -0.156 95 -0.490 0.1061 0 9.47 nu 30.891 21.864 10.501 95 2.577 87.3050 log10nu 1.339 1.340 1.235 95 0.658 2.0477 effSz 0.964 0.963 0.979 95 0.527 1.4203 0 100.00 Visualization We can plot the posterior predictive distribution vs. observed data density. library(bayesplot) pp_check( stan_data$y, rstan::extract(fit, par = &#39;y_rep&#39;)$y_rep[1:10, ], fun = &#39;dens_overlay&#39; ) We can expand this to incorporate the separate groups and observed values. Solid lines and dots represent the observed data. Plots from the BEST model. walk(c(&quot;mean&quot;, &quot;sd&quot;, &quot;effect&quot;, &quot;nu&quot;), function(p) plot(BESTout, which = p)) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstant_testBEST.R "],["bayesian-linear-regression.html", "Bayesian Linear Regression Data Setup Model Code Estimation Comparison Visualize Source", " Bayesian Linear Regression The following provides a simple working example of a standard regression model using Stan via rstan. It will hopefully to allow some to more easily jump in to using Stan if they are comfortable with R. You would normally just use rstanarm or brms for such a model however. Data Setup Create a correlation matrix of one’s choosing assuming response as last column/row. This approach allows for some collinearity in the predictors. library(tidyverse) cormat = matrix( c( 1, .2, -.1, .3, .2, 1, .1, .2, -.1, .1, 1, .1, .3, .2, .1, 1 ), ncol = 4, byrow = TRUE ) cormat [,1] [,2] [,3] [,4] [1,] 1.0 0.2 -0.1 0.3 [2,] 0.2 1.0 0.1 0.2 [3,] -0.1 0.1 1.0 0.1 [4,] 0.3 0.2 0.1 1.0 cormat = Matrix::nearPD(cormat, corr = TRUE)$mat n = 1000 means = rep(0, ncol(cormat)) d = MASS::mvrnorm(n, means, cormat, empirical = TRUE) colnames(d) = c(&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;y&#39;) d[,&#39;y&#39;] = d[,&#39;y&#39;] - .1 # unnecessary, just to model a non-zero intercept str(d) num [1:1000, 1:4] -0.114 -0.409 4.198 1.098 0.331 ... - attr(*, &quot;dimnames&quot;)=List of 2 ..$ : NULL ..$ : chr [1:4] &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;y&quot; cor(d) X1 X2 X3 y X1 1.0 0.2 -0.1 0.3 X2 0.2 1.0 0.1 0.2 X3 -0.1 0.1 1.0 0.1 y 0.3 0.2 0.1 1.0 # Prepare for Stan # create X (add intercept column) and y for vectorized version later X = cbind(1, d[,1:3]); colnames(X) = c(&#39;Intercept&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;) y = d[,4] Model Code Initial preparation, create the data list object. dat = list( N = n, k = 4, y = y, X = X ) Create the Stan model code. data { // Data block; declarations only int&lt;lower = 0&gt; N; // Sample size int&lt;lower = 0&gt; k; // Dimension of model matrix matrix [N, k] X; // Model Matrix vector[N] y; // Target } /* transformed data { // Transformed data block; declarations and statements. None needed here. } */ parameters { // Parameters block; declarations only vector[k] beta; // Coefficient vector real&lt;lower = 0&gt; sigma; // Error scale } transformed parameters { // Transformed parameters block; declarations and statements. } model { // Model block; declarations and statements. vector[N] mu; mu = X * beta; // Linear predictor // priors beta ~ normal(0, 1); sigma ~ cauchy(0, 1); // With sigma bounded at 0, this is half-cauchy // likelihood y ~ normal(mu, sigma); } generated quantities { // Generated quantities block; declarations and statements. real rss; real totalss; real R2; // Calculate Rsq as a demonstration vector[N] y_hat; y_hat = X * beta; rss = dot_self(y - y_hat); totalss = dot_self(y - mean(y)); R2 = 1 - rss/totalss; } Estimation Run the model and examine results. The following assumes a character string or file (bayes_linreg) of the previous model code. library(rstan) fit = sampling( bayes_linreg, data = dat, thin = 4, verbose = FALSE ) Note the pars argument in the following. You must specify desired parameters or it will print out everything, including the y_hat, i.e. expected values. Also note that by taking into account the additional uncertainty estimating sigma, you get a shrunken Rsq (see Gelman &amp; Pardoe 2006 sec. 3). print( fit, digits_summary = 3, pars = c(&#39;beta&#39;, &#39;sigma&#39;, &#39;R2&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: 17507cf73e3a44aeee4c4249d3521a85. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat beta[1] -0.100 0.001 0.030 -0.158 -0.099 -0.041 920 0.997 beta[2] 0.284 0.001 0.030 0.225 0.284 0.340 1067 1.003 beta[3] 0.133 0.001 0.032 0.072 0.132 0.196 1010 1.001 beta[4] 0.116 0.001 0.029 0.061 0.115 0.175 932 1.001 sigma 0.940 0.001 0.021 0.899 0.940 0.981 1147 1.000 R2 0.120 0.000 0.003 0.114 0.120 0.123 918 0.998 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:33:03 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Comparison Compare to basic lm result. modlm = lm(y ~ ., data.frame(d)) # Compare summary(modlm) Call: lm(formula = y ~ ., data = data.frame(d)) Residuals: Min 1Q Median 3Q Max -2.70296 -0.68588 0.01811 0.66373 3.10820 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.10000 0.02965 -3.372 0.000774 *** X1 0.28526 0.03051 9.349 &lt; 2e-16 *** X2 0.13141 0.03051 4.307 1.82e-05 *** X3 0.11538 0.03004 3.840 0.000131 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9377 on 996 degrees of freedom Multiple R-squared: 0.1234, Adjusted R-squared: 0.1208 F-statistic: 46.73 on 3 and 996 DF, p-value: &lt; 2.2e-16 Visualize Visualize the posterior predictive distribution. # shinystan::launch_shinystan(fit) # diagnostic plots library(bayesplot) pp_check( dat$y, rstan::extract(fit, par = &#39;y_hat&#39;)$y_hat[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstan_linregwithprior.R "],["bayesian-beta-regression.html", "Bayesian Beta Regression Data Setup Model Code Estimation Comparison Visualization Source", " Bayesian Beta Regression The following provides an example of beta regression using Stan/rstan, with comparison to results with R’s betareg package. Data Setup Several data sets from are available betareg to play with, but as they are a bit problematic in one way or another I instead focus on a simple simulated data set. library(tidyverse) library(betareg) # Data for assessing the contribution of non-verbal IQ to children&#39;s reading # skills in dyslexic and non-dyslexic children. # issue: 30% of data has a value of .99 # data(&quot;ReadingSkills&quot;) # ?ReadingSkills # y = ReadingSkills$accuracy # # brmod = betareg(accuracy ~ dyslexia + iq, data = ReadingSkills) # X = cbind(1, scale(model.matrix(brmod)[,c(&#39;dyslexia&#39;,&#39;iq&#39;)], scale=F)) # or this, issue: ignores batch effects # data(&quot;GasolineYield&quot;) # ?GasolineYield # # y = GasolineYield$yield # X = cbind(1, scale(GasolineYield[,c(&#39;gravity&#39;,&#39;pressure&#39;,&#39;temp&#39;)])) # yet another data option, issue: only two binary predictors # data(WeatherTask) # ?WeatherTask # # y = WeatherTask$agreement # brmod = betareg(agreement ~ priming + eliciting, data = WeatherTask) # X = model.matrix(brmod) # simulated data; probably a better illustration, or at least better behaved one. set.seed(1234) N = 500 # Sample size x_1 = rnorm(N) # Predictors x_2 = rnorm(N) X = cbind(1, x_1, x_2) beta = c(-1, .2, -.3) mu = plogis(X %*% beta) # add noise if desired + rnorm(N, sd=.01) phi = 10 A = mu * phi B = (1 - mu) * phi y = rbeta(N, A, B) d = data.frame(x_1, x_2, y) qplot(y, geom=&#39;density&#39;) Model Code data { int&lt;lower=1&gt; N; // sample size int&lt;lower=1&gt; K; // K predictors vector&lt;lower=0,upper=1&gt;[N] y; // response matrix[N,K] X; // predictor matrix } parameters { vector[K] theta; // reg coefficients real&lt;lower=0&gt; phi; // dispersion parameter } transformed parameters{ vector[K] beta; beta = theta * 5; // same as beta ~ normal(0, 5); fairly diffuse } model { // model calculations vector[N] LP; // linear predictor vector[N] mu; // transformed linear predictor vector[N] A; // parameter for beta distn vector[N] B; // parameter for beta distn LP = X * beta; for (i in 1:N) { mu[i] = inv_logit(LP[i]); } A = mu * phi; B = (1.0 - mu) * phi; // priors theta ~ normal(0, 1); phi ~ cauchy(0, 5); // different options for phi //phi ~ inv_gamma(.001, .001); //phi ~ uniform(0, 500); // put upper on phi if using this // likelihood y ~ beta(A, B); } generated quantities { vector[N] y_rep; for (i in 1:N) { real mu; real A; real B; mu = inv_logit(X[i] * beta); A = mu * phi; B = (1.0 - mu) * phi; y_rep[i] = beta_rng(A, B); } } Estimation We create a data list for Stan and estimate the model. # Stan data list stan_data = list(N = length(y), K = ncol(X), y = y, X = X) library(rstan) fit = sampling( bayes_beta, data = stan_data, thin = 4, verbose = FALSE ) # model for later comparison brmod = betareg(y ~ ., data = d) Comparison Estimates are almost idential in this particular case. print( fit, pars = c(&#39;beta&#39;, &#39;phi&#39;), digits_summary = 3, probs = c(.025, .5, .975) ) Inference for Stan model: 71bce272e309aa5260f24d407b92d24c. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat beta[1] -0.988 0.001 0.030 -1.048 -0.987 -0.928 973 0.999 beta[2] 0.126 0.001 0.029 0.070 0.124 0.184 930 1.003 beta[3] -0.327 0.001 0.031 -0.387 -0.327 -0.262 930 1.003 phi 10.599 0.022 0.673 9.302 10.591 11.892 952 0.999 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:33:34 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). summary(brmod) Call: betareg(formula = y ~ ., data = d) Standardized weighted residuals 2: Min 1Q Median 3Q Max -4.1584 -0.5925 0.0781 0.6632 2.6250 Coefficients (mean model with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.98934 0.02982 -33.18 &lt; 2e-16 *** x_1 0.12662 0.02801 4.52 6.18e-06 *** x_2 -0.32758 0.03049 -10.74 &lt; 2e-16 *** Phi coefficients (precision model with identity link): Estimate Std. Error z value Pr(&gt;|z|) (phi) 10.6731 0.6545 16.31 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Type of estimator: ML (maximum likelihood) Log-likelihood: 339.1 on 4 Df Pseudo R-squared: 0.2057 Number of iterations: 12 (BFGS) + 2 (Fisher scoring) Visualization Posterior predictive check. library(bayesplot) pp_check( stan_data$y, rstan::extract(fit, par = &#39;y_rep&#39;)$y_rep[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstanBetaRegression.R "],["bayesian-mixed-model.html", "Bayesian Mixed Model Data Setup Model Code Estimation Comparison Visualize Source", " Bayesian Mixed Model Explore the classic sleepstudy example of lme4. Part of this code was based on that seen on this old Stan thread, but you can look at the underlying code for rstanarm or brms for a fully optimized approach compared to this conceptual one. Data Setup The data comes from the lme4 package. It deals with reaction time to some task vs. sleep deprivation over 10 days. library(tidyverse) library(lme4) data(sleepstudy) # ?sleepstudy dat = list( N = nrow(sleepstudy), I = n_distinct(sleepstudy$Subject), Subject = as.numeric(sleepstudy$Subject), Days = sleepstudy$Days, RT = sleepstudy$Reaction ) Model Code Create the Stan model code. data { // data setup int&lt;lower = 1&gt; N; // sample size int&lt;lower = 1&gt; I; // number of subjects vector&lt;lower = 0&gt;[N] RT; // Response: reaction time vector&lt;lower = 0&gt;[N] Days; // Days in study int&lt;lower = 1, upper = I&gt; Subject[N]; // Subject } transformed data { real IntBase; real RTsd; IntBase = mean(RT); // Intercept starting point RTsd = sd(RT); } parameters { real Intercept01; // fixed effects real beta01; vector&lt;lower = 0&gt;[2] sigma_u; // sd for ints and slopes real&lt;lower = 0&gt; sigma_y; // residual sd vector[2] gamma[I]; // individual effects cholesky_factor_corr[2] Omega_chol; // correlation matrix for random intercepts and slopes (chol decomp) } transformed parameters { vector[I] gammaIntercept; // individual effects (named) vector[I] gammaDays; real Intercept; real beta; Intercept = IntBase + Intercept01 * RTsd; beta = beta01 * 10; for (i in 1:I){ gammaIntercept[i] = gamma[i, 1]; gammaDays[i] = gamma[i, 2]; } } model { matrix[2,2] D; matrix[2,2] DC; vector[N] mu; // Linear predictor vector[2] gamma_mu; // vector of Intercept and beta D = diag_matrix(sigma_u); gamma_mu[1] = Intercept; gamma_mu[2] = beta; // priors Intercept01 ~ normal(0, 1); // example of weakly informative priors; beta01 ~ normal(0, 1); // remove to essentially duplicate lme4 via improper prior Omega_chol ~ lkj_corr_cholesky(2.0); sigma_u ~ cauchy(0, 2.5); // prior for RE scale sigma_y ~ cauchy(0, 2.5); // prior for residual scale DC = D * Omega_chol; for (i in 1:I) // loop for Subject random effects gamma[i] ~ multi_normal_cholesky(gamma_mu, DC); // likelihood for (n in 1:N) mu[n] = gammaIntercept[Subject[n]] + gammaDays[Subject[n]] * Days[n]; RT ~ normal(mu, sigma_y); } generated quantities { matrix[2, 2] Omega; // correlation of RE vector[N] y_hat; Omega = tcrossprod(Omega_chol); for (n in 1:N) y_hat[n] = gammaIntercept[Subject[n]] + gammaDays[Subject[n]] * Days[n]; } Estimation Run the model and examine results. The following assumes a character string or file (bayes_mixed) of the previous model code. library(rstan) fit = sampling( bayes_mixed, data = dat, thin = 4, verbose = FALSE ) Comparison Compare to lme4 result. print( fit, digits_summary = 3, pars = c(&#39;Intercept&#39;, &#39;beta&#39;, &#39;sigma_y&#39;, &#39;sigma_u&#39;, &#39;Omega[1,2]&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: 2506143d3919a87ea11841b6a26e9ada. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat Intercept 252.224 0.211 6.801 238.987 252.342 266.310 1042 0.999 beta 10.189 0.054 1.668 6.891 10.176 13.476 962 0.998 sigma_y 25.901 0.052 1.568 23.080 25.815 29.188 897 1.000 sigma_u[1] 23.900 0.230 6.125 13.200 23.469 37.385 707 1.003 sigma_u[2] 6.162 0.047 1.452 3.953 5.951 9.660 953 1.000 Omega[1,2] 0.102 0.010 0.266 -0.407 0.106 0.606 778 0.999 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:34:19 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). mod_lme = lmer(Reaction ~ Days + (Days | Subject), sleepstudy) mod_lme Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: Reaction ~ Days + (Days | Subject) Data: sleepstudy REML criterion at convergence: 1743.628 Random effects: Groups Name Std.Dev. Corr Subject (Intercept) 24.741 Days 5.922 0.07 Residual 25.592 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 cbind( coef(mod_lme)$Subject, matrix(get_posterior_mean(fit, par = c(&#39;gammaIntercept&#39;, &#39;gammaDays&#39;))[, &#39;mean-all chains&#39;], ncol = 2) ) (Intercept) Days 1 2 308 253.6637 19.6662617 255.2697 19.4436027 309 211.0064 1.8476053 212.5908 1.6576886 310 212.4447 5.0184295 215.1223 4.5968134 330 275.0957 5.6529356 273.1367 5.9194018 331 273.6654 7.3973743 272.1793 7.6053031 332 260.4447 10.1951090 260.3817 10.1720560 333 268.2456 10.2436499 267.4948 10.3554621 334 244.1725 11.5418676 245.0794 11.2856905 335 251.0714 -0.2848792 249.6814 -0.1358559 337 286.2956 19.0955511 284.6991 19.2866572 349 226.1949 11.6407181 229.1268 11.1449475 350 238.3351 17.0815038 240.3025 16.7056700 351 255.9830 7.4520239 255.2410 7.6661530 352 272.2688 14.0032871 271.0198 14.0739847 369 254.6806 11.3395008 255.0000 11.2681311 370 225.7921 15.2897709 228.4023 14.7781029 371 252.2122 9.4791297 252.3174 9.5119921 372 263.7197 11.7513080 263.0256 11.8830409 Visualize Visualize the posterior predictive distribution. # shinystan::launch_shinystan(fit) # diagnostic plots library(bayesplot) pp_check( dat$RT, rstan::extract(fit, par = &#39;y_hat&#39;)$y_hat[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstan_MixedModelSleepstudy_withREcorrelation.R "],["bayesian-mixed-mediation.html", "Bayesian Multilevel Mediation Data Setup Model Code Estimation Comparison Visualization Source", " Bayesian Multilevel Mediation The following demonstrates an indirect effect in a multilevel situation. It is based on Yuan &amp; MacKinnon 2009, which provides some Bugs code. In what follows we essentially have two models, one where the ‘mediator’ is the response; the other regards the primary response of interest (noted y). They will be referred to with Med or Main respectively. Data Setup The two main models are expressed conceptually as follows: \\[\\textrm{Mediator} \\sim \\alpha_{Med} + \\beta_{Med}\\cdot X\\] \\[y \\sim \\alpha_{Main} + \\beta_{1\\_{Main}}\\cdot X + \\beta_{2\\_{Main}}\\cdot \\textrm{Mediator}\\] However, there will be random effects for a grouping variable for each coefficient, i.e. random intercepts and slopes, for both the mediator model and the outcome model. Let’s create data to this effect. In the following we will ultimately have 1000 total observations, with 50 groups (20 observations each). library(tidyverse) set.seed(8675309) N = 1000 n_groups = 50 n_per_group = N/n_groups # random effects for mediator model # create cov matrix of RE etc. with no covariance between model random effects # covmat_RE = matrix(c(1,-.15,0,0,0, # -.15,.4,0,0,0, # 0,0,1,-.1,.15, # 0,0,-.1,.3,0, # 0,0,.15,0,.2), nrow=5, byrow = T) # or with slight cov added to indirect coefficient RE; both matrices are pos def covmat_RE = matrix(c( 1.00, -0.15, 0.00, 0.00, 0.00, -0.15, 0.64, 0.00, 0.00, -0.10, 0.00, 0.00, 1.00, -0.10, 0.15, 0.00, 0.00, -0.10, 0.49, 0.00, 0.00, -0.10, 0.15, 0.00, 0.25), nrow = 5, byrow = TRUE) # inspect covmat_RE [,1] [,2] [,3] [,4] [,5] [1,] 1.00 -0.15 0.00 0.00 0.00 [2,] -0.15 0.64 0.00 0.00 -0.10 [3,] 0.00 0.00 1.00 -0.10 0.15 [4,] 0.00 0.00 -0.10 0.49 0.00 [5,] 0.00 -0.10 0.15 0.00 0.25 # inspect as correlation cov2cor(covmat_RE) [,1] [,2] [,3] [,4] [,5] [1,] 1.0000 -0.1875 0.0000000 0.0000000 0.00 [2,] -0.1875 1.0000 0.0000000 0.0000000 -0.25 [3,] 0.0000 0.0000 1.0000000 -0.1428571 0.30 [4,] 0.0000 0.0000 -0.1428571 1.0000000 0.00 [5,] 0.0000 -0.2500 0.3000000 0.0000000 1.00 colnames(covmat_RE) = rownames(covmat_RE) = c(&#39;alpha_Med&#39;, &#39;beta_Med&#39;, &#39;alpha_Main&#39;, &#39;beta1_Main&#39;, &#39;beta2_Main&#39;) # simulate random effects re = MASS::mvrnorm( n_groups, mu = rep(0, 5), Sigma = covmat_RE, empirical = TRUE ) # random effects for mediator model ranef_alpha_Med = rep(re[, &#39;alpha_Med&#39;], e = n_per_group) ranef_beta_Med = rep(re[, &#39;beta_Med&#39;], e = n_per_group) # random effects for main model ranef_alpha_Main = rep(re[, &#39;alpha_Main&#39;], e = n_per_group) ranef_beta1_Main = rep(re[, &#39;beta1_Main&#39;], e = n_per_group) ranef_beta2_Main = rep(re[, &#39;beta2_Main&#39;], e = n_per_group) ## fixed effects alpha_Med = 2 beta_Med = .2 alpha_Main = 1 beta1_Main = .3 beta2_Main = -.2 # residual variance resid_Med = MASS::mvrnorm(N, 0, .75^2, empirical = TRUE) resid_Main = MASS::mvrnorm(N, 0, .50^2, empirical = TRUE) # Collect parameters for later comparison params = c( alpha_Med = alpha_Med, beta_Med = beta_Med, sigma_Med = sd(resid_Med), alpha_Main = alpha_Main, beta1_Main = beta1_Main, beta2_Main = beta2_Main, sigma_y = sd(resid_Main), alpha_Med_sd = sqrt(diag(covmat_RE)[1]), beta_Med_sd = sqrt(diag(covmat_RE)[2]), alpha_sd = sqrt(diag(covmat_RE)[3]), beta1_sd = sqrt(diag(covmat_RE)[4]), beta2_sd = sqrt(diag(covmat_RE)[5]) ) ranefs = cbind( gamma_alpha_Med = unique(ranef_alpha_Med), gamma_beta_Med = unique(ranef_beta_Med), gamma_alpha = unique(ranef_alpha_Main), gamma_beta1 = unique(ranef_beta1_Main), gamma_beta2 = unique(ranef_beta2_Main) ) Finally, we can create the data for analysis. X = rnorm(N, sd = 2) Med = (alpha_Med + ranef_alpha_Med) + (beta_Med + ranef_beta_Med) * X + resid_Med[, 1] y = (alpha_Main + ranef_alpha_Main) + (beta1_Main + ranef_beta1_Main) * X + (beta2_Main + ranef_beta2_Main) * Med + resid_Main[, 1] group = rep(1:n_groups, e = n_per_group) standat = list( X = X, Med = Med, y = y, Group = group, J = length(unique(group)), N = length(y) ) Model Code In the following, the cholesky decomposition of the RE covariance matrix is used for efficiency. As a rough guide, the default data with rN` observations took about a minute or so to run. data { int&lt;lower = 1&gt; N; // Sample size vector[N] X; // Explanatory variable vector[N] Med; // Mediator vector[N] y; // Response int&lt;lower = 1&gt; J; // Number of groups int&lt;lower = 1,upper = J&gt; Group[N]; // Groups } parameters{ real alpha_Med; // mediator model reg parameters and related real beta_Med; real&lt;lower = 0&gt; sigma_alpha_Med; real&lt;lower = 0&gt; sigma_beta_Med; real&lt;lower = 0&gt; sigmaMed; real alpha_Main; // main model reg parameters and related real beta1_Main; real beta2_Main; real&lt;lower = 0&gt; sigma_alpha; real&lt;lower = 0&gt; sigma_beta1; real&lt;lower = 0&gt; sigma_beta2; real&lt;lower = 0&gt; sigma_y; cholesky_factor_corr[5] Omega_chol; // chol decomp of corr matrix for random effects vector&lt;lower = 0&gt;[5] sigma_ranef; // sd for random effects matrix[J,5] gamma; // random effects } transformed parameters{ vector[J] gamma_alpha_Med; vector[J] gamma_beta_Med; vector[J] gamma_alpha; vector[J] gamma_beta1; vector[J] gamma_beta2; for (j in 1:J){ gamma_alpha_Med[j] = gamma[j,1]; gamma_beta_Med[j] = gamma[j,2]; gamma_alpha[j] = gamma[j,3]; gamma_beta1[j] = gamma[j,4]; gamma_beta2[j] = gamma[j,5]; } } model { vector[N] mu_y; // linear predictors for response and mediator vector[N] mu_Med; matrix[5,5] D; matrix[5,5] DC; // priors // mediator model // fixef // for scale params the cauchy is a little more informative here due // to the nature of the data sigma_alpha_Med ~ cauchy(0, 1); sigma_beta_Med ~ cauchy(0, 1); alpha_Med ~ normal(0, sigma_alpha_Med); beta_Med ~ normal(0, sigma_beta_Med); // residual scale sigmaMed ~ cauchy(0, 1); // main model // fixef sigma_alpha ~ cauchy(0, 1); sigma_beta1 ~ cauchy(0, 1); sigma_beta2 ~ cauchy(0, 1); alpha_Main ~ normal(0, sigma_alpha); beta1_Main ~ normal(0, sigma_beta1); beta2_Main ~ normal(0, sigma_beta2); // residual scale sigma_y ~ cauchy(0, 1); // ranef sampling via cholesky decomposition sigma_ranef ~ cauchy(0, 1); Omega_chol ~ lkj_corr_cholesky(2.0); D = diag_matrix(sigma_ranef); DC = D * Omega_chol; for (j in 1:J) // loop for Group random effects gamma[j] ~ multi_normal_cholesky(rep_vector(0, 5), DC); // Linear predictors for (n in 1:N){ mu_Med[n] = alpha_Med + gamma_alpha_Med[Group[n]] + (beta_Med + gamma_beta_Med[Group[n]]) * X[n]; mu_y[n] = alpha_Main + gamma_alpha[Group[n]] + (beta1_Main + gamma_beta1[Group[n]]) * X[n] + (beta2_Main + gamma_beta2[Group[n]]) * Med[n] ; } // sampling for primary models Med ~ normal(mu_Med, sigmaMed); y ~ normal(mu_y, sigma_y); } generated quantities{ real naive_ind_effect; real avg_ind_effect; real total_effect; matrix[5,5] cov_RE; vector[N] y_hat; cov_RE = diag_matrix(sigma_ranef) * tcrossprod(Omega_chol) * diag_matrix(sigma_ranef); naive_ind_effect = beta_Med*beta2_Main; avg_ind_effect = beta_Med*beta2_Main + cov_RE[2,5]; // add cov of random slopes for mediator effects total_effect = avg_ind_effect + beta1_Main; for (n in 1:N){ y_hat[n] = alpha_Main + gamma_alpha[Group[n]] + (beta1_Main + gamma_beta1[Group[n]]) * X[n] + (beta2_Main + gamma_beta2[Group[n]]) * Med[n] ; } } Estimation Run the model and examine results. The following assumes a character string or file (bayes_med_model) of the previous model code. library(rstan) fit = sampling( bayes_med_model, data = standat, iter = 3000, warmup = 2000, thin = 4, cores = 4, control = list(adapt_delta = .99, max_treedepth = 15) ) Comparison Main parameters include fixed and random effect standard deviation, plus those related to indirect effect. mainpars = c( &#39;alpha_Med&#39;, &#39;beta_Med&#39;, &#39;sigmaMed&#39;, &#39;alpha_Main&#39;, &#39;beta1_Main&#39;, &#39;beta2_Main&#39;, &#39;sigma_y&#39;, &#39;sigma_ranef&#39;, &#39;naive_ind_effect&#39;, &#39;avg_ind_effect&#39;, &#39;total_effect&#39; ) print( fit, digits = 3, probs = c(.025, .5, 0.975), pars = mainpars ) Inference for Stan model: e771ba356dcfd779b3cf0ac752042346. 4 chains, each with iter=3000; warmup=2000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat alpha_Med 1.996 0.010 0.150 1.693 1.999 2.280 244 1.015 beta_Med 0.142 0.008 0.113 -0.047 0.139 0.369 220 1.021 sigmaMed 0.744 0.001 0.018 0.711 0.744 0.780 940 1.001 alpha_Main 0.964 0.008 0.158 0.646 0.966 1.281 425 1.008 beta1_Main 0.273 0.006 0.108 0.050 0.275 0.483 284 1.002 beta2_Main -0.153 0.004 0.081 -0.307 -0.151 -0.002 410 1.004 sigma_y 0.495 0.000 0.012 0.472 0.494 0.520 946 1.001 sigma_ranef[1] 1.031 0.004 0.108 0.845 1.026 1.269 841 1.005 sigma_ranef[2] 0.839 0.003 0.089 0.685 0.831 1.031 1058 0.999 sigma_ranef[3] 1.046 0.004 0.123 0.839 1.031 1.313 997 1.002 sigma_ranef[4] 0.776 0.003 0.085 0.625 0.773 0.969 1033 0.998 sigma_ranef[5] 0.514 0.002 0.061 0.401 0.511 0.647 952 1.000 naive_ind_effect -0.024 0.002 0.025 -0.082 -0.018 0.006 231 1.025 avg_ind_effect -0.115 0.002 0.067 -0.259 -0.113 0.005 863 1.001 total_effect 0.158 0.007 0.125 -0.087 0.159 0.391 356 1.000 Samples were drawn using NUTS(diag_e) at Sat Dec 12 12:18:03 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can use a piecemeal mixed model via lme4 for initial comparison. However, it can’t directly estimate mediated effect, and it won’t pick up on correlation of random effects between models. library(lme4) mod_Med = lmer(Med ~ X + (1 + X | group)) summary(mod_Med) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: Med ~ X + (1 + X | group) REML criterion at convergence: 2647.9 Scaled residuals: Min 1Q Median 3Q Max -3.14663 -0.67172 0.03569 0.64922 2.87581 Random effects: Groups Name Variance Std.Dev. Corr group (Intercept) 0.9739 0.9869 X 0.6414 0.8009 -0.22 Residual 0.5522 0.7431 Number of obs: 1000, groups: group, 50 Fixed effects: Estimate Std. Error t value (Intercept) 2.0046 0.1416 14.155 X 0.1901 0.1139 1.668 Correlation of Fixed Effects: (Intr) X -0.217 mod_Main = lmer(y ~ X + Med + (1 + X + Med | group)) summary(mod_Main) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: y ~ X + Med + (1 + X + Med | group) REML criterion at convergence: 2030.7 Scaled residuals: Min 1Q Median 3Q Max -3.6523 -0.6368 -0.0054 0.6344 2.8578 Random effects: Groups Name Variance Std.Dev. Corr group (Intercept) 0.9816 0.9908 X 0.5352 0.7316 -0.11 Med 0.2367 0.4865 0.35 -0.03 Residual 0.2445 0.4945 Number of obs: 1000, groups: group, 50 Fixed effects: Estimate Std. Error t value (Intercept) 0.98977 0.14891 6.647 X 0.28636 0.10507 2.725 Med -0.17958 0.07226 -2.485 Correlation of Fixed Effects: (Intr) X X -0.097 Med 0.225 -0.041 # should equal the naive estimate in the following code lme_indirect_effect = fixef(mod_Med)[&#39;X&#39;] * fixef(mod_Main)[&#39;Med&#39;] Using the mediation package will provide a better estimate, and can handle this simple mixed model setting. # library(mediation) mediation_mixed = mediation::mediate( model.m = mod_Med, model.y = mod_Main, treat = &#39;X&#39;, mediator = &#39;Med&#39; ) summary(mediation_mixed) Causal Mediation Analysis Quasi-Bayesian Confidence Intervals Mediator Groups: group Outcome Groups: group Output Based on Overall Averages Across Groups Estimate 95% CI Lower 95% CI Upper p-value ACME -0.1144 -0.1805 -0.06 &lt;2e-16 *** ADE 0.2877 0.0777 0.50 0.01 ** Total Effect 0.1733 -0.0385 0.40 0.11 Prop. Mediated -0.5990 -6.6281 4.56 0.11 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Sample Size Used: 1000 Simulations: 1000 Extract parameters for comparison. pars_primary = get_posterior_mean(fit, pars = mainpars)[, 5] pars_re_cov = get_posterior_mean(fit, pars = &#39;Omega_chol&#39;)[, 5] # or take &#39;cov_RE&#39; from monte carlo sim pars_re = get_posterior_mean(fit, pars = c(&#39;sigma_ranef&#39;))[, 5] Fixed effects and random effect variances. param true bayes lme4 alpha_Med 2.00 1.996 2.005 beta_Med 0.20 0.142 0.190 sigma_Med 0.75 0.744 0.743 alpha_Main 1.00 0.964 0.990 beta1_Main 0.30 0.273 0.286 beta2_Main -0.20 -0.153 -0.180 sigma_y 0.50 0.495 0.494 alpha_Med_sd.alpha_Med 1.00 1.031 0.987 beta_Med_sd.beta_Med 0.80 0.839 0.801 alpha_sd.alpha_Main 1.00 1.046 0.991 beta1_sd.beta1_Main 0.70 0.776 0.732 beta2_sd.beta2_Main 0.50 0.514 0.486 Compare the covariances of the random effects. The first shows the full covariance matrix for mediator and outcome, then broken out separately. $true alpha_Med beta_Med alpha_Main beta1_Main beta2_Main alpha_Med 1.00 -0.15 0.00 0.00 0.00 beta_Med -0.15 0.64 0.00 0.00 -0.10 alpha_Main 0.00 0.00 1.00 -0.10 0.15 beta1_Main 0.00 0.00 -0.10 0.49 0.00 beta2_Main 0.00 -0.10 0.15 0.00 0.25 $estimates [,1] [,2] [,3] [,4] [,5] [1,] 1.06 -0.16 0.02 0.02 -0.04 [2,] -0.16 0.69 -0.01 -0.03 -0.09 [3,] 0.02 -0.01 1.05 -0.08 0.15 [4,] 0.02 -0.03 -0.08 0.57 -0.01 [5,] -0.04 -0.09 0.15 -0.01 0.24 $vcov_Med alpha_Med beta_Med alpha_Med 1.00 -0.15 beta_Med -0.15 0.64 $vcov_Med_bayes [,1] [,2] [1,] 1.06 -0.16 [2,] -0.16 0.69 $vcov_Med_lme4 (Intercept) X (Intercept) 0.97 -0.18 X -0.18 0.64 $vcov_Main alpha_Main beta1_Main beta2_Main alpha_Main 1.00 -0.10 0.15 beta1_Main -0.10 0.49 0.00 beta2_Main 0.15 0.00 0.25 $vcov_Main_bayes [,1] [,2] [,3] [1,] 1.05 -0.08 0.15 [2,] -0.08 0.57 -0.01 [3,] 0.15 -0.01 0.24 $vcov_Main_lme4 (Intercept) X Med (Intercept) 0.98 -0.08 0.17 X -0.08 0.54 -0.01 Med 0.17 -0.01 0.24 Compare indirect effects true est_bayes naive_bayes naive_lmer mediation_pack -0.14 -0.115 -0.024 -0.034 -0.114 Note that you can use brms to estimate this model as follows. The i allows the random effects to correlate across Mediator and outcome models. We have to convert the correlation estimate back to the covariance estimate to get the indirect value to compare to our base Stan result. library(brms) f = bf(Med ~ X + (1 + X |i| group)) + bf(y ~ X + Med + (1 + X + Med |i| group)) + set_rescor(FALSE) fit_brm = brm( f, data = data.frame(X, Med, y, group), cores = 4, thin = 4, seed = 1234, control = list(adapt_delta = .99, max_treedepth = 15) ) summary(fit_brm) Family: MV(gaussian, gaussian) Links: mu = identity; sigma = identity mu = identity; sigma = identity Formula: Med ~ X + (1 + X | i | group) y ~ X + Med + (1 + X + Med | i | group) Data: data.frame(X, Med, y, group) (Number of observations: 1000) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 4; total post-warmup samples = 1000 Group-Level Effects: ~group (Number of levels: 50) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Med_Intercept) 1.05 0.11 0.86 1.30 1.00 859 781 sd(Med_X) 0.85 0.09 0.68 1.05 1.00 744 796 sd(y_Intercept) 1.06 0.12 0.84 1.33 1.00 938 988 sd(y_X) 0.78 0.09 0.64 0.98 1.00 849 1029 sd(y_Med) 0.51 0.06 0.41 0.64 1.00 816 876 cor(Med_Intercept,Med_X) -0.19 0.14 -0.44 0.09 1.00 847 992 cor(Med_Intercept,y_Intercept) 0.02 0.14 -0.26 0.28 1.00 925 993 cor(Med_X,y_Intercept) -0.02 0.15 -0.31 0.26 1.00 879 994 cor(Med_Intercept,y_X) 0.03 0.14 -0.23 0.31 1.00 945 866 cor(Med_X,y_X) -0.05 0.14 -0.33 0.23 1.00 918 813 cor(y_Intercept,y_X) -0.11 0.15 -0.39 0.18 1.00 886 946 cor(Med_Intercept,y_Med) -0.07 0.15 -0.36 0.21 1.00 849 990 cor(Med_X,y_Med) -0.22 0.14 -0.48 0.06 1.00 871 882 cor(y_Intercept,y_Med) 0.31 0.15 -0.00 0.58 1.00 762 882 cor(y_X,y_Med) -0.01 0.15 -0.30 0.28 1.00 903 987 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Med_Intercept 2.01 0.15 1.70 2.29 1.00 684 850 y_Intercept 0.99 0.16 0.67 1.32 1.00 856 961 Med_X 0.19 0.12 -0.04 0.42 1.00 817 794 y_X 0.29 0.11 0.07 0.51 1.01 841 914 y_Med -0.18 0.08 -0.33 -0.03 1.00 919 884 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma_Med 0.74 0.02 0.71 0.78 1.00 1007 732 sigma_y 0.50 0.01 0.47 0.52 1.00 936 782 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). hypothesis( fit_brm, &#39;b_y_Med*b_Med_X + cor_group__Med_X__y_Med*sd_group__Med_X*sd_group__y_Med = 0&#39;, class = NULL, seed = 1234 ) Hypothesis Tests for class : Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star 1 (b_y_Med*b_Med_X+... = 0 -0.13 0.08 -0.3 0 NA NA --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. Visualization library(bayesplot) pp_check( standat$y, rstan::extract(fit, par = &#39;y_hat&#39;)$y_hat[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstan_multilevelMediation.R "],["bayesian-irt.html", "Bayesian IRT One Parameter IRT Two Parameter IRT Three Parameter IRT Four Parameter IRT Source", " Bayesian IRT The following shows some code demonstration for one through four parameter IRT models, though will only extensively explore the first two. You can learn more about IRT models in general in my structural equation modeling document. One Parameter IRT Data Setup This data set has the responses of 316 participants on 24 items of a questionnaire on verbal aggression. Other covariates are also provided. For simplicity I will focus on the four ‘DoShout’ items. library(tidyverse) data(&quot;VerbAgg&quot;, package = &quot;lme4&quot;) glimpse(VerbAgg) Rows: 7,584 Columns: 9 $ Anger &lt;int&gt; 20, 11, 17, 21, 17, 21, 39, 21, 24, 16, 15, 18, 36, 22, 16, 18, 23, 16, 21, 25, 22, 15, 26, 13, 33, 17, 17, 22, 21, 17, 19, 18, 33, 19, 25, 17, 12, 14, 25, 22, 20, 25, 12, 16, 23, 19, 22, 15, 25, 35, 24… $ Gender &lt;fct&gt; M, M, F, F, F, F, F, F, F, F, F, F, M, M, F, F, F, F, F, F, F, F, F, F, F, F, F, F, M, F, F, M, M, F, F, F, F, F, M, M, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, M, F, F, F, F, F, F, F, F, F… $ item &lt;fct&gt; S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantC… $ resp &lt;ord&gt; no, no, perhaps, perhaps, perhaps, yes, yes, no, no, yes, perhaps, yes, yes, yes, perhaps, perhaps, perhaps, perhaps, no, perhaps, perhaps, yes, yes, perhaps, no, no, yes, yes, perhaps, yes, yes, perhap… $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,… $ btype &lt;fct&gt; curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse,… $ situ &lt;fct&gt; other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other,… $ mode &lt;fct&gt; want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want… $ r2 &lt;fct&gt; N, N, Y, Y, Y, Y, Y, N, N, Y, Y, Y, Y, Y, Y, Y, Y, Y, N, Y, Y, Y, Y, Y, N, N, Y, Y, Y, Y, Y, Y, N, Y, Y, Y, N, N, N, N, Y, N, Y, N, N, Y, Y, N, N, Y, N, N, Y, Y, N, Y, Y, Y, N, N, Y, Y, Y, Y, Y, N, Y, Y… verbagg_items = VerbAgg %&gt;% filter(btype == &#39;shout&#39;, situ == &#39;self&#39;) %&gt;% select(id, item, r2) head(verbagg_items) id item r2 1 1 S3WantShout Y 2 2 S3WantShout N 3 3 S3WantShout N 4 4 S3WantShout N 5 5 S3WantShout N 6 6 S3WantShout Y verbagg_items_wide = verbagg_items %&gt;% pivot_wider(id_cols = id, names_from = item, names_prefix = &#39;item_&#39;, values_from = r2) head(verbagg_items_wide) # A tibble: 6 x 5 id item_S3WantShout item_S4WantShout item_S3DoShout item_S4DoShout &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 1 Y N N Y 2 2 N N N N 3 3 N N N N 4 4 N N N N 5 5 N N N N 6 6 Y N N N While we often think of the data in ‘wide form’, with one row per person and multiple columns respective to each item, and the subsequent Stan code will use that, it is generally both tidier and more straightforward for modeling with the long format, where one can use standard mixed model approaches. r2 is the target variable of interest in that case. In the long format, the model for a single person is as follows, where \\(Z\\) is the latent person (\\(p\\))score, and \\(i\\) is the \\(i^{th}\\) item. \\[\\textrm{logit}(\\pi) = \\textrm{disc} (Z_p - \\beta_i)\\] Another formulation is the following, and corresponds to what brms will use. \\[\\textrm{logit}(\\pi) = \\beta_i + \\textrm{disc}\\cdot Z_p\\] Model Code data { int N; // Number of people int J; // Number of items int Y[N,J]; // Binary Target } transformed data{ } parameters { vector[J] difficulty; // Item difficulty real&lt;lower = 0&gt; discrim; // Item discrimination (constant) vector[N] Z; // Latent person ability } model { matrix[N, J] lmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); for (j in 1:J){ lmat[,j] = discrim * (Z - difficulty[j]); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli_logit(lmat[,j]); } Estimation First we create a Stan-friendly data list and then estimate the model. The following assumes a character string or file (bayes_irt1_model) of the previous model code. verbagg_items_wide_mat = apply( as.matrix(verbagg_items_wide[, -1]) == &#39;Y&#39;, 2, as.integer ) stan_data = list( N = nrow(verbagg_items_wide_mat), J = ncol(verbagg_items_wide_mat), Y = verbagg_items_wide_mat ) library(rstan) fit_1pm = sampling( bayes_irt1_model, data = stan_data, thin = 4 ) Comparison Now we compare to brms. I use the author’s article as a guide for this model, and note again that it is following the second parameterization depicted above. library(brms) # half normal for variance parameter, full for coefficients prior_1pm &lt;- prior(&quot;normal(0, 3)&quot;, class = &quot;sd&quot;, group = &quot;id&quot;) + prior(&quot;normal(0, 3)&quot;, class = &quot;b&quot;) brms_1pm = brm( r2 ~ 0 + item + (1 | id), data = verbagg_items, family = bernoulli, prior = prior_1pm, thin = 4, cores = 4 ) If you want to compare to standard IRT in either parameterization, you can use the ltm package. library(ltm) irt_rasch_par1 = rasch(verbagg_items_wide_mat, IRT.param = FALSE) irt_rasch_par2 = rasch(verbagg_items_wide_mat, IRT.param = TRUE) print( fit_1pm, digits = 3, par = c(&#39;discrim&#39;, &#39;difficulty&#39;), probs = c(.025, .5, 0.975) ) Inference for Stan model: 0715d0cee215483765f01043abb55ea9. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat discrim 1.859 0.007 0.197 1.502 1.852 2.250 812 1.001 difficulty[1] 0.968 0.004 0.121 0.750 0.965 1.215 913 1.001 difficulty[2] 0.665 0.004 0.106 0.460 0.663 0.874 788 1.000 difficulty[3] 1.854 0.006 0.176 1.534 1.846 2.237 844 1.005 difficulty[4] 1.255 0.005 0.137 1.003 1.250 1.524 804 0.998 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:39:05 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). summary(brms_1pm) Family: bernoulli Links: mu = logit Formula: r2 ~ 0 + item + (1 | id) Data: verbagg_items (Number of observations: 1264) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 4; total post-warmup samples = 1000 Group-Level Effects: ~id (Number of levels: 316) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 1.90 0.21 1.53 2.32 1.00 832 890 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS itemS3WantShout -1.79 0.22 -2.25 -1.37 1.00 947 883 itemS4WantShout -1.23 0.21 -1.66 -0.83 1.01 860 809 itemS3DoShout -3.43 0.31 -4.08 -2.86 1.00 907 914 itemS4DoShout -2.35 0.25 -2.86 -1.90 1.00 812 852 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). brms_diff = fixef(brms_1pm)[,&#39;Estimate&#39;] brms_discrim = VarCorr(brms_1pm)$id$sd[1] fit_params = summary(fit_1pm, digits = 3, par = c(&#39;discrim&#39;, &#39;difficulty&#39;))$summary[,&#39;mean&#39;] After extracting, we can show either parameterization for either model. For example, brms item difficulties = our model -discrim*difficulties. # A tibble: 5 x 5 parma model brms model_par2 brms_par1 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 discrim 1.86 1.90 1.86 1.90 2 difficulty[1] 0.968 -1.79 -1.80 0.942 3 difficulty[2] 0.665 -1.23 -1.24 0.647 4 difficulty[3] 1.85 -3.43 -3.45 1.81 5 difficulty[4] 1.26 -2.35 -2.33 1.23 Two Parameter IRT Now we can try a two parameter model. Data setup is the same as before. Model Code data { int N; int J; int Y[N, J]; } parameters { vector[J] difficulty; vector&lt;lower = 0&gt;[J] discrim; // Now per-item discrimination vector[N] Z; } model { matrix[N, J] lmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); for (j in 1:J){ lmat[,j] = discrim[j] * (Z - difficulty[j]); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli_logit(lmat[,j]); } Estimation First, our custom Stan model. The following assumes a character string or file (bayes_irt2_model) of the previous model code. library(rstan) fit_2pm = sampling( bayes_irt2_model, data = stan_data, thin = 4, iter = 4000, warmup = 3000, cores = 4, control = list(adapt_delta = .99) ) Comparison Now we compare to brms. I use the author’s article as a guide for this model, and note that it is following the second parameterization. Took a little over 30 seconds on my machine, though of course you may experience differently. library(brms) # half normal for variance parameter, full for coefficients prior_2pm &lt;- prior(&quot;normal(0, 5)&quot;, class = &quot;b&quot;, nlpar = &quot;Z&quot;) + prior(&quot;normal(0, 5)&quot;, class = &quot;b&quot;, nlpar = &quot;logdiscr&quot;) + prior(&quot;constant(1)&quot;, class = &quot;sd&quot;, group = &quot;id&quot;, nlpar = &quot;Z&quot;) + prior(&quot;normal(0, 3)&quot;, class = &quot;sd&quot;, group = &quot;item&quot;, nlpar = &quot;Z&quot;) + prior(&quot;normal(0, 3)&quot;, class = &quot;sd&quot;, group = &quot;item&quot;, nlpar = &quot;logdiscr&quot;) formula_2pm = bf( r2 ~ exp(logdiscr) * Z, Z ~ 1 + (1 |i| item) + (1 | id), logdiscr ~ 1 + (1 |i| item), nl = TRUE ) brms_2pm = brm( formula_2pm, data = verbagg_items, family = bernoulli, prior = prior_2pm, thin = 4, iter = 4000, warmup = 3000, cores = 4, control = list(adapt_delta = .99, max_treedepth = 15) ) print( fit_2pm, digits = 3, par = c(&#39;discrim&#39;, &#39;difficulty&#39;), probs = c(.025, .5, 0.975) ) Inference for Stan model: 503e7ed2dc96cbd6b316fe17abc2a86f. 4 chains, each with iter=4000; warmup=3000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat discrim[1] 1.033 0.008 0.249 0.599 1.003 1.567 891 1.003 discrim[2] 3.026 0.098 1.630 1.603 2.632 7.436 279 1.000 discrim[3] 1.462 0.014 0.389 0.803 1.425 2.321 784 0.998 discrim[4] 4.461 0.158 3.688 1.882 3.609 12.287 544 1.002 difficulty[1] 1.420 0.011 0.314 0.946 1.376 2.207 866 0.998 difficulty[2] 0.595 0.004 0.106 0.401 0.589 0.815 738 1.000 difficulty[3] 2.196 0.017 0.464 1.595 2.115 3.144 703 0.998 difficulty[4] 1.034 0.006 0.129 0.820 1.021 1.311 513 1.003 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:40:25 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). summary(brms_2pm) Family: bernoulli Links: mu = logit Formula: r2 ~ exp(logdiscr) * Z Z ~ 1 + (1 | i | item) + (1 | id) logdiscr ~ 1 + (1 | i | item) Data: verbagg_items (Number of observations: 1264) Samples: 4 chains, each with iter = 4000; warmup = 3000; thin = 4; total post-warmup samples = 1000 Group-Level Effects: ~id (Number of levels: 316) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Z_Intercept) 1.00 0.00 1.00 1.00 1.00 1000 1000 ~item (Number of levels: 4) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Z_Intercept) 1.09 0.73 0.33 3.07 1.00 706 816 sd(logdiscr_Intercept) 0.94 0.79 0.05 2.87 1.00 515 648 cor(Z_Intercept,logdiscr_Intercept) 0.18 0.50 -0.87 0.91 1.00 781 758 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Z_Intercept -1.22 0.67 -2.49 0.11 1.00 733 710 logdiscr_Intercept 0.66 0.56 -0.36 2.04 1.00 896 654 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). brms_diff = coef(brms_2pm)$item[,,&#39;Z_Intercept&#39;][,&#39;Estimate&#39;] brms_discrim = exp(coef(brms_2pm)$item[,,&#39;logdiscr_Intercept&#39;][,&#39;Estimate&#39;]) fit_diff = summary(fit_2pm, digits = 3, par = &#39;difficulty&#39;)$summary[,&#39;mean&#39;] fit_discrim = summary(fit_2pm, digits = 3, par = &#39;discrim&#39;)$summary[,&#39;mean&#39;] # A tibble: 8 x 3 parma model brms &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 discrim[1] 1.03 1.15 2 discrim[2] 3.03 2.39 3 discrim[3] 1.46 1.62 4 discrim[4] 4.46 3.24 5 difficulty[1] 1.42 -1.30 6 difficulty[2] 0.595 -0.622 7 difficulty[3] 2.20 -1.98 8 difficulty[4] 1.03 -1.09 Here is the non-Bayesian demo if interested. library(ltm) irt_2pm_par1 = ltm(verbagg_items_wide_mat ~ z1, IRT.param = FALSE) irt_2pm_par2 = ltm(verbagg_items_wide_mat ~ z1, IRT.param = TRUE) coef(irt_2pm_par1) coef(irt_2pm_par2) Three Parameter IRT For the three parameter model I only show the Stan code. This model adds a per-item guessing parameter, which serves as a lower bound, to the two parameter model. data { int N; int J; int Y[N,J]; } parameters { vector[J] difficulty; vector&lt;lower = 0&gt;[J] discrim; vector&lt;lower = 0, upper = .25&gt;[J] guess; vector[N] Z; } model { matrix[N, J] pmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); guess ~ beta(1, 19); for (j in 1:J){ pmat[,j] = guess[j] + (1 - guess[j]) * inv_logit(discrim[j] * (Z - difficulty[j])); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli(pmat[,j]); } Four Parameter IRT For the four parameter model I only show the Stan code. This model adds a per-item ceiling parameter, which serves as an upper bound, to the three parameter model. data { int N; int J; int Y[N,J]; } parameters { vector[J] difficulty; vector&lt;lower = 0&gt;[J] discrim; vector&lt;lower = 0, upper = .25&gt;[J] guess; vector&lt;lower = .95, upper = 1&gt;[J] ceiling; vector[N] Z; } model { matrix[N, J] pmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); guess ~ beta(1, 19); ceiling ~ beta(49, 1); for (j in 1:J){ pmat[,j] = guess[j] + (ceiling[j] - guess[j]) * inv_logit(discrim[j] * (Z - difficulty[j])); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli(pmat[,j]); } Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/Bayesian/StanBugsJags/IRT_models "],["bayesian-cfa.html", "Bayesian CFA Data Setup Model Code Estimation Comparison Source", " Bayesian CFA Data Setup For an empirical data set we can use the big five data from the psych package. For simplicity, I will only examine three of the five factors, and only the first 3 items of each. I have a version that has already reverse-scored the items that need be, but this is not necessary. We we will restrict ourselves to complete data and only a sample of 280 observations (~10%). library(tidyverse) data(&#39;big_five&#39;, package = &#39;noiris&#39;) # data(&#39;bfi&#39;, package = &#39;psych&#39;) big_five_no_miss = big_five %&gt;% select(matches(&#39;(^E|^O|^N)[1-3]&#39;)) %&gt;% drop_na() %&gt;% slice_sample(n = 280) Model Code The model code is quite verbose, and definitely not efficient, but hopefully clarifies this ‘latent linear model’ underlying the observations. data { int&lt;lower = 1&gt; N; // sample size int&lt;lower = 1&gt; P; // number of variables int&lt;lower = 1&gt; K; // number of factors matrix[N,P] X; // data matrix of order [N,P] } transformed data { int&lt;lower = 1&gt; L; L = P - K; // Number of free loadings } parameters { vector[P] b; // intercepts vector[L] lambda01; // initial factor loadings matrix[N, K] FS; // factor scores, matrix of order [N,K] corr_matrix[K] phi; // factor correlations vector&lt;lower = 0, upper = 2&gt;[K] sd_lv; // std dev of the latent factors vector&lt;lower = 0, upper = 2&gt;[P] sd_x; // std dev of the disturbances vector&lt;lower = 0, upper = 5&gt;[L] sd_lambda; // hyper parameter for loading std dev } transformed parameters { vector[L] lambda; // factor loadings lambda = lambda01 .* sd_lambda; // lambda as normal(0, sd_lambda) } model { matrix[N,P] mu; matrix[K,K] Ld; vector[K] muFactors; muFactors = rep_vector(0, K); // Factor means, set to zero Ld = diag_matrix(sd_lv) * cholesky_decompose(phi); for(n in 1:N) { mu[n,1] = b[1] + FS[n,1]; // Extraversion mu[n,2] = b[2] + FS[n,1]*lambda[1]; mu[n,3] = b[3] + FS[n,1]*lambda[2]; mu[n,4] = b[4] + FS[n,2]; // Neuroticism mu[n,5] = b[5] + FS[n,2]*lambda[3]; mu[n,6] = b[6] + FS[n,2]*lambda[4]; mu[n,7] = b[7] + FS[n,3]; // Openness mu[n,8] = b[8] + FS[n,3]*lambda[5]; mu[n,9] = b[9] + FS[n,3]*lambda[6]; } // priors phi ~ lkj_corr(2.0); sd_x ~ cauchy(0, 2.5); sd_lambda ~ cauchy(0, 2.5); sd_lv ~ cauchy(0, 2.5); b ~ normal(0, 10); lambda01 ~ normal(0, 1); // likelihood for(i in 1:N){ FS[i] ~ multi_normal_cholesky(muFactors, Ld); X[i] ~ normal(mu[i], sd_x); } } Estimation Note that this will likely take a while with the full data set, but you can bump the iterations down or decrease the sample size and get roughly the same estimates. With these defaults you might get some divergent warnings or other issues to possibly deal with. stan_data = list( N = nrow(big_five_no_miss), P = ncol(big_five_no_miss), K = 3, X = big_five_no_miss ) library(rstan) fit_cfa = sampling( bayes_cfa, data = stan_data, thin = 4, cores = 4, control = list(adapt_delta = .95, max_treedepth = 15) ) Comparison Here are the raw factor loadings and factor correlations. print( fit_cfa, digits = 3, par = c(&#39;lambda&#39;, &#39;phi&#39;), probs = c(.025, .5, 0.975) ) Inference for Stan model: 6522f4cb208793aee047a3160d67d5b3. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat lambda[1] 1.452 0.022 0.270 1.020 1.422 2.088 147 1.044 lambda[2] 1.055 0.019 0.236 0.666 1.027 1.607 162 1.008 lambda[3] 0.930 0.005 0.074 0.798 0.928 1.093 240 1.000 lambda[4] 0.696 0.003 0.063 0.580 0.695 0.822 500 0.998 lambda[5] 0.783 0.011 0.195 0.425 0.774 1.207 340 1.004 lambda[6] 1.288 0.028 0.302 0.811 1.251 2.015 114 1.045 phi[1,1] 1.000 NaN 0.000 1.000 1.000 1.000 NaN NaN phi[1,2] -0.238 0.004 0.082 -0.382 -0.241 -0.068 502 1.007 phi[1,3] 0.564 0.007 0.101 0.355 0.568 0.745 236 1.005 phi[2,1] -0.238 0.004 0.082 -0.382 -0.241 -0.068 502 1.007 phi[2,2] 1.000 0.000 0.000 1.000 1.000 1.000 835 0.996 phi[2,3] -0.163 0.003 0.085 -0.327 -0.163 0.003 696 0.999 phi[3,1] 0.564 0.007 0.101 0.355 0.568 0.745 236 1.005 phi[3,2] -0.163 0.003 0.085 -0.327 -0.163 0.003 696 0.999 phi[3,3] 1.000 0.000 0.000 1.000 1.000 1.000 937 0.996 Samples were drawn using NUTS(diag_e) at Wed Nov 25 17:59:17 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can compare our results with those of the lavaan package, which uses standard maximum likelihood via the cfa function default settings. library(lavaan) mod = &quot; E =~ E1 + E2 + E3 N =~ N1 + N2 + N3 O =~ O1 + O2 + O3 &quot; fit_lav = cfa(mod, data = big_five_no_miss) # summary(fit_lav) The following shows how to extract the parameter estimates and convert them to standardized form, followed by how to get the parameter estimates from the lavaan output. # loadings lambda = get_posterior_mean(fit_cfa, par = &#39;lambda&#39;)[,&#39;mean-all chains&#39;] lambda = c(1, lambda[1:2], 1, lambda[3:4], 1, lambda[5:6]) # standard deviations of factors and observed sd_F = rep(get_posterior_mean(fit_cfa, par = &#39;sd_lv&#39;)[,&#39;mean-all chains&#39;], e = 3) x_sd = apply(stan_data$X, 2, sd) # standardize lambda_std_F = sd_F*lambda lambda_std_all = sd_F/x_sd*lambda # get factor correlations fit_cors = matrix(get_posterior_mean(fit_cfa, par = &#39;phi&#39;)[, 5], 3, 3) lav_par = parameterEstimates(fit_lav, standardized = TRUE) First we compare the loadings, both raw and standardized (either standardize the latent variable only or the latent and observed variables). lhs op rhs est std.lv std.all std.nox lambda_est lambda_std_lv lambda_std_all E =~ E1 1.000 0.822 0.505 0.505 1.000 0.808 0.495 E =~ E2 1.375 1.130 0.706 0.706 1.452 1.172 0.732 E =~ E3 1.042 0.857 0.599 0.599 1.055 0.852 0.595 N =~ N1 1.000 1.441 0.897 0.897 1.000 1.450 0.900 N =~ N2 0.932 1.342 0.852 0.852 0.930 1.348 0.854 N =~ N3 0.705 1.015 0.642 0.642 0.696 1.009 0.637 O =~ O1 1.000 0.728 0.617 0.617 1.000 0.714 0.605 O =~ O2 0.775 0.564 0.362 0.362 0.783 0.559 0.358 O =~ O3 1.228 0.893 0.675 0.675 1.288 0.920 0.694 fit_cors E N O E 1.0000000 -0.2377586 0.5638754 N -0.2377586 1.0000000 -0.1625852 O 0.5638754 -0.1625852 1.0000000 lav_cors E N O E 1.0000000 -0.2420697 0.6042405 N -0.2420697 1.0000000 -0.1749015 O 0.6042405 -0.1749015 1.0000000 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/StanBugsJags/cfa "],["bayesian-non-parametric.html", "Bayesian Nonparametric Models Chinese Restaurant Process Indian Buffet Process Source", " Bayesian Nonparametric Models The following provides some conceptual code for the Chinese restaurant and Indian buffet process for categorical and continuous/combinations of categorical latent variables respectively. For more detail, see the Bayesian nonparametric section of my structural equation modeling document. Chinese Restaurant Process To start, we have a couple functions demonstrating the Chinese restaurant process. The first is succinct and more conceptual, but notably slower. crp &lt;- function(alpha, n) { table_assignments = 1 for (i in 2:n){ table_counts = table(table_assignments) # counts of table assignments nt = length(table_counts) # number of tables table_prob = table_counts/(i - 1 + alpha) # probabilities of previous table assignments # sample assignment based on probability of current tables and potential next table current_table_assignment = sample(1:(nt+1), 1, prob = c(table_prob, 1 - sum(table_prob))) # concatenate new to previous table assignments table_assignments = c(table_assignments, current_table_assignment) } table_assignments } The following function is similar to the restaurant function here https://github.com/mcdickenson/shinyapps, and notably faster. crpF &lt;- function(alpha, n) { table_assignments = c(1, rep(NA, n-1)) table_counts = 1 for (i in 2:n){ init = c(table_counts, alpha) table_prob = init/sum(init) current_table_assignment = sample(seq_along(init), 1, prob = table_prob) table_assignments[i] = current_table_assignment if (current_table_assignment == length(init)) { table_counts[current_table_assignment] = 1 } else { table_counts[current_table_assignment] = table_counts[current_table_assignment] + 1 } } table_assignments } # library(microbenchmark) # test = microbenchmark(crp(alpha = 1, n = 1000), # crpF(alpha = 1, n = 1000), times = 100) # test # ggplot2::autoplot(test) Visualize some examples at a given setting. out = replicate(5 , crpF(alpha = 1, n = 500), simplify = FALSE) library(tidyverse) map_df( out, function(x) data.frame(table(x)), .id = &#39;result&#39; ) %&gt;% rename(cluster = x) %&gt;% ggplot(aes(cluster, Freq)) + geom_col() + facet_grid(~ result) Visualize cluster membership. With smaller alpha, there is more tendency to stick to fewer clusters. set.seed(123) n = 100 crp_1 = crp(alpha = 1, n = n) crp_1_mat = matrix(0, nrow = n, ncol = n_distinct(crp_1)) for (i in 1:n_distinct(crp_1)) { crp_1_mat[, i] = ifelse(crp_1 == i, 1, 0) } crp_4 = crp(alpha = 5, n = n) crp_4_mat = matrix(0, nrow = n, ncol = n_distinct(crp_4)) for (i in 1:n_distinct(crp_4)) { crp_4_mat[, i] = ifelse(crp_4 == i, 1, 0) } heatmaply::heatmaply( crp_1_mat, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) heatmaply::heatmaply( crp_4_mat, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) Indian Buffet Process The following demonstrates the Indian buffet process for continuous latent variable settings. ibp &lt;- function(alpha, N){ # preallocate assignments with upper bound of N*alpha number of latent factors assignments = matrix(NA, nrow = N, ncol = N*alpha) # start with some dishes/assignments dishes = rpois(1, alpha) zeroes = ncol(assignments) - dishes # fill in the rest of potential dishes assignments[1, ] = c(rep(1, dishes), rep(0, zeroes)) for(i in 2:N){ prev = i - 1 # esoteric line that gets the last dish sampled without a search for it last_previously_sampled_dish = sum(colSums(assignments[1:prev, , drop = FALSE]) &gt; 0) # initialize dishes_previously_sampled = matrix(0, nrow=1, ncol=last_previously_sampled_dish) # calculate probability of sampling from previous dishes dish_prob = colSums(assignments[1:prev, 1:last_previously_sampled_dish, drop = FALSE]) / i dishes_previously_sampled[1, ] = rbinom(n = last_previously_sampled_dish, size = 1, prob = dish_prob) # sample new dish and assign based on results new_dishes = rpois(1, alpha/i) zeroes = ncol(assignments) - (last_previously_sampled_dish + new_dishes) assignments[i,] = c(dishes_previously_sampled, rep(1,new_dishes), rep(0, zeroes)) } # return only the dimensions sampled last_sampled_dish = sum(colSums(assignments[1:prev,]) &gt; 0) assignments[, 1:last_sampled_dish] } As before, we can compare different settings. set.seed(123) ibp_1 = ibp(1, 100) ibp_4 = ibp(5, 100) heatmaply::heatmaply( ibp_1, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) heatmaply::heatmaply( ibp_4, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/crp.R "],["bayesian-stochastic-volatility.html", "Bayesian Stochastic Volatility Model Data Setup Model Code Estimation Results Visualization Source", " Bayesian Stochastic Volatility Model Stochastic Volatility Model for centered time series over \\(t\\) equally spaced points. The latent parameter \\(h\\) is the log volatility, φ the persistence of the volatility and μ the mean log volatility. ϵ is the white-noise shock and δ the shock on volatility. The Stan code is based on that in the manual (at the time I originally played with it). y_t = exp(h_t/2)*ϵ_t h_t = μ + φ*(h_{t-1}-μ) + δ_t*σ h_1 ~ N(μ, σ/sqrt(1-φ^2)) ϵ_t ~ N(0,1); δ_t ~ N(0,1) With some rearranging: ϵ_t = y_t*exp(-h_t/2) y_t ~ N(0, exp(h_t/2) h_t ~ N(μ + φ*(h_t-μ), σ) Data Setup The data regards inflation based on the U.S. consumer price index (inflation = 400*log(cpi_t/cpi_{t-1}), from the second quarter of 1947 to the second quarter of 2011 (from Statistical Computation and Modeling 2014, chap 11). library(tidyverse) d = read_csv( &#39;https://raw.githubusercontent.com/m-clark/Datasets/master/us%20cpi/USCPI.csv&#39;, col_names = &#39;inflation&#39; ) inflation = pull(d, inflation) summary(inflation) Min. 1st Qu. Median Mean 3rd Qu. Max. -9.557 1.843 3.248 3.634 4.819 15.931 inflation_cen = scale(inflation, scale = FALSE) Model Code This original code keeps to the above formulation but can take a long time to converge. ϵ_t and δ_t are implicit. data { int&lt;lower = 0&gt; N_t; // Number of time points (equally spaced) vector[N_t] y; // mean corrected response at time t } parameters { real mu; // mean log volatility real&lt;lower = -1,upper = 1&gt; phi; // persistence of volatility real&lt;lower = 0&gt; sigma; // white noise shock scale vector[N_t] h; // log volatility at time t } model { //priors phi ~ uniform(-1, 1); sigma ~ cauchy(0, 5); mu ~ cauchy(0, 10); //likelihood h[1] ~ normal(mu, sigma / sqrt(1 - phi * phi)); for (t in 2:N_t) h[t] ~ normal(mu + phi * (h[t - 1] - mu), sigma); for (t in 1:N_t) y ~ normal(0, exp(h[t] / 2)); } This code is more performant and will be used to actually estimate the model. data { int&lt;lower = 0&gt; N_t; // N time points (equally spaced) vector[N_t] y; // mean corrected response at time t } parameters { real mu; // mean log volatility real&lt;lower = -1,upper = 1&gt; phi; // persistence of volatility real&lt;lower = 0&gt; sigma; // white noise shock scale vector[N_t] h_std; // standardized log volatility at time t } transformed parameters{ vector[N_t] h; // log volatility at time t h = h_std * sigma; h[1] = h[1] / sqrt(1-phi * phi); h = h + mu; for (t in 2:N_t) h[t] = h[t] + phi * (h[t-1] - mu); } model { //priors phi ~ uniform(-1, 1); sigma ~ cauchy(0, 5); mu ~ cauchy(0, 10); h_std ~ normal(0, 1); //likelihood y ~ normal(0, exp(h/2)); } generated quantities{ vector[N_t] y_rep; for (t in 1:N_t){ y_rep[t] = normal_rng(0, exp(h[t]/2)); } } Estimation We can use c() to get rid of matrix format, or specify as matrix instead of vector in model code. stan_data = list(N_t = length(inflation_cen), y = c(inflation_cen)) library(rstan) fit = sampling( bayes_sv, data = stan_data, cores = 4, thin = 4 ) Results Explore the results. print( fit, digits = 3, par = c(&#39;mu&#39;, &#39;phi&#39;, &#39;sigma&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: c65225b34c51f358116525cb9ba6c87c. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat mu 1.609 0.014 0.423 0.787 1.602 2.481 853 0.999 phi 0.893 0.002 0.040 0.803 0.897 0.966 723 0.999 sigma 0.620 0.004 0.114 0.396 0.619 0.851 763 0.997 Samples were drawn using NUTS(diag_e) at Wed Nov 25 18:00:01 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Visualization With the necessary components in place, we can visualize our predictions. Compare to fig. 11.1 in the text. # Create y_rep &#39;by-hand&#39; h = extract(fit, &#39;h&#39;)$h # y_rep = apply(h, 1, function(h) rnorm(length(inflation), 0, exp(h / 2))) # or just extract y_rep = extract(fit, &#39;y_rep&#39;)$y_rep h = colMeans(h) library(lubridate) library(scales) series = ymd(paste0(rep(1947:2014, e = 4), &#39;-&#39;, c(&#39;01&#39;, &#39;04&#39;, &#39;07&#39;, &#39;10&#39;), &#39;-&#39;, &#39;01&#39;)) seriestext = series[1:length(inflation)] Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/stochasticVolatility.R "],["bayesian-multinomial-model.html", "Bayesian Multinomial Models Data Setup Model Code Estimation Comparison Adding Complexity Source", " Bayesian Multinomial Models I spent some time on these models to better understand them in the traditional and Bayesian context, as well as profile potential speed gains in the Stan code. If you were doing what many would call ‘multinomial regression’ without qualification, I can recommend brms with the ‘categorical’ distribution. However, I’m not aware of it being able to accommodate choice-specific variables easily, i.e. ones that vary across choices (though it does accommodate choice specific effects). I show the standard model here with the usual demonstration, and show some code for the most complex setting of choice-specific, individual-specific, and choice-constant variables. See the [multinomial chapter][Multinomial] for the non-Bayesian approach. Data Setup Depending on the complexity of the data, you may need to create a data set specific to the problem. library(haven) library(tidyverse) program = read_dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;) %&gt;% as_factor() %&gt;% mutate(prog = relevel(prog, ref = &quot;academic&quot;)) head(program[,1:5]) # A tibble: 6 x 5 id female ses schtyp prog &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 45 female low public vocation 2 108 male middle public general 3 15 male high public vocation 4 67 male low public vocation 5 153 male middle public vocation 6 51 female high public general library(mlogit) programLong = program %&gt;% select(id, prog, ses, write) %&gt;% mlogit.data( shape = &#39;wide&#39;, choice = &#39;prog&#39;, id.var = &#39;id&#39; ) head(programLong) ~~~~~~~ first 10 observations out of 600 ~~~~~~~ id prog ses write chid alt idx 1 1 FALSE low 44 11 academic 11:emic 2 1 FALSE low 44 11 general 11:eral 3 1 TRUE low 44 11 vocation 11:tion 4 2 FALSE middle 41 9 academic 9:emic 5 2 FALSE middle 41 9 general 9:eral 6 2 TRUE middle 41 9 vocation 9:tion 7 3 TRUE low 65 159 academic 159:emic 8 3 FALSE low 65 159 general 159:eral 9 3 FALSE low 65 159 vocation 159:tion 10 4 TRUE low 50 30 academic 30:emic ~~~ indexes ~~~~ chid id alt 1 11 1 academic 2 11 1 general 3 11 1 vocation 4 9 2 academic 5 9 2 general 6 9 2 vocation 7 159 3 academic 8 159 3 general 9 159 3 vocation 10 30 4 academic indexes: 1, 1, 2 X = model.matrix(prog ~ ses + write, data = program) y = program$prog X = X[order(y),] y = y[order(y)] Model Code data { int K; int N; int D; int y[N]; matrix[N,D] X; } transformed data { vector[D] zeros; zeros = rep_vector(0, D); } parameters { matrix[D, K-1] beta_raw; } transformed parameters { matrix[D, K] beta; beta = append_col(zeros, beta_raw); } model { matrix[N, K] L; # Linear predictor L = X * beta; // prior for coefficients to_vector(beta_raw) ~ normal(0, 10); // likelihood for (n in 1:N) y[n] ~ categorical_logit(to_vector(L[n])); } Estimation We’ll get the data prepped for Stan, and the model code is assumed to be in an object bayes_multinom. # N = sample size, x is the model matrix, y integer version of class outcome, k= # number of classes, D is dimension of model matrix stan_data = list( N = nrow(X), X = X, y = as.integer(y), K = n_distinct(y), D = ncol(X) ) library(rstan) fit = sampling( bayes_multinom, data = stan_data, thin = 4, cores = 4 ) Comparison We’ll need to do a bit of reordering, but otherwise we can see that the models come to similar conclusions. print( fit, digits = 3, par = c(&#39;beta&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: 6f6b51695c6eeda4b74c0665f4447c97. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat beta[1,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[1,2] 2.834 0.037 1.214 0.453 2.823 5.212 1100 0.999 beta[1,3] 5.271 0.039 1.187 2.906 5.227 7.640 910 0.999 beta[2,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[2,2] -0.537 0.014 0.437 -1.388 -0.533 0.332 1030 1.002 beta[2,3] 0.340 0.016 0.488 -0.616 0.346 1.282 956 1.001 beta[3,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[3,2] -1.207 0.016 0.508 -2.193 -1.199 -0.232 1073 1.000 beta[3,3] -1.001 0.021 0.617 -2.187 -0.979 0.145 864 0.999 beta[4,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[4,2] -0.058 0.001 0.022 -0.103 -0.058 -0.015 1068 1.000 beta[4,3] -0.116 0.001 0.023 -0.160 -0.115 -0.072 912 1.000 Samples were drawn using NUTS(diag_e) at Wed Nov 25 18:00:51 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). fit_coefs = get_posterior_mean(fit, par = &#39;beta_raw&#39;)[, 5] mlogit_mod = mlogit(prog ~ 1 | ses + write, data = programLong) mlogit_coefs = coef(mlogit_mod)[c(1, 3, 5, 7, 2, 4, 6, 8)] m_logit fit (Intercept):general 2.852 2.834 (Intercept):vocation 5.218 5.271 sesmiddle:general -0.533 -0.537 sesmiddle:vocation 0.291 0.340 seshigh:general -1.163 -1.207 seshigh:vocation -0.983 -1.001 write:general -0.058 -0.058 write:vocation -0.114 -0.116 Adding Complexity The following adds choice-specific (a.k.a. alternative-specific) variables, e.g. among product choices, this may include price. Along with this we may have, along with choice constant, and the typical individual varying covariates. This code worked at the time, but I wasn’t interested enough to try it again recently. You can use the classic ‘travel’ data as an example (available as TravelMode in AER), or fishing from mlogit. Essentially you’ll have three separate data components- a matrix for individual-specific covariates, one for alternative specific, and one for alternative constant covariates. data { int K; // number of choices int N; // number of individuals int D; // number of indiv specific variables int G; // number of alt specific variables int T; // number of alt constant variables int y[N*K]; // choices vector[N*K] choice; // choice made (logical) matrix[N, D] X; // data for indiv specific effects matrix[N*K, G] Y; // data for alt specific effects matrix[N*(K-1), T] Z; // data for alt constant effects } parameters { matrix[D, K-1] beta; // individual specific coefs matrix[G, K] gamma; // choice specific coefs for alt-specific variables vector[T] theta; // choice constant coefs for alt-specific variables } model { matrix[N, K-1] Vx; // Utility for individual vars vector[N*K] Vy0; matrix[N, K-1] Vy; // Utility for alt-specific/alt-varying vars vector[N*(K-1)] Vz0; matrix[N, (K-1)] Vz; // Utility for alt-specific/alt-constant vars matrix[N, K-1] V; // combined utilities vector[N] baseProbVec; // reference group probabilities real ll0; // intermediate log likelihood real loglik; // final log likelihood // priors to_vector(beta) ~ normal(0, 10); // diffuse priors on coefficients to_vector(gamma) ~ normal(0, 10); to_vector(theta) ~ normal(0, 10); // likelihood // &#39;Utilities&#39; Vx = X * beta; for(alt in 1:K){ vector[G] par; int start; int end; par = gamma[,alt]; start = N*alt - N+1; end = N*alt; Vy0[start:end] = Y[start:end,] * par; if(alt &gt; 1) Vy[,alt-1] = Vy0[start:end] - Vy0[1:N]; } Vz0 = Z * theta; for(alt in 1:(K-1)){ int start; int end; start = N*alt - N+1; end = N*alt; Vz[,alt] = Vz0[start:end]; } V = Vx + Vy + Vz; for(n in 1:N) baseProbVec[n] = 1/(1 + sum(exp(V[n]))); ll0 = dot_product(to_vector(V), choice[(N+1):(N*K)]); // just going to assume no neg index loglik = sum(log(baseProbVec)) + ll0; target += loglik; } generated quantities { matrix[N, K-1] fitted_nonref; vector[N] fitted_ref; matrix[N, K] fitted; matrix[N, K-1] Vx; // Utility for individual variables vector[N*K] Vy0; matrix[N, K-1] Vy; // Utility for alt-specific/alt-varying variables vector[N*(K-1)] Vz0; matrix[N, (K-1)] Vz; // Utility for alt-specific/alt-constant variables matrix[N, K-1] V; // combined utilities vector[N] baseProbVec; // reference group probabilities Vx = X * beta; for(alt in 1:K) { vector[G] par; int start; int end; par = gamma[,alt]; start = N*alt - N+1; end = N*alt; Vy0[start:end] = Y[start:end, ] * par; if (alt &gt; 1) Vy[,alt-1] = Vy0[start:end] - Vy0[1:N]; } Vz0 = Z * theta; for(alt in 1:(K-1)){ int start; int end; start = N*alt-N+1; end = N*alt; Vz[,alt] = Vz0[start:end]; } V = Vx + Vy + Vz; for(n in 1:N) baseProbVec[n] = 1 / (1 + sum(exp(V[n]))); fitted_nonref = exp(V) .* rep_matrix(baseProbVec, K-1); for(n in 1:N) fitted_ref[n] = 1 - sum(fitted_nonref[n]); fitted = append_col(fitted_ref, fitted_nonref); } Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/Bayesian/multinomial "],["bayesian-variational.html", "Variational Bayes Regression Data Setup Function Estimation Comparison Visualization Supplemental Example Source", " Variational Bayes Regression The following provides a function for estimating the parameters of a linear regression via variational inference. See Drugowitsch (2014) for an overview of the method outlined in Bishop (2006). For the primary function I will use the notation used in the Drugowitsch article in most cases. Here w, represents the coefficients, and τ the precision (inverse variance). The likelihood for target y is N(Xw, τ-1). Priors for w and tau are normal inverse gamma N(0, (τα)-1) Gamma(a0, b0). References: Drugowitsch: http://arxiv.org/abs/1310.5438 See here and here for his Matlab implementations. Bishop: Pattern Recognition and Machine Learning Data Setup We can simulate some data as a starting point, in this case, basic tabular data used in the standard regression problem. Here, I explicitly note the intercept, as it is added to the model matrix within the vb_reg function. library(tidyverse) set.seed(1234) n = 100 d = 3 coefs = c(1, 2, 3, 5) sigma = 2 X = replicate(d, rnorm(n)) # predictors colnames(X) = paste0(&#39;X&#39;, 1:d) y = cbind(1, X) %*% coefs + rnorm(n, sd = sigma) # target df = data.frame(X, y) We can also look at the higher dimension case as done in Drugowitsch section 2.6.2. n = 150 ntest = 50 d = 100 coefs = rnorm(d + 1) sigma = 1 X_train = cbind(1, replicate(d, rnorm(n))) y_train = X_train %*% coefs + rnorm(n, sd = sigma) X_test = cbind(1, replicate(d, rnorm(ntest))) y_test = X_test %*% coefs + rnorm(ntest, sd = sigma) Function First, the main function. For this demo, automatic relevance determination is an argument rather than a separate function. vb_reg &lt;- function( X, y, a0 = 10e-2, b0 = 10e-4, c0 = 10e-2, d0 = 10e-4, tol = 1e-8, maxiter = 1000, ard = F ) { # X: model matrix # y: the response # a0, b0 prior parameters for tau # c0, d0 hyperprior parameters for alpha # tol: tolerance value to end iterations # maxiter: alternative way to end iterations # initializations X = cbind(1, X) D = ncol(X) N = nrow(X) w = rep(0, D) XX = crossprod(X) Xy = crossprod(X,y) a_N = a0 + N/2 if (!ard) { c_N = c0 + D/2 E_alpha = c0/d0 } else { c_N = c0 + 1/2 E_alpha = rep(c0/d0, D) } tolCurrent = 1 iter = 0 LQ = 0 while(iter &lt; maxiter &amp;&amp; tolCurrent &gt; tol ){ iter = iter + 1 # wold = w if(!ard){ b_N = b0 + 1/2 * (crossprod(y - X%*%w) + E_alpha * crossprod(w)) VInv = diag(E_alpha, D) + XX V = solve(VInv) w = V %*% Xy E_wtau = a_N/b_N * crossprod(w) + sum(diag(V)) d_N = d0 + 1/2*E_wtau E_alpha = c(c_N/d_N) } else { b_N = b0 + 1/2 * (crossprod(y - X%*%w) + t(w) %*% diag(E_alpha) %*% w) VInv = diag(E_alpha) + XX V = solve(VInv) w = V %*% Xy E_wtau = a_N/b_N*crossprod(w) + sum(diag(V)) d_N = d0 + 1/2*(c(w)^2 * c(a_N/b_N) + diag(V)) E_alpha = c(c_N/d_N) } LQ_old = LQ suppressWarnings({ LQ = -N/2*log(2*pi) - 1/2 * (a_N/b_N * crossprod(y- crossprod(t(X), w)) + sum(XX * V)) + 1/2 * determinant(V, log = TRUE)$modulus + D/2 - lgamma(a0) + a0 * log(b0) - b0 * a_N / b_N + lgamma(a_N) - a_N * log(b_N) + a_N - lgamma(c0) + c0*log(d0) + lgamma(c_N) - sum(c_N*log(d_N)) }) tolCurrent = abs(LQ - LQ_old) # alternate tolerance, comment out LQ_old up to this line if using # tolCurrent = sum(abs(w - wold)) } res = list( coef = w, sigma = sqrt(1 / (E_wtau / crossprod(w))), LQ = LQ, iterations = iter, tol = tolCurrent ) if (iter &gt;= maxiter) append(res, warning(&#39;Maximum iterations reached.&#39;)) else res } Estimation First we can estimate the model using the smaller data. fit_small = vb_reg(X, y, tol = 1e-8, ard = FALSE) glimpse(fit_small) List of 5 $ coef : num [1:4, 1] 1.01 2.29 3.29 5.02 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; .. ..$ : NULL $ sigma : num [1, 1] 2.08 $ LQ : num [1, 1] -233 ..- attr(*, &quot;logarithm&quot;)= logi TRUE $ iterations: num 8 $ tol : num [1, 1] 1.11e-10 ..- attr(*, &quot;logarithm&quot;)= logi TRUE # With automatic relevance determination fit_small_ard = vb_reg(X, y, tol = 1e-8, ard = TRUE) glimpse(fit_small_ard) List of 5 $ coef : num [1:4, 1] 0.955 2.269 3.283 5.047 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; .. ..$ : NULL $ sigma : num [1, 1] 2.09 $ LQ : num [1, 1] -229 ..- attr(*, &quot;logarithm&quot;)= logi TRUE $ iterations: num 9 $ tol : num [1, 1] 7.46e-09 ..- attr(*, &quot;logarithm&quot;)= logi TRUE lm_mod = lm(y ~ ., data = df) Now with the higher dimensional data. We fit using the training data and will estimate the error on training and test using the yardstick package. fit_vb = vb_reg(X_train[,-1], y_train) fit_glm = glm.fit(X_train, y_train) # predictions vb_pred_train = X_train %*% fit_vb[[&#39;coef&#39;]] vb_pred_test = X_test %*% fit_vb[[&#39;coef&#39;]] glm_pred_train = fitted(fit_glm) glm_pred_test = X_test %*% coef(fit_glm) # error vb_train_error = yardstick::rmse_vec(y_train[,1], vb_pred_train[,1]) vb_test_error = yardstick::rmse_vec(y_test[,1], vb_pred_test[,1]) glm_train_error = yardstick::rmse_vec(y_train[,1], glm_pred_train) glm_test_error = yardstick::rmse_vec(y_test[,1], glm_pred_test[,1]) Comparison For the smaller data, we will compare the coefficients. no_ard ard lm 1.010 0.955 1.012 2.291 2.269 2.300 3.286 3.283 3.297 5.024 5.047 5.045 For the higher dimensional data, we will compare root mean square error. vb glm train 0.574 0.566 test 1.876 1.982 Visualization In general the results are as expected where the standard approach overfits relative to VB regression. The following visualizes them, similar to Drugowitsch figure 1. Supplemental Example And now for a notably higher dimension case with irrelevant predictors as in Drugowitsch section 2.6.3. This is problematic for the GLM with having more covariates than data points (rank deficient), and as such it will throw a warning, as will the predict function. It’s really not even worth looking at but I have the code for consistency. This will take a while to estimate, and without ARD, even bumping up the iterations to 2000 it will still likely hit the max before reaching the default tolerance level. However, the results appear very similar to that of Drugowitsch Figure 2. set.seed(1234) n = 500 ntest = 50 d = 1000 deff = 100 coefs = rnorm(deff + 1) sigma = 1 X_train = cbind(1, replicate(d, rnorm(n))) y_train = X_train %*% c(coefs, rep(0, d - deff)) + rnorm(n, sd = sigma) X_test = cbind(1, replicate(d, rnorm(ntest))) y_test = X_test %*% c(coefs, rep(0, d - deff)) + rnorm(ntest, sd = sigma) fit_vb = vb_reg(X_train[,-1], y_train) fit_vb_ard = vb_reg(X_train[,-1], y_train, ard = TRUE) # fit_glm = glm(y_train ~ ., data = data.frame(X_train[,-1])) # predictions vb_pred_train = X_train %*% fit_vb[[&#39;coef&#39;]] vb_pred_test = X_test %*% fit_vb[[&#39;coef&#39;]] # vb_ard_pred_train = X_train %*% fit_vb_ard[[&#39;coef&#39;]] vb_ard_pred_test = X_test %*% fit_vb_ard[[&#39;coef&#39;]] # glm_pred_train = fitted(fit_glm) # glm_pred_test = X_test %*% coef(fit_glm) # error vb_train_error = yardstick::rmse_vec(y_train[,1], vb_pred_train[,1]) vb_test_error = yardstick::rmse_vec(y_test[,1], vb_pred_test[,1]) # error vb_ard_train_error = yardstick::rmse_vec(y_train[,1], vb_ard_pred_train[,1]) vb_ard_test_error = yardstick::rmse_vec(y_test[,1], vb_ard_pred_test[,1]) # glm_train_error = yardstick::rmse_vec(y_train[,1], glm_pred_train) # glm_test_error = yardstick::rmse_vec(y_test[,1], glm_pred_test[,1]) mse_results = data.frame( vb = c(vb_train_error, vb_test_error), vbARD = c(vb_ard_train_error, vb_ard_test_error)#, # glm = c(glm_train_error, glm_test_error) ) rownames(mse_results) = c(&#39;train&#39;, &#39;test&#39;) kable_df(mse_results) vb vbARD train 0.641 0.002 test 8.378 2.323 Note how ARD correctly estimates (nearly) zero for irrelevant predictors. N Mean SD Min Q1 Median Q3 Max % Missing 900 0 0 -0.2 0 0 0 0.4 0 Visualized, as before. Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/Bayesian/multinomial "],["bayesian-topic-model.html", "Topic Model Data Setup Function Estimation Comparison Source", " Topic Model An implementation of Gibbs sampling for topic models for the example in section 4 of Steyvers and Griffiths (2007). A very clear intro in my opinion. The core of the function’s code comprises mostly cosmetic changes to that found here. Added are the creation of a function with several arguments, plotting etc. Data Setup library(tidyverse) vocab = factor(c(&quot;river&quot;, &quot;stream&quot;, &quot;bank&quot;, &quot;money&quot;, &quot;loan&quot;)) K = 2 # n of topics v = length(vocab) # number of unique words d = 16 # number of documents Topic 1 gives equal probability to money loan and bank (zero for river and stream). Topic 2 gives equal probability to river stream and bank (zero for money and loan). Next we create a document term matrix. Each doc consists of a mix of 16 tokens of the vocab- the first few regard financial banks, the last several water banks, and the docs in between possess a mixed vocab. dtm = matrix(c(0,0,4,6,6, 0,0,5,7,4, 0,0,7,5,4, 0,0,7,6,3, 0,0,7,2,7, 0,0,9,3,4, 1,0,4,6,5, 1,2,6,4,3, 1,3,6,4,2, 2,3,6,1,4, 2,3,7,3,1, 3,6,6,1,0, 6,3,6,0,1, 2,8,6,0,0, 4,7,5,0,0, 5,7,4,0,0), ncol = v, byrow = TRUE) rownames(dtm) = paste0(&#39;doc&#39;, 1:d) colnames(dtm) = vocab Next we create additional objects to initialize the setup. # matrix of words in each document wordmat = t(apply(dtm, 1, function(row) rep(vocab, row))) # initialize random topic assignments to words T0 = apply(wordmat, c(1, 2), function(token) sample(1:2, 1)) # word by topic matrix of counts containing the number of times word w is # assigned to topic j C_wt = sapply(vocab, function(word) cbind(sum(T0[wordmat == word] == 1), sum(T0[wordmat == word] == 2))) C_wt = t(C_wt) rownames(C_wt) = vocab # topic by document matrix of counts containing the number of times topic j is # assigned to a word in document d C_dt = t(apply(T0, 1, table)) Function Note that this function is not self contained in that it uses some of the objects created above, assuming they are in the global environment. It has the capacity for a pseudo-visualization which can be instructive, but isn’t shown for this document (and it requries corrplot), and I haven’t tested it recently. topic_model &lt;- function( alpha, beta, nsim = 2000, warmup = nsim / 2, thin = 10, verbose = TRUE, plot = FALSE, dotsortext = &#39;dots&#39; ) { # Arguments: # alpha and beta: hyperparameters # nsim: the total number of simulations # warmup: the number of initial sims to discard # thin: keep every thin simulation after warmup # verbose: to print every 100th iteration and total time # plot: to visualize every x sim; the plot will show current estimates of # theta, phi, and topic assignments for each term in the docs; can be # visualized as dots or the actual terms; for the latter you may need to # fiddle with your viewing area size so that terms don&#39;t appear to run # together; plotting will increase runtime # requires corrplot for visualizations, and abind to create arrays from the # lists. # initialize Z = T0 # topic assignments saveSim = seq(warmup + 1, nsim, thin) # iterations to save theta_list = list() # saved theta estimates phi_list = list() # saved phi estimates p = proc.time() # for every simulation... for (s in 1:nsim) { if(verbose &amp;&amp; s %% 100 == 0) { # paste every 100th iteration if desired secs = round((proc.time() - p)[3],2) min = round(secs/60, 2) message(paste0( &#39;Iteration number: &#39;, s, &#39;\\n&#39;, &#39;Total time: &#39;, ifelse(secs &lt;= 60, paste(secs, &#39;seconds&#39;), paste(min, &#39;minutes&#39;)) )) } # Visualization if(plot &gt; 0 &amp;&amp; s &gt;1 &amp;&amp; s %% plot == 0) { # plot every value of argument require(corrplot) layout(matrix(c(1, 1, 2, 2, 3, 3, 3, 3, 3, 3), ncol = 10)) corrplot( theta, is.corr = FALSE, method = &#39;color&#39;, tl.cex = .75, tl.col = &#39;gray50&#39;, cl.pos = &#39;n&#39;, addgrid = NA ) corrplot( phi, is.corr = FALSE, method = &#39;color&#39;, tl.cex = 1, tl.col = &#39;gray50&#39;, cl.pos = &#39;n&#39;, addgrid = NA ) if (dotsortext == &#39;dots&#39;) { Zplot = Z Zplot[Zplot == 2] = -1 corrplot( Zplot, is.corr = FALSE, method = &#39;circle&#39;, tl.cex = .75, tl.col = &#39;gray50&#39;, cl.pos = &#39;n&#39;, addgrid = NA ) } else { cols = apply(Z, c(1, 2), function(topicvalue) ifelse(topicvalue == 1, &#39;#053061&#39;, &#39;#67001F&#39;)) plot( 1:nrow(wordmat), 1:ncol(wordmat), type = &quot;n&quot;, axes = FALSE, xlab = &#39;&#39;, ylab = &#39;&#39; ) text(col(wordmat), rev(row(wordmat)), wordmat, col = cols, cex = .75) } } # for every document and every word in the document for (i in 1:d) { for (j in 1:length(wordmat[i,])) { word.id = which(vocab == wordmat[i, j]) topic.old = Z[i, j] # Decrement counts before computing equation (3) in paper noted above C_dt[i, topic.old] = C_dt[i, topic.old] - 1 C_wt[word.id, topic.old] = C_wt[word.id, topic.old] - 1 # Calculate equation (3) for each topic vals = prop.table(C_wt + beta, 2)[word.id,] * prop.table(C_dt[i,] + alpha) # Sample the new topic from the results for (3); # note, sample function does not require you to have the probs sum to 1 # explicitly, i.e. prob=c(1,1,1) is the same as prob = c(1/3, 1/3, 1/3) Z.new = sample(1:K, 1, prob = vals) # Set the new topic and update counts Z[i,j] = Z.new C_dt[i, Z.new] = C_dt[i, Z.new] + 1 C_wt[word.id, Z.new] = C_wt[word.id, Z.new] + 1 } } theta = prop.table(C_dt + alpha,1) # doc topic distribution phi = prop.table(C_wt + beta,2) # word topic distribution # save simulations if (s %in% saveSim){ theta_list[[paste(s)]] = theta phi_list[[paste(s)]] = phi } } layout(1) # reset plot window list( theta = theta, phi = phi, theta_sims = abind::abind(theta_list, along = 3), phi_sims = abind::abind(phi_list, along = 3) ) } Estimation We use the values of alpha and beta as suggested in paper. The following will result in 500 posterior sample points, so bump if you want more than that. These settings take around 20 seconds. set.seed(1234) alpha = K/50 beta = .01 fit_tm = topic_model( alpha = alpha, beta = beta, nsim = 1000, warmup = 500, thin = 1, verbose = FALSE, plot = FALSE ) Comparison First we can explore the results, in particular topic assignment probabilities and term topic probabilities. str(fit_tm, 1) List of 4 $ theta : num [1:16, 1:2] 0.00249 0.00249 0.00249 0.00249 0.00249 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ phi : num [1:5, 1:2] 0.267293 0.415735 0.316774 0.000099 0.000099 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ theta_sims: num [1:16, 1:2, 1:500] 0.00249 0.00249 0.00249 0.00249 0.00249 ... ..- attr(*, &quot;dimnames&quot;)=List of 3 $ phi_sims : num [1:5, 1:2, 1:500] 2.38e-01 4.00e-01 3.62e-01 9.52e-05 9.52e-05 ... ..- attr(*, &quot;dimnames&quot;)=List of 3 purrr::map(fit_tm[1:2], round, 2) $theta 1 2 doc1 0.00 1.00 doc2 0.00 1.00 doc3 0.00 1.00 doc4 0.00 1.00 doc5 0.00 1.00 doc6 0.00 1.00 doc7 0.06 0.94 doc8 0.19 0.81 doc9 0.50 0.50 doc10 0.44 0.56 doc11 0.44 0.56 doc12 0.81 0.19 doc13 0.87 0.13 doc14 1.00 0.00 doc15 1.00 0.00 doc16 1.00 0.00 $phi [,1] [,2] river 0.27 0.00 stream 0.42 0.00 bank 0.32 0.41 money 0.00 0.31 loan 0.00 0.28 Now we can compare our result to reported paper estimates for \\(\\phi\\) and topicmodels package (note delta is the beta above; the vignette actually references Steyvers &amp; Griffiths paper). library(topicmodels) fit_lda = LDA( dtm, k = 2, method = &#39;Gibbs&#39;, control = list( alpha = alpha, delta = .01, iter = 5500, burnin = 500, thin = 10, initialize = &#39;random&#39; ) ) Start with \\(\\phi\\) estimates. Note that the labels of what is topic 1 vs. topic 2 is arbitrary, the main thing is the separation of the words that should go to different topics, while bank is probable for both topics. fit.1 fit.2 paper.1 paper.2 ldapack.1 ldapack.2 river 0.27 0.00 0.00 0.25 0.29 0.00 stream 0.42 0.00 0.00 0.40 0.46 0.00 bank 0.32 0.41 0.39 0.35 0.25 0.44 money 0.00 0.31 0.32 0.00 0.00 0.29 loan 0.00 0.28 0.29 0.00 0.00 0.27 Interval estimates for term probabilities for each topic. library(coda) phi_estimates_topic_1 = as.mcmc(t(fit_tm$phi_sims[, 1, ])) phi_estimates_topic_2 = as.mcmc(t(fit_tm$phi_sims[, 2, ])) # summary(phi_estimates_topic_1) rowname Topic lower upper river…1 Topic 1 0.24 0.28 stream…2 Topic 1 0.36 0.43 bank…3 Topic 1 0.29 0.38 money…4 Topic 1 0.00 0.01 loan…5 Topic 1 0.00 0.01 river…6 Topic 2 0.00 0.01 stream…7 Topic 2 0.00 0.00 bank…8 Topic 2 0.36 0.42 money…9 Topic 2 0.30 0.33 loan…10 Topic 2 0.27 0.30 Symmetrized or mean Kullback-Liebler divergence for topic (dis)similarity. kl_divergence = .5 * sum( apply( fit_tm$phi, 1, function(row) row[1] * log2(row[1] / row[2]) + row[2] * log2(row[2] / row[1]) ) ) kl_divergence [1] 7.678387 We can compare with various other packages for KL-divergence (not shown). mean(c( entropy::KL.empirical(fit_tm$phi[,1], fit_tm$phi[,2], unit = &#39;log2&#39;), entropy::KL.empirical(fit_tm$phi[,2], fit_tm$phi[,1], unit = &#39;log2&#39;) )) Symmetrized or mean Kullback-Liebler divergence for document (dis)similarity. # example docs2compare = c(1, 16) .5 * sum(apply(fit_tm$theta[docs2compare, ], 2, function(topicprob) topicprob[1] * log(topicprob[1] / topicprob[2]) + topicprob[2] * log(topicprob[2] / topicprob[1]))) [1] 5.964141 Let’s create a function to work on theta to produce Kullback-Liebler or Jensen-Shannon divergence. divergence &lt;- function(input, method = &#39;KL&#39;) { if (method == &#39;KL&#39;) { .5 * sum(apply(fit_tm$theta[input, ], 2, function(topicprob) topicprob[1] * log2(topicprob[1] / topicprob[2]) + topicprob[2] * log2(topicprob[2] / topicprob[1]))) } else { .5 * sum(apply(fit_tm$theta[input, ], 2, function(topicprob) topicprob[1] * log2(topicprob[1] / mean(topicprob)) + topicprob[2] * log2(topicprob[2] / mean(topicprob)))) } } Now apply to all pairs of documents. # Now do for all docpairs = combn(1:d, 2) kl_divergence = apply(docpairs, 2, divergence) mat0 = matrix(0, d, d) mat0[lower.tri(mat0)] = kl_divergence kl_divergence = mat0 + t(mat0) dimnames(kl_divergence) = list(rownames(dtm)) We can visualizes as follows, red implies documents are more dissimilar. Likewise, we can visualize Jensen-Shannon divergence. js_divergence = apply(docpairs, 2, divergence, method = &#39;JS&#39;) Here are examples of diagnostics with the word topic probability estimates. These use the bayesplot package. library(bayesplot) mcmc_combo(phi_estimates_topic_1) mcmc_acf(phi_estimates_topic_1) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/topicModelgibbs.R "],["maximum-likelihood.html", "Maximum Likelihood Linear Model Source", " Maximum Likelihood This is a brief refresher on maximum likelihood estimation using a standard regression approach as an example, and more or less assumes one hasn’t tried to roll their own such function in a programming environment before. Given the likelihood’s role in Bayesian estimation and statistics in general, and the ties between specific Bayesian results and maximum likelihood estimates one typically comes across, one should be conceptually comfortable with some basic likelihood estimation. The following is taken directly from my document with mostly just cleaned up code and visualization. The TLDR version can be viewed in the Linear Regression chapter. In the standard model setting we attempt to find parameters \\(\\theta\\) that will maximize the probability of the data we actually observe. We’ll start with an observed random target vector \\(y\\) with \\(i...N\\) independent and identically distributed observations and some data-generating process underlying it \\(f(\\cdot|\\theta)\\). We are interested in estimating the model parameter(s), \\(\\theta\\), that would make the data most likely to have occurred. The probability density function for \\(y\\) given some particular estimate for the parameters can be noted as \\(f(y_i|\\theta)\\). The joint probability distribution of the (independent) observations given those parameters, \\(f(y_i|\\theta)\\), is the product of the individual densities, and is our likelihood function. We can write it out generally as: \\[\\mathcal{L}(\\theta) = \\prod_{i=1}^N f(y_i|\\theta)\\] Thus, the likelihood for one set of parameter estimates given a fixed set of data y, is equal to the probability of the data given those (fixed) estimates. Furthermore, we can compare one set, \\(\\mathcal{L}(\\theta_A)\\), to that of another, \\(\\mathcal{L}(\\theta_B)\\), and whichever produces the greater likelihood would be the preferred set of estimates. We can get a sense of this with the following visualization, based on a single parameter. The data is drawn from Poisson distributed variable with mean \\(\\theta=5\\). We note the calculated likelihood increases as we estimate values for \\(\\theta\\) closer to \\(5\\), or more precisely, whatever the mean observed value is for the data. However, with more and more data, the final ML estimate will converge on the true value. Final estimate = 5.02 For computational reasons, we instead work with the sum of the natural log probabilities, and so deal with the log likelihood: \\[\\ln\\mathcal{L}(\\theta) = \\sum_{i=1}^N \\ln[f(y_i|\\theta)]\\] Concretely, we calculate a log likelihood for each observation and then sum them for the total likelihood for parameter(s) \\(\\theta\\). The likelihood function incorporates our assumption about the sampling distribution of the data given some estimate for the parameters. It can take on many forms and be notably complex depending on the model in question, but once specified, we can use any number of optimization approaches to find the estimates of the parameter that make the data most likely. As an example, for a normally distributed variable of interest we can write the log likelihood as follows: \\[\\ln\\mathcal{L}(\\theta) = \\sum_{i=1}^N \\ln[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(y-\\mu)^2}{2\\sigma^2})]\\] Example In the following we will demonstrate the maximum likelihood approach to estimation for a simple setting incorporating a normal distribution, where we estimate the mean and variance/sd for a set of values \\(y\\). First the data is created, and then we create the function that will compute the log likelihood. Using the built in R distributions makes it fairly straightforward to create our own likelihood function and feed it into an optimization function to find the best parameters. We will set things up to work with the bbmle package, which has some nice summary functionality and other features. However, one should take a glance at optim and the other underlying functions that do the work. # for replication set.seed(1234) # create the data y = rnorm(1000, mean = 5, sd = 2) starting_values = c(0, 1) # the log likelihood function simple_ll &lt;- function(mu, sigma, verbose = TRUE) { ll = sum(dnorm(y, mean = mu, sd = sigma, log = TRUE)) if (verbose) message(paste(mu, sigma, ll)) -ll } The simple_ll function takes starting points for the parameters as arguments, in this case we call them \\(\\mu\\) and \\(\\sigma\\), which will be set to 0 and 1 respectively. Only the first line (ll = -sum…) is actually necessary, and we use dnorm to get the density for each point. Since this optimizer is by default minimization, we reverse the sign of the sum so as to minimize the negative log likelihood, which is the same as maximizing the likelihood. Note that the bit of other code just allows you to see the estimates as the optimization procedure searches for the best values. I do not show that here but you’ll see it in your console if trace = TRUE. We are now ready to obtain maximum likelihood estimates for the parameters. For comparison we will use bbmle due to its nice summary result, but you can use optim as in the other demonstrations. For the mle2 function we will need the function we’ve created, plus other inputs related to that function or the underlying optimizing function used (by default optim). In this case we will use an optimization procedure that will allow us to set a lower bound for \\(\\sigma\\). This isn’t strictly necessary, but otherwise you would get warnings and possibly lack of convergence if negative estimates for \\(\\sigma\\) were allowed. # using optim, and L-BFGS-B so as to constrain sigma to be positive by setting # the lower bound at zero mlnorm = bbmle::mle2( simple_ll, start = list(mu = 2, sigma = 1), method = &quot;L-BFGS-B&quot;, lower = c(sigma = 0), trace = TRUE ) mlnorm Call: bbmle::mle2(minuslogl = simple_ll, start = list(mu = 2, sigma = 1), method = &quot;L-BFGS-B&quot;, trace = TRUE, lower = c(sigma = 0)) Coefficients: mu sigma 4.946803 1.993680 Log-likelihood: -2108.92 # compare to an intercept only regression model summary(lm(y~1)) Call: lm(formula = y ~ 1) Residuals: Min 1Q Median 3Q Max -6.7389 -1.2933 -0.0264 1.2848 6.4450 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.94681 0.06308 78.42 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.995 on 999 degrees of freedom We can see that the ML estimates are the same as the lm model estimates based on least squares, and which given the sample size are close to the true values. In terms of the parameters we estimate, instead of the curve presented previously, in the typical case of two or more parameters we can think of a likelihood surface that represents the possible likelihood values given any particular set of estimates. Given some starting point, the optimization procedure then travels along the surface looking for a minimum/maximum point. For simpler settings such as this, we can visualize the likelihood surface and its minimum point. The optimizer travels along this surface until it finds a minimum (the surface plot is interactive- feel free to adjust). I also plot the path of the optimizer from a top down view. The large dot noted represents the minimum negative log likelihood. Please note that there are many other considerations in optimization completely ignored here, but for our purposes and the audience for which this is intended, we do not want to lose sight of the forest for the trees. We now move next to a slightly more complicated regression example. Linear Model In the standard regression context, our expected value for the target variable comes from our linear predictor, i.e. the weighted combination of our explanatory variables, and we estimate the regression weights/coefficients and possibly other relevant parameters. We can expand our previous example to the standard linear model without too much change. In this case we estimate a mean for each observation, but otherwise assume the variance is constant across observations. Again, we first construct some data so that we know exactly what to expect, then write out the likelihood function with starting parameters. As we need to estimate our intercept and coefficient for the X predictor (collectively referred to as \\(\\beta\\)), we can think of our likelihood explicitly as before: \\[\\ln\\mathcal{L}(\\beta, \\sigma^2) = \\sum_{i=1}^N \\ln[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(y-X\\beta)^2}{2\\sigma^2})]\\] # for replication set.seed(1234) # predictor X = rnorm(1000) # coefficients for intercept and predictor beta = c(5, 2) # add intercept to X and create y with some noise y = cbind(1, X) %*% beta + rnorm(1000, sd = 2.5) regression_ll &lt;- function(sigma = 1, Int = 0, b1 = 0) { coefs = c(Int, b1) mu = cbind(1,X)%*%coefs ll = -sum(dnorm(y, mean = mu, sd = sigma, log = TRUE)) message(paste(sigma, Int, b1, ll)) ll } mlopt = bbmle::mle2(regression_ll, method = &quot;L-BFGS-B&quot;, lower = c(sigma = 0)) summary(mlopt) Maximum likelihood estimation Call: bbmle::mle2(minuslogl = regression_ll, method = &quot;L-BFGS-B&quot;, lower = c(sigma = 0)) Coefficients: Estimate Std. Error z value Pr(z) sigma 2.447823 0.054735 44.721 &lt; 2.2e-16 *** Int 5.039976 0.077435 65.087 &lt; 2.2e-16 *** b1 2.139284 0.077652 27.549 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: 4628.273 # plot(profile(mlopt), absVal=F) modlm = lm(y ~ X) summary(modlm) Call: lm(formula = y ~ X) Residuals: Min 1Q Median 3Q Max -7.9152 -1.6097 0.0363 1.6343 7.6711 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.03998 0.07751 65.02 &lt;2e-16 *** X 2.13928 0.07773 27.52 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.45 on 998 degrees of freedom Multiple R-squared: 0.4315, Adjusted R-squared: 0.4309 F-statistic: 757.5 on 1 and 998 DF, p-value: &lt; 2.2e-16 - 2 * logLik(modlm) &#39;log Lik.&#39; 4628.273 (df=3) As before, our estimates and final log likelihood value are about where they should be, and reflect the lm output, as the OLS estimates are the maximum likelihood estimates. The visualization becomes more difficult beyond two parameters, but we can examine slices similar to the previous plot. To move to generalized linear models, very little changes of the process outside of the distribution assumed and that we are typically modeling a function of the target variable (e.g. \\(\\log(y)=X\\beta; \\mu = e^{X\\beta}\\)). Source Original code available at: https://m-clark.github.io/bayesian-basics/appendix.html#maximum-likelihood-review "],["penalized-maximum-likelihood.html", "Penalized Maximum Likelihood Introduction L1 (lasso) regularization L2 (ridge) regularization", " Penalized Maximum Likelihood Introduction This demonstration regards a standard regression model via penalized likelihood. See the Maximum Likelihood chapter for a starting point. Here the penalty is specified (via lambda argument), but one would typically estimate the model via cross-validation or some other fashion. Two penalties are possible with the function. One using the (squared) L2 norm (aka ridge regression, Tikhonov regularization), another using the L1 norm (aka lasso) which has the possibility of penalizing coefficients to zero, and thus can serve as a model selection procedure. I have more technical approaches to the lasso and ridge in the lasso and ridge sections. Note that both L2 and L1 approaches can be seen as maximum a posteriori (MAP) estimates for a Bayesian regression with a specific prior on the coefficients. The L2 approach is akin to a normal prior with zero mean, while L1 is akin to a zero mean Laplace prior. See the Bayesian regression chapter for an approach in that regard. Data Setup library(tidyverse) set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X,y) Functions A straightforward function to estimate the log likelihood. penalized_ML &lt;- function( par, X, y, lambda = .1, type = &#39;L2&#39; ) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = T) # log likelihood switch( type, &#39;L2&#39; = -sum(L) + lambda * crossprod(beta[-1]), # the intercept is not penalized &#39;L1&#39; = -sum(L) + lambda * sum(abs(beta[-1])) ) } This next function is a glmnet style approach that will put the lambda coefficient on equivalent scale. It uses a different objective function. Note that glmnet is actually using elasticnet, which mixes both L1 and L2 penalties. penalized_ML2 &lt;- function( par, X, y, lambda = .1, type = &#39;L2&#39; ) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense switch( type, &#39;L2&#39; = .5 * crossprod(y - X %*% beta) / N + lambda * crossprod(beta[-1]), &#39;L1&#39; = .5 * crossprod(y - X %*% beta) / N + lambda * sum(abs(beta[-1])) ) } Estimation Setup the model matrix for use with optim. X = cbind(1, X) We’ll need to set initial values. Note we’d normally want to handle the sigma parameter differently as it’s bounded by zero, but we’ll ignore for demonstration. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) fit_l2 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, control = list(reltol = 1e-12) ) fit_l1 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, type = &#39;L1&#39;, control = list(reltol = 1e-12) ) Comparison Compare to lm in base R. fit_lm = lm(y ~ ., dfXy) sigma2 intercept b1 b2 fit_l2 0.219 -0.432 0.133 0.111 fit_l1 0.219 -0.432 0.131 0.109 fit_lm 0.226 -0.432 0.133 0.112 Compare to glmnet. Setting alpha to 0 and 1 is equivalent to L2 and L1 penalties respectively. You also wouldn’t want to specify lambda normally, and rather let it come about as part of the estimation procedure. We do so here just for demonstration. library(glmnet) glmnetL2 = glmnet( X[, -1], y, alpha = 0, lambda = .01, standardize = FALSE ) glmnetL1 = glmnet( X[, -1], y, alpha = 1, lambda = .01, standardize = FALSE ) pars_L2 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, control = list(reltol = 1e-12) )$par pars_L1 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, type = &#39;L1&#39;, control = list(reltol = 1e-12) )$par (Intercept) V1 V2 s0 -0.4324 0.1301 0.1094 pars_L2 -0.4324 0.1301 0.1094 s0 -0.4325 0.1207 0.1005 pars_L1 -0.4325 0.1207 0.1005 See the subsequent chapters for an additional look at both lasso and ridge regression approaches. Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/penalized_ML.R L1 (lasso) regularization See Tibshirani (1996) for the original paper, or Murphy PML (2012) for a nice overview (watch for typos in depictions). Data Setup library(tidyverse) set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N*p), ncol = p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p - 6)) y = scale(X %*% b + rnorm(N, sd = .5)) lambda = .1 Function Coordinate descent function. lasso &lt;- function( X, # model matrix y, # target lambda = .1, # penalty parameter soft = TRUE, # soft vs. hard thresholding tol = 1e-6, # tolerance iter = 100, # number of max iterations verbose = TRUE # print out iteration number ) { # soft thresholding function soft_thresh &lt;- function(a, b) { out = rep(0, length(a)) out[a &gt; b] = a[a &gt; b] - b out[a &lt; -b] = a[a &lt; -b] + b out } w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y) tol_curr = 1 J = ncol(X) a = rep(0, J) c_ = rep(0, J) i = 1 while (tol &lt; tol_curr &amp;&amp; i &lt; iter) { w_old = w a = colSums(X^2) l = length(y)*lambda # for consistency with glmnet approach c_ = sapply(1:J, function(j) sum( X[,j] * (y - X[,-j] %*% w_old[-j]) )) if (soft) { for (j in 1:J) { w[j] = soft_thresh(c_[j]/a[j], l/a[j]) } } else { w = w_old w[c_&lt; l &amp; c_ &gt; -l] = 0 } tol_curr = crossprod(w - w_old) i = i + 1 if (verbose &amp;&amp; i%%10 == 0) message(i) } w } Estimation Note, if lambda = 0, i.e. no penalty, the result is the same as what you would get from the base R lm.fit. fit_soft = lasso( X, y, lambda = lambda, tol = 1e-12, soft = TRUE ) fit_hard = lasso( X, y, lambda = lambda, tol = 1e-12, soft = FALSE ) The glmnet approach is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso (alpha = 0 would be ridge). We set the lambda to a couple values while only wanting the one set to the same lambda value as above (s). library(glmnet) fit_glmnet = coef( glmnet( X, y, alpha = 1, lambda = c(10, 1, lambda), thresh = 1e-12, intercept = FALSE ), s = lambda ) library(lassoshooting) fit_ls = lassoshooting( X = X, y = y, lambda = length(y) * lambda, thr = 1e-12 ) Comparison We can now compare the coefficients of all our results. lm lasso_soft lasso_hard lspack glmnet truth X1 0.535 0.435 0.535 0.435 0.436 0.500 X2 -0.530 -0.429 -0.530 -0.429 -0.429 -0.500 X3 0.234 0.124 0.234 0.124 0.124 0.250 X4 -0.294 -0.207 -0.294 -0.207 -0.208 -0.250 X5 0.126 0.020 0.126 0.020 0.020 0.125 X6 -0.159 -0.055 -0.159 -0.055 -0.055 -0.125 X7 -0.017 0.000 0.000 0.000 0.000 0.000 X8 0.010 0.000 0.000 0.000 0.000 0.000 X9 -0.005 0.000 0.000 0.000 0.000 0.000 X10 0.011 0.000 0.000 0.000 0.000 0.000 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/lasso.R L2 (ridge) regularization Compare to the lasso section. Data Setup library(tidyverse) set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N * p), ncol = p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4)) y = scale(X %*% b + rnorm(N, sd = .5)) Function ridge &lt;- function(w, X, y, lambda = .1) { # X: model matrix; # y: target; # lambda: penalty parameter; # w: the weights/coefficients crossprod(y - X %*% w) + lambda * length(y) * crossprod(w) } Estimation Note, if lambda = 0, i.e. no penalty, the result is the same as what you would get from the base R lm.fit. fit_ridge = optim( rep(0, ncol(X)), ridge, X = X, y = y, lambda = .1, method = &#39;BFGS&#39; ) Analytical result. fit_ridge2 = solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y) An alternative approach using ‘augmented’ data (note sigma is ignored as it equals 1, but otherwise X/sigma and y/sigma). X2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X))) y2 = c(y, rep(0, ncol(X))) tail(X2) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [505,] 0 0 0 0 7.071068 0.000000 0.000000 0.000000 0.000000 0.000000 [506,] 0 0 0 0 0.000000 7.071068 0.000000 0.000000 0.000000 0.000000 [507,] 0 0 0 0 0.000000 0.000000 7.071068 0.000000 0.000000 0.000000 [508,] 0 0 0 0 0.000000 0.000000 0.000000 7.071068 0.000000 0.000000 [509,] 0 0 0 0 0.000000 0.000000 0.000000 0.000000 7.071068 0.000000 [510,] 0 0 0 0 0.000000 0.000000 0.000000 0.000000 0.000000 7.071068 tail(y2) [1] 0 0 0 0 0 0 fit_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2) The glmnet approach is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso (alpha = 0 would be ridge). We set the lambda to a couple values while only wanting the one set to the same lambda value as above (s). library(glmnet) fit_glmnet = coef( glmnet( X, y, alpha = 0, lambda = c(10, 1, .1), thresh = 1e-12, intercept = F ), s = .1 ) Comparison We can now compare the coefficients of all our results. lm ridge ridge2 ridge3 glmnet truth X1 0.535 0.485 0.485 0.485 0.485 0.500 X2 -0.530 -0.481 -0.481 -0.481 -0.481 -0.500 X3 0.234 0.209 0.209 0.209 0.209 0.250 X4 -0.294 -0.269 -0.269 -0.269 -0.269 -0.250 X5 0.126 0.115 0.115 0.115 0.115 0.125 X6 -0.159 -0.146 -0.146 -0.146 -0.146 -0.125 X7 -0.017 -0.022 -0.022 -0.022 -0.022 0.000 X8 0.010 0.007 0.007 0.007 0.007 0.000 X9 -0.005 0.001 0.001 0.001 0.001 0.000 X10 0.011 0.011 0.011 0.011 0.011 0.000 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ridge.R "],["newton-irls.html", "Newton and IRLS Data Setup Functions Estimation Comparison Source", " Newton and IRLS Here we demonstrate Newton’s and Iterated Reweighted Least Squares approaches with a logistic regression model. For the following, I had Murphy’s PML text open and more or less followed the algorithms in chapter 8. Note that for Newton’s method, this doesn’t implement a line search to find a more optimal stepsize at a given iteration. Data Setup Predict graduate school admission based on gre, gpa, and school rank (higher is more prestige). See corresponding demo here. The only difference is that I treat rank as numeric rather than categorical. We will be comparing results to base R glm function, so I will use it to create the model data. library(tidyverse) admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) fit_glm = glm(admit ~ gre + gpa + rank, data = admit, family = binomial) # summary(fit_glm) X = model.matrix(fit_glm) y = fit_glm$y Functions Newton’s Method newton &lt;- function( X, y, tol = 1e-12, iter = 500, stepsize = .5 ) { # Args: # X: model matrix # y: target # tol: tolerance # iter: maximum number of iterations # stepsize: (0, 1) # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it +1 ll_old = ll mu = plogis(X %*% beta)[,1] g = crossprod(X, mu-y) # gradient S = diag(mu*(1-mu)) H = t(X) %*% S %*% X # hessian beta = beta - stepsize * solve(H) %*% g ll = sum(dbinom(y, prob = mu, size = 1, log = TRUE)) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll ) } IRLS Note that glm is actually using IRLS, so the results from this should be fairly spot on. irls &lt;- function(X, y, tol = 1e-12, iter = 500) { # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it + 1 ll_old = ll eta = X %*% beta mu = plogis(eta)[,1] s = mu * (1 - mu) S = diag(s) z = eta + (y - mu)/s beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z)) ll = sum( dbinom( y, prob = plogis(X %*% beta), size = 1, log = TRUE ) ) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll, weights = plogis(X %*% beta) * (1 - plogis(X %*% beta)) ) } Estimation fit_newton = newton( X = X, y = y, stepsize = .9, tol = 1e-8 # tol set to 1e-8 as in glm default ) fit_newton $beta [,1] (Intercept) -3.449548577 gre 0.002293959 gpa 0.777013649 rank -0.560031371 $iter [1] 8 $tol [1] 2.581544e-10 $loglik [1] -229.7209 # fit_glm tol set to 1e-8 as in glm default. irls_result = irls(X = X, y = y, tol = 1e-8) str(irls_result) List of 5 $ beta : num [1:4, 1] -3.44955 0.00229 0.77701 -0.56003 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;gre&quot; &quot;gpa&quot; &quot;rank&quot; .. ..$ : NULL $ iter : num 4 $ tol : num 6e-09 $ loglik : num -230 $ weights: num [1:400, 1] 0.1536 0.2168 0.2026 0.1268 0.0884 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:400] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ : NULL # fit_glm Comparison Compare all results. beta1 beta2 beta3 beta4 iter tol loglik newton -3.45 0.002 0.777 -0.56 8 0 -229.721 irls -3.45 0.002 0.777 -0.56 4 0 -229.721 glm_default -3.45 0.002 0.777 -0.56 4 NA -229.721 Compare weights between the irls and glm results. head(cbind(irls_result$weights, fit_glm$weights)) [,1] [,2] 1 0.15362250 0.15362250 2 0.21679615 0.21679615 3 0.20255723 0.20255724 4 0.12676333 0.12676334 5 0.08835918 0.08835918 6 0.23528108 0.23528108 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/newton_irls.R "],["nelder-mead.html", "Nelder-Mead Functions Second Version Source", " Nelder-Mead This is based on the pure Python implementation by François Chollet, also found in the supplemental section. This is mostly just an academic exercise on my part. I’m not sure how much one would use the basic NM for many situations. In my experience BFGS and other approaches would be faster, more accurate, and less sensitive to starting values for the types of problems I’ve played around with. Others who actually spend their time researching such things seem to agree. There were two issues regarding the original code on GitHub, and I’ve implemented the suggested corrections with notes. The initial R function code is not very R-like, as the goal was to keep more similar to the original Python for comparison, which used a list approach. I also provide a more R-like/cleaner version that uses matrices instead of lists, but which still sticks the same approach for the most part. For both functions, comparisons are made using the optimx package, but feel free to use base R’s optim instead. Functions First Version f function to optimize, must return a scalar score and operate over an array of the same dimensions as x_start x_start initial position step look-around radius in initial step no_improve_thr See no_improv_break no_improv_break break after no_improv_break iterations with an improvement lower than no_improv_thr max_iter always break after this number of iterations. Set it to 0 to loop indefinitely. alpha parameters of the algorithm (see Wikipedia page for reference) gamma parameters of the algorithm (see Wikipedia page for reference) rho parameters of the algorithm (see Wikipedia page for reference) sigma parameters of the algorithm (see Wikipedia page for reference) verbose Print iterations? This function returns the best parameter array and best score. nelder_mead &lt;- function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init dim = length(x_start) prev_best = f(x_start) no_improv = 0 res = list(list(x_start = x_start, prev_best = prev_best)) for (i in 1:dim) { x = x_start x[i] = x[i] + step score = f(x) res = append(res, list(list(x_start = x, prev_best = score))) } # simplex iter iters = 0 while (TRUE) { # order idx = sapply(res, `[[`, 2) res = res[order(idx)] # ascending order best = res[[1]][[2]] # break after max_iter if (max_iter &gt; 0 &amp; iters &gt;= max_iter) return(res[[1]]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[[1]]) # centroid x0 = rep(0, dim) for (tup in 1:(length(res)-1)) { for (i in 1:dim) { x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1) } } # reflection xr = x0 + alpha * (x0 - res[[length(res)]][[1]]) rscore = f(xr) if (res[[1]][[2]] &lt;= rscore &amp; rscore &lt; res[[length(res)-1]][[2]]) { res[[length(res)]] = list(xr, rscore) next } # expansion if (rscore &lt; res[[1]][[2]]) { # xe = x0 + gamma*(x0 - res[[length(res)]][[1]]) # issue with this xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[[length(res)]] = list(xe, escore) next } else { res[[length(res)]] = list(xr, rscore) next } } # contraction # xc = x0 + rho*(x0 - res[[length(res)]][[1]]) # issue with wiki consistency for rho values (and optim) xc = x0 + rho * (res[[length(res)]][[1]] - x0) cscore = f(xc) if (cscore &lt; res[[length(res)]][[2]]) { res[[length(res)]] = list(xc, cscore) next } # reduction x1 = res[[1]][[1]] nres = list() for (tup in res) { redx = x1 + sigma * (tup[[1]] - x1) score = f(redx) nres = append(nres, list(list(redx, score))) } res = nres } res } Example The function to minimize. f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } Estimate. nelder_mead( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) [[1]] [1] -1.570797e+00 -2.235577e-07 1.637460e-14 [[2]] [1] -1 Compare to optimx. You may see warnings. optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0 A Regression Model I find a regression model to be more applicable/intuitive for my needs, so provide an example for that case. Data Setup library(tidyverse) set.seed(8675309) N = 500 n_preds = 5 # number of predictors X = cbind(1, matrix(rnorm(N * n_preds), ncol = n_preds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } fit_nm = nelder_mead( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12, verbose = FALSE ) Comparison Compare to optimx. fit_nm_optimx = optimx::optimx( runif(ncol(X)), fn = f, # model function method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, #rho maxit = 2000, reltol = 1e-12 ) ) We’ll add base R lm estimates. p1 p2 p3 p4 p5 p6 value nm_func -0.962 0.594 0.049 0.276 0.975 -0.075 501.315 nm_optimx -0.962 0.594 0.049 0.276 0.975 -0.075 501.315 lm -0.962 0.594 0.049 0.276 0.975 -0.075 501.315 Second Version This is a more natural R approach in my opinion. nelder_mead2 = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init npar = length(x_start) nc = npar + 1 prev_best = f(x_start) no_improv = 0 res = matrix(c(x_start, prev_best), ncol = nc) colnames(res) = c(paste(&#39;par&#39;, 1:npar, sep = &#39;_&#39;), &#39;score&#39;) for (i in 1:npar) { x = x_start x[i] = x[i] + step score = f(x) res = rbind(res, c(x, score)) } # simplex iter iters = 0 while (TRUE) { # order res = res[order(res[, nc]), ] # ascending order best = res[1, nc] # break after max_iter if (max_iter &amp; iters &gt;= max_iter) return(res[1, ]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[1, ]) nr = nrow(res) # centroid: more efficient than previous double loop x0 = colMeans(res[(1:npar), -nc]) # reflection xr = x0 + alpha * (x0 - res[nr, -nc]) rscore = f(xr) if (res[1, &#39;score&#39;] &lt;= rscore &amp; rscore &lt; res[npar, &#39;score&#39;]) { res[nr,] = c(xr, rscore) next } # expansion if (rscore &lt; res[1, &#39;score&#39;]) { xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[nr, ] = c(xe, escore) next } else { res[nr, ] = c(xr, rscore) next } } # contraction xc = x0 + rho * (res[nr, -nc] - x0) cscore = f(xc) if (cscore &lt; res[nr, &#39;score&#39;]) { res[nr,] = c(xc, cscore) next } # reduction x1 = res[1, -nc] nres = res for (i in 1:nr) { redx = x1 + sigma * (res[i, -nc] - x1) score = f(redx) nres[i, ] = c(redx, score) } res = nres } } Example The function to minimize. f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } nelder_mead2( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) par_1 par_2 par_3 score -1.570797e+00 -2.235577e-07 1.622809e-14 -1.000000e+00 optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model set.seed(8675309) N = 500 n_preds = 5 X = cbind(1, matrix(rnorm(N * n_preds), ncol = n_preds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } fit_nm = nelder_mead2( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12 ) Comparison Compare to optimx and lm as before. fit_nm_optimx = optimx::optimx( runif(ncol(X)), fn = f, method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 2000, reltol = 1e-12 ) )[1:(n_preds + 1)] fit_lm = lm.fit(X, y)$coef nm_func nm_optimx lm truth p1 -0.962 -0.962 -0.962 -0.909 p2 0.594 0.594 0.594 0.620 p3 0.049 0.049 0.049 0.074 p4 0.276 0.276 0.276 0.320 p5 0.975 0.975 0.975 0.956 p6 -0.075 -0.075 -0.075 -0.080 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/nelder_mead.R "],["em.html", "Expectation-Maximization Mixture Model Multivariate Mixture Model Probit Model PCA Probabilistic PCA State Space Model", " Expectation-Maximization Mixture Model The following code is based on algorithms noted in Murphy, 2012 Probabilistic Machine Learning, specifically, Chapter 11, section 4. Data Setup This example uses Old Faithful geyser eruptions. This is only a univariate mixture for either eruption time or wait time. The next example will be doing both variables, i.e. multivariate normal. ‘Geyser’ is supposedly more accurate, though seems to have arbitrarily assigned some duration values. See this source also, but that only has intervals. Some July 1995 data is available. library(tidyverse) # faithful data set is in base R data(faithful) head(faithful) eruptions waiting 1 3.600 79 2 1.800 54 3 3.333 74 4 2.283 62 5 4.533 85 6 2.883 55 eruptions = as.matrix(faithful[, 1, drop = FALSE]) wait_times = as.matrix(faithful[, 2, drop = FALSE]) Function The fitting function. em_mixture &lt;- function( params, X, clusters = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are starting parameters (means, covariances, cluster probability), # data, number of clusters desired, tolerance, maximum iterations, and whether # to show iterations # Starting points N = nrow(X) nams = names(params) mu = params$mu var = params$var probs = params$probs # Other initializations # initialize cluster &#39;responsibilities&#39;, i.e. probability of cluster # membership for each observation i ri = matrix(0, ncol = clusters, nrow = N) it = 0 converged = FALSE if (showits) # Show iterations cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { probsOld = probs muOld = mu varOld = var riOld = ri # E # Compute responsibilities for (k in 1:clusters){ ri[, k] = probs[k] * dnorm(X, mu[k], sd = sqrt(var[k]), log = FALSE) } ri = ri/rowSums(ri) # M rk = colSums(ri) # rk is the weighted average cluster membership size probs = rk/N mu = (t(X) %*% ri) / rk var = (t(X^2) %*% ri) / rk - mu^2 # could do mu and var via log likelihood here, but this is more straightforward parmlistold = rbind(probsOld, muOld, varOld) parmlistcurrent = rbind(probs, mu, var) it = it + 1 # if showits true, &amp; it =1 or divisible by 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(parmlistold - parmlistcurrent)) &lt;= tol } clust = which(round(ri) == 1, arr.ind = TRUE) # create cluster membership clust = clust[order(clust[, 1]), 2] # order according to row rather than cluster out = list( probs = probs, mu = mu, var = var, resp = ri, cluster = clust ) out } Estimation Starting parameters require mean, variance and class probability. Note that starting values for mean must be within the data range or it will break. start_values_1 = list(mu = c(2, 5), var = c(1, 1), probs = c(.5, .5)) start_values_2 = list(mu = c(50, 90), var = c(1, 15), probs = c(.5, .5)) mix_erupt = em_mixture(start_values_1, X = eruptions, tol = 1e-8) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... mix_waiting = em_mixture(start_values_2, X = wait_times, tol = 1e-8) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... Comparison Compare to flexmix package results. library(flexmix) flex_erupt = flexmix(eruptions ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) flex_wait = flexmix(wait_times ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) The following provides means, variances and probability of group membership. Note that the cluster label is arbitrary so cluster 1 for one model may be cluster 2 in another. Eruptions First we’ll compare results on the eruptions variable. mean_var = rbind(mix_erupt$mu, sqrt(mix_erupt$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var) = c(&#39;cluster 1&#39;, &#39;cluster 2&#39;) mean_var_flex = parameters(flex_erupt) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var_flex) = c(&#39;cluster 1 flex&#39;, &#39;cluster 2 flex&#39;) prob_membership = mix_erupt$probs prob_membership_flex = flex_erupt@size / sum(flex_erupt@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) $params cluster 1 cluster 2 cluster 1 flex cluster 2 flex means 2.0186078 4.2733434 2.0186378 4.2733671 variances 0.2356218 0.4370631 0.2361087 0.4378364 $clusterpobs prob_membership prob_membership_flex 1 0.3484046 0.3492647 2 0.6515954 0.6507353 Waiting Now we compare the result for waiting times. mean_var = rbind(mix_waiting$mu, sqrt(mix_waiting$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var) = c(&#39;cluster 1&#39;, &#39;cluster 2&#39;) mean_var_flex = parameters(flex_wait) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var_flex) = c(&#39;cluster 1 flex&#39;, &#39;cluster 2 flex&#39;) prob_membership = mix_waiting$probs prob_membership_flex = flex_wait@size / sum(flex_wait@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) $params cluster 1 cluster 2 cluster 1 flex cluster 2 flex means 54.614856 80.091069 54.616631 80.090988 variances 5.871219 5.867734 5.884843 5.879324 $clusterpobs prob_membership prob_membership_flex 1 0.3608861 0.3639706 2 0.6391139 0.6360294 Visualization Points are colored by class membership, followed by the probability of being in cluster 1. Supplemental Example This uses the MASS version (reversed columns). These don’t look even remotely the same data on initial inspection- geyser is even more rounded and of opposite conclusion. Turns out geyser is offset by 1, such that duration 1 should be coupled with waiting 2 and on down. Still the rounding at 2 and 4 (and whatever division was done on duration) makes this fairly poor data. I’ve cleaned this up a little bit in case someone wants to play with it for additional practice, but it’s not evaluated. data(geyser, package = &#39;MASS&#39;) geyser = data.frame(duration = geyser$duration[-299], waiting = geyser$waiting[-1]) # compare to faithful library(patchwork) qplot(data = faithful, x = eruptions, y = waiting, alpha = I(.25)) / qplot(data = geyser, x = duration, y = waiting, alpha = I(.25)) X3 = matrix(geyser[,1]) X4 = matrix(geyser[,2]) # MASS version test3 = em_mixture(start_values_1, X = X3, tol = 1e-8) test4 = em_mixture(start_values_2, X = X4, tol = 1e-8) flexmod3 = flexmix(X3 ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) flexmod4 = flexmix(X4 ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) # note variability differences compared to faithful dataset # Eruptions/Duration mean_var = rbind(test3$mu, sqrt(test3$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) mean_var_flex = parameters(flexmod3) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) prob_membership = test3$probs prob_membership_flex = flexmod3@size / sum(flexmod3@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) # Waiting mean_var = rbind(test4$mu, sqrt(test4$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) mean_var_flex = parameters(flexmod4) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) prob_membership = test4$probs prob_membership_flex = flexmod4@size / sum(flexmod4@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) # Some plots library(ggplot2) qplot(x = eruptions, y = waiting, data = faithful) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_waiting$cluster))) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = mix_waiting$resp[, 1])) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20Mixture.R Multivariate Mixture Model The following code is based on algorithms noted in Murphy, 2012 Probabilistic Machine Learning. Specifically, Chapter 11, section 4. Function This estimating function will be used for both examples. em_mixture &lt;- function( params, X, clusters = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are # params: starting parameters (means, covariances, cluster probability) # X: data # clusters: number of clusters desired # tol: tolerance # maxits: maximum iterations # showits: whether to show iterations require(mvtnorm) # Starting points N = nrow(X) mu = params$mu var = params$var probs = params$probs # initializations # cluster &#39;responsibilities&#39;, i.e. probability of cluster membership for each # observation i ri = matrix(0, ncol=clusters, nrow=N) ll = 0 # log likelihood it = 0 # iteration count converged = FALSE # convergence # Show iterations if showits == true if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while (!converged &amp; it &lt; maxits) { probsOld = probs # muOld = mu # Use direct values or loglike for convergence check # varOld = var llOld = ll riOld = ri ### E # Compute responsibilities for (k in 1:clusters){ ri[,k] = probs[k] * dmvnorm(X, mu[k, ], sigma = var[[k]], log = FALSE) } ri = ri/rowSums(ri) ### M rk = colSums(ri) # rk is weighted average cluster membership size probs = rk/N for (k in 1:clusters){ varmat = matrix(0, ncol = ncol(X), nrow = ncol(X)) # initialize to sum matrices for (i in 1:N){ varmat = varmat + ri[i,k] * X[i,]%*%t(X[i,]) } mu[k,] = (t(X) %*% ri[,k]) / rk[k] var[[k]] = varmat/rk[k] - mu[k,]%*%t(mu[k,]) ll[k] = -.5*sum( ri[,k] * dmvnorm(X, mu[k,], sigma = var[[k]], log = TRUE) ) } ll = sum(ll) # compare old to current for convergence parmlistold = c(llOld, probsOld) # c(muOld, unlist(varOld), probsOld) parmlistcurrent = c(ll, probs) # c(mu, unlist(var), probs) it = it + 1 # if showits true, &amp; it =1 or modulo of 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = min(abs(parmlistold - parmlistcurrent)) &lt;= tol } clust = which(round(ri) == 1, arr.ind = TRUE) # create cluster membership clust = clust[order(clust[,1]), 2] # order accoring to row rather than cluster list( probs = probs, mu = mu, var = var, resp = ri, cluster = clust, ll = ll ) } Example 1: Old Faithful This example uses Old Faithful geyser eruptions as before. This is can be compared to the univariate code from the other chapter. See also http://www.geyserstudy.org/geyser.aspx?pGeyserNo=OLDFAITHFUL Data Setup library(tidyverse) data(&quot;faithful&quot;) Estimation Create starting values and estimate. mustart = rbind(c(3, 60), c(3, 60.1)) # must be at least slightly different covstart = list(cov(faithful), cov(faithful)) probs = c(.01, .99) # params is a list of mu, var, and probs starts = list(mu = mustart, var = covstart, probs = probs) mix_faithful = em_mixture( params = starts, X = as.matrix(faithful), clusters = 2, tol = 1e-12, maxits = 1500, showits = TRUE ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... 60... 65... 70... 75... 80... str(mix_faithful) List of 6 $ probs : num [1:2] 0.356 0.644 $ mu : num [1:2, 1:2] 2.04 4.29 54.48 79.97 $ var :List of 2 ..$ : num [1:2, 1:2] 0.0692 0.4352 0.4352 33.6973 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : NULL .. .. ..$ : chr [1:2] &quot;eruptions&quot; &quot;waiting&quot; ..$ : num [1:2, 1:2] 0.17 0.941 0.941 36.046 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : NULL .. .. ..$ : chr [1:2] &quot;eruptions&quot; &quot;waiting&quot; $ resp : num [1:272, 1:2] 2.59e-09 1.00 8.42e-06 1.00 1.00e-21 ... $ cluster: int [1:272] 2 1 2 1 2 1 2 2 1 2 ... $ ll : num 477 Visualize. Comparison Compare to mclust results. Options are set to be more similar to the settings demonstrated. library(mclust) mix_mclust = mclust::Mclust( faithful[, 1:2], 2, modelNames = &#39;VVV&#39;, control = emControl(tol = 1e-12) ) detach(package:mclust) # str(mix_mclust, 1) Compare means. t(mix_faithful$mu) [,1] [,2] [1,] 2.036388 4.289662 [2,] 54.478516 79.968115 mix_mclust$parameters$mean [,1] [,2] eruptions 4.289662 2.036388 waiting 79.968115 54.478517 Compare variances. mix_faithful$var [[1]] eruptions waiting [1,] 0.06916767 0.4351676 [2,] 0.43516762 33.6972821 [[2]] eruptions waiting [1,] 0.1699684 0.9406093 [2,] 0.9406093 36.0462113 mix_mclust$parameters$variance$sigma , , 1 eruptions waiting eruptions 0.1699684 0.9406089 waiting 0.9406089 36.0462071 , , 2 eruptions waiting eruptions 0.06916769 0.4351678 waiting 0.43516784 33.6972835 Compare classifications. Reverse in case arbitrary labeling of one of the clusters is opposite. table(mix_faithful$cluster, mix_mclust$classification) 1 2 1 0 97 2 175 0 table(ifelse(mix_faithful$cluster == 2, 1, 2), mix_mclust$classification) 1 2 1 175 0 2 0 97 # compare responsibilities; reverse one if arbitrary numbering of one of them is opposite # cbind(round(mix_faithful$resp[,1], 2), round(mix_mclust$z[,2], 2)) # cluster &#39;1&#39; # cbind(round(mix_faithful$resp[,2], 2), round(mix_mclust$z[,1], 2)) # cluster &#39;2&#39; Example 2: Iris Data Setup Set up the data. iris2 = iris %&gt;% select(-Species) Estimation Run and examine. We add noise to our starting value, and the function is notably sensitive to starts, but we don’t want to cheat too badly. mustart = iris %&gt;% group_by(Species) %&gt;% summarise(across(.fns = function(x) mean(x) + runif(1, 0, .5))) %&gt;% select(-Species) %&gt;% as.matrix() # use purrr::map due to mclust::map masking covstart = iris %&gt;% split(.$Species) %&gt;% purrr::map(select, -Species) %&gt;% purrr::map(function(x) cov(x) + diag(runif(4, 0, .5))) probs = c(.1, .2, .7) starts = list(mu = mustart, var = covstart, probs = probs) mix_mclust_iris = em_mixture( params = starts, X = as.matrix(iris2), clusters = 3, tol = 1e-8, maxits = 1500, showits = T ) Iterations of EM: 1... 5... table(mix_mclust_iris$cluster, iris$Species) setosa versicolor virginica 1 50 0 0 2 0 50 7 3 0 0 43 Comparison Compare to mclust results. library(mclust) mclust_iris = mclust::Mclust(iris[,1:4], 3) table(mclust_iris$classification, iris$Species) setosa versicolor virginica 1 50 0 0 2 0 45 0 3 0 5 50 detach(package:mclust) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20Mixture%20MV.R Probit Model The following regards models for a binary response. See Murphy, 2012 Probabilistic Machine Learning Chapter 11.4. Data Setup library(tidyverse) admission = haven::read_dta(&quot;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&quot;) Probit via Maximum Likelihood Function We’ll start with the a basic maximum likelihood function for a standard probit. See the logistic regression and previous chapter on probit models for comparison. probit_mle &lt;- function(params, X, y){ # Arguments are starting parameters (coefficients), model matrix, response b = params mu = X %*% b # linear predictor # compute the log likelihood either way # ll = sum(y * pnorm(mu, log.p = TRUE) + (1 - y) * pnorm(-mu, log.p = TRUE)) ll = sum(dbinom(y, 1, prob = pnorm(mu), log = TRUE)) -ll } Estimation Estimate with optim. # input data X = as.matrix(cbind(1, admission[, 2:4])) y = as.matrix(admission[, 1]) init = c(0, 0, 0, 0) # Can set tolerance really low to duplicate glm result fit_mle = optim( par = init, fn = probit_mle, X = X, y = y, control = list(maxit = 1000, reltol = 1e-12) ) # extract coefficients coefs_mle = fit_mle$par Comparison Compare to glm. fit_glm = glm( admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), control = list(maxit = 500, epsilon = 1e-8), data = admission ) summary(fit_glm) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), data = admission, control = list(maxit = 500, epsilon = 1e-08)) Deviance Residuals: Min 1Q Median 3Q Max -1.5626 -0.8920 -0.6403 1.1631 2.2097 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.0915039 0.6718360 -3.113 0.00185 ** gre 0.0013982 0.0006487 2.156 0.03112 * gpa 0.4643599 0.1950263 2.381 0.01727 * rank -0.3317117 0.0745524 -4.449 8.61e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.48 on 396 degrees of freedom AIC: 467.48 Number of Fisher Scoring iterations: 4 coefs_glm = coef(fit_glm) Compare. (Intercept) gre gpa rank coefs_mle -2.092 0.001 0.464 -0.332 coefs_glm -2.092 0.001 0.464 -0.332 EM for Latent Variable Approach to Probit Now for the EM approach, which assumes an continuous latent variable underlying the binary target. Function em_probit &lt;- function( params, X, y, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # params: starting parameters (coefficients) # X: model matrix # y: response # tol: tolerance, # maxits: maximum iterations # showits: whether to show iterations #starting points b = params mu = X%*%b it = 0 converged = FALSE z = rnorm(length(y)) # z is the latent variable ~N(0,1) # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) # while no convergence and we haven&#39;t reached our max iterations do this stuff while ((!converged) &amp; (it &lt; maxits)) { z_old = z # create &#39;old&#39; values for comparison # E step create a new z based on current values z = ifelse( y == 1, mu + dnorm(mu) / pnorm(mu), mu - dnorm(mu) / pnorm(-mu) ) # M step estimate b b = solve(t(X)%*%X) %*% t(X)%*%z mu = X%*%b ll = sum(y * pnorm(mu, log.p = TRUE) + (1 - y) * pnorm(-mu, log.p = TRUE)) it = it + 1 if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(z_old - z)) &lt;= tol } # Show last iteration if (showits) cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list(b = t(b), ll = ll) } Estimation Use the same setup and starting values to estimate the parameters. # can lower tolerance to duplicate glm result fit_em = em_probit( params = init, X = X, y = y, tol = 1e-12, maxit = 100 ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 51... # fit_em coefs_em = fit_em$b Comparison Compare all results. fit (Intercept) gre gpa rank logLik glm -2.092 0.001 0.464 -0.332 -229.74 mle -2.092 0.001 0.464 -0.332 229.74 em -2.092 0.001 0.464 -0.332 -229.74 Visualize Show estimates over niter iterations and visualize. X2 = X X2[, 2:3] = scale(X2[, 2:3]) niter = 20 fit_em = map_df(1:niter, function(x) as_tibble( em_probit( params = init, X = X2, y = y, tol = 1e-8, maxit = x, showits = F )$b) ) gdat = fit_em %&gt;% rowid_to_column(&#39;iter&#39;) %&gt;% pivot_longer(-iter, names_to = &#39;coef&#39;) %&gt;% mutate( coef = factor(coef, labels = c(&#39;Intercept&#39;, &#39;gre&#39;, &#39;gpa&#39;, &#39;rank&#39;)) ) %&gt;% arrange(iter, coef) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20probit%20example.R PCA The following is an EM algorithm for principal components analysis. See Murphy, 2012 Probabilistic Machine Learning 12.2.5. Some of the constructed object is based on output from pca function used below. Data Setup The state.x77 is from base R, which includes various state demographics. We will first standardize the data. library(tidyverse) X = scale(state.x77) Function The estimating function. Note that it uses orth from pracma, but I show the core of the underlying code if you don’t want to install it. # orth &lt;- function(M) { # svdM = svd(M) # U = svdM$u # s = svdM$d # tol = max(dim(M)) * max(s) * .Machine$double.eps # r = sum(s &gt; tol) # # U[, 1:r, drop = FALSE] # } em_pca &lt;- function( X, n_comp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # n_comp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # starting points and other initializations N = nrow(X) D = ncol(X) L = n_comp Xt = t(X) Z = t(replicate(L, rnorm(N))) # latent variables W = replicate(L, rnorm(D)) # loadings it = 0 converged = FALSE if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) # while no convergence and we haven&#39;t reached our max iterations do this stuff while ((!converged) &amp; (it &lt; maxits)) { Z_old = Z # create &#39;old&#39; values for comparison Z = solve(t(W)%*%W) %*% crossprod(W, Xt) # E W = Xt%*%t(Z) %*% solve(tcrossprod(Z)) # M it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(Z_old-Z)) &lt;= tol } # calculate reconstruction error Xrecon_em = W %*% Z reconerr = sum((Xrecon_em - t(X))^2) # orthogonalize W = pracma::orth(W) # for orthonormal basis of W; pcaMethods package has also evs = eigen(cov(X %*% W)) evals = evs$values evecs = evs$vectors W = W %*% evecs Z = X %*% W if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, reconerr = reconerr, Xrecon_em = t(Xrecon_em) ) } Estimation fit_em = em_pca( X = X, n_comp = 2, tol = 1e-12, maxit = 1000 ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... 60... 65... 70... 74... str(fit_em) # examine results List of 4 $ scores : num [1:50, 1:2] 3.79 -1.053 0.867 2.382 0.241 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings : num [1:8, 1:2] 0.126 -0.299 0.468 -0.412 0.444 ... $ reconerr : num 136 $ Xrecon_em: num [1:50, 1:8] 0.383 2.109 0.416 -0.228 1.472 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : chr [1:8] &quot;Population&quot; &quot;Income&quot; &quot;Illiteracy&quot; &quot;Life Exp&quot; ... Comparison Extract reconstructed values and loadings for comparison. Xrecon_em = fit_em$Xrecon_em loadings_em = fit_em$loadings scores_em = fit_em$scores Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different in sign, but otherwise should be comparable. library(pcaMethods) # install via BiocManager::install(&quot;pcaMethods&quot;) fit_pcam = pca( X, nPcs = 2, method = &#39;svd&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(fit_pcam) scores_pcam = scores(fit_pcam) Compare loadings and scores. sum((abs(loadings_pcam) - abs(loadings_em))^2) [1] 2.166071e-24 cbind(scores_pcam, data.frame(EM = scores_em)) %&gt;% head() PC1 PC2 EM.1 EM.2 Alabama 3.7898873 -0.2347790 3.7898873 -0.2347790 Alaska -1.0531355 5.4561751 -1.0531355 5.4561751 Arizona 0.8674288 0.7450615 0.8674288 0.7450615 Arkansas 2.3817776 -1.2883437 2.3817776 -1.2883437 California 0.2413815 3.5095228 0.2413815 3.5095228 Colorado -2.0621814 0.5056639 -2.0621814 0.5056639 Calculate mean squared reconstruction error and compare. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_em - X)^2) [1] 0.3392252 mean((Xrecon_pcam - X)^2) [1] 0.3392252 mean(abs(Xrecon_pcam - Xrecon_em)) [1] 6.109616e-13 Visualize # qplot(Xrecon_pcam[,1], X[,1]) # qplot(Xrecon_pcam[,2], X[,2]) qplot(Xrecon_em[,1], Xrecon_pcam[,1]) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20for%20pca.R Probabilistic PCA The following is an EM algorithm for probabilistic principal components analysis. Based on Tipping and Bishop, 1999, and also Murphy 2012 Probabilistic ML, with some code snippets inspired by the ppca function used below. See also standard PCA. Data Setup state.x77 is from base R, which includes various state demographics. We will first standardize the data. library(tidyverse) X = scale(state.x77) Function The estimating function. Note that it uses orth from pracma, but I show the core of the underlying code if you don’t want to install it. orth &lt;- function(M) { svdM = svd(M) U = svdM$u s = svdM$d tol = max(dim(M)) * max(s) * .Machine$double.eps r = sum(s &gt; tol) U[, 1:r, drop = FALSE] } em_ppca &lt;- function( X, n_comp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # n_comp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # require(pracma) tr &lt;- function(x) sum(diag(x), na.rm = TRUE) # matrix trace # starting points and other initializations N = nrow(X) D = ncol(X) L = n_comp S = (1/N) * t(X)%*%X evals = eigen(S)$values evecs = eigen(S)$vectors V = evecs[,1:L] Lambda = diag(evals[1:L]) # latent variables Z = t(replicate(L, rnorm(N))) # variance; average variance associated with discarded dimensions sigma2 = 1/(D - L) * sum(evals[(L+1):D]) # loadings; this and sigma2 starting points will be near final estimate W = V %*% chol(Lambda - sigma2 * diag(L)) %*% diag(L) it = 0 converged = FALSE ll = 0 # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { # create &#39;old&#39; values for comparison if(exists(&#39;W_new&#39;)){ W_old = W_new } else { W_old = W } ll_old = ll Psi = sigma2*diag(L) M = t(W_old) %*% W_old + Psi # E and M W_new = S %*% W_old %*% solve( Psi + solve(M) %*% t(W_old) %*% S %*% W_old ) sigma2 = 1/D * tr(S - S %*% W_old %*% solve(M) %*% t(W_new)) Z = solve(M) %*% t(W_new) %*% t(X) ZZ = sigma2*solve(M) + Z%*%t(Z) # log likelihood as in paper # ll = .5*sigma2*D + .5*tr(ZZ) + .5*sigma2 * X%*%t(X) - # 1/sigma2 * t(Z)%*%t(W_new)%*%t(X) + .5*sigma2 * tr(t(W_new)%*%W_new%*%ZZ) # ll = -sum(ll) # more straightforward ll = dnorm(X, mean = t(W_new %*% Z), sd = sqrt(sigma2), log = TRUE) ll = -sum(ll) it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(ll_old-ll)) &lt;= tol } W = pracma::orth(W_new) # for orthonormal basis of W; pcaMethods package has also evs = eigen(cov(X %*% W)) evecs = evs$vectors W = W %*% evecs Z = X %*% W Xrecon = Z %*% t(W) reconerr = sum((Xrecon - X)^2) if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, Xrecon = Xrecon, reconerr = reconerr, ll = ll, sigma2 = sigma2 ) } Estimation fit_em = em_ppca( X = X, n_comp = 2, tol = 1e-12, maxit = 100 ) Iterations of EM: 1... 2... str(fit_em) List of 6 $ scores : num [1:50, 1:2] 3.79 -1.053 0.867 2.382 0.241 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings: num [1:8, 1:2] 0.126 -0.299 0.468 -0.412 0.444 ... $ Xrecon : num [1:50, 1:8] 0.383 2.109 0.416 -0.228 1.472 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ reconerr: num 136 $ ll : num 369 $ sigma2 : num 0.452 Comparison Extract reconstructed values and loadings for comparison. Xrecon = fit_em$Xrecon loadings_em = fit_em$loadings scores_em = fit_em$scores Compare to standard pca on full data set if desired. standard_pca = princomp(scale(state.x77)) scores_standard_pca = standard_pca$scores[,1:2] loadings_standard_pca = standard_pca$loadings[,1:2] Xrecon_standard_pca = scores_standard_pca%*%t(loadings_standard_pca) Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) fit_pcam = pca( X, nPcs = 2, threshold = 1e-8, method = &#39;ppca&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(fit_pcam) scores_pcam = scores(fit_pcam) Compare loadings and scores. round(cbind(loadings_pcam, loadings_em, loadings_standard_pca), 3) PC1 PC2 Comp.1 Comp.2 Population 0.126 0.411 0.126 0.411 0.126 0.411 Income -0.299 0.519 -0.299 0.519 -0.299 0.519 Illiteracy 0.468 0.053 0.468 0.053 0.468 0.053 Life Exp -0.412 -0.082 -0.412 -0.082 -0.412 -0.082 Murder 0.444 0.307 0.444 0.307 0.444 0.307 HS Grad -0.425 0.299 -0.425 0.299 -0.425 0.299 Frost -0.357 -0.154 -0.357 -0.154 -0.357 -0.154 Area -0.033 0.588 -0.033 0.588 -0.033 0.588 sum((abs(loadings_pcam) - abs(loadings_em)) ^ 2) [1] 1.324404e-18 cbind(scores_pcam, data.frame(EM = scores_em)) %&gt;% head() PC1 PC2 EM.1 EM.2 Alabama 3.7898873 -0.2347790 3.7898873 -0.2347790 Alaska -1.0531355 5.4561751 -1.0531355 5.4561751 Arizona 0.8674288 0.7450615 0.8674288 0.7450615 Arkansas 2.3817776 -1.2883437 2.3817776 -1.2883437 California 0.2413815 3.5095228 0.2413815 3.5095228 Colorado -2.0621814 0.5056639 -2.0621814 0.5056639 Compare reconstructed data sets. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_pcam - X)^2) [1] 0.3392252 mean(abs(Xrecon_pcam - Xrecon)) [1] 3.905797e-10 mean(abs(Xrecon_pcam - Xrecon)) [1] 3.905797e-10 Visualize Show the reconstructed vs. observed data. Compare component scores. Missing Data Example A slightly revised approach can be taken in the case of missing values. Data Setup # create some missing values set.seed(123) X_miss = X NAindex = sample(length(X), 20) X_miss[NAindex] = NA Function This estimating function largely follows the previous em_ppca_miss &lt;- function( X, n_comp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # n_comp: number of components # tol: tolerance level # maxits: maximum iterations # showits: show iterations # require(pracma) # for orthonormal basis of W; pcaMethods package has also, see basic orth function tr &lt;- function(x) sum(diag(x), na.rm = TRUE) # matrix trace # starting points and other initializations X_orig = X X = X N = nrow(X_orig) D = ncol(X_orig) L = n_comp NAs = is.na(X_orig) X[NAs] = 0 S = (1/N) * t(X)%*%X evals = eigen(S)$values evecs = eigen(S)$vectors V = evecs[,1:L] Lambda = diag(evals[1:L]) # latent variables Z = t(replicate(L, rnorm(N))) # variance; average variance associated with discarded dimensions sigma2 = 1/(D-L) * sum(evals[(L+1):D]) # loadings W = V %*% chol(Lambda-sigma2*diag(L)) %*% diag(L) it = 0 converged = FALSE ll = 0 # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { if(exists(&#39;W_new&#39;)){ W_old = W_new } else { W_old = W } ll_old = ll # deal with missingness via projection proj = t(W_old%*%Z) X_new = X_orig X_new[NAs] = proj[NAs] X = X_new Psi = sigma2*diag(L) M = t(W_old) %*% W_old + Psi # E and M W_new = S %*% W_old %*% solve( Psi + solve(M)%*%t(W_old)%*%S%*%W_old ) sigma2 = 1/D * tr(S - S%*%W_old%*%solve(M)%*%t(W_new)) Z = solve(M)%*%t(W_new)%*%t(X) # log likelihood as in paper # ZZ = sigma2*solve(M) + Z%*%t(Z) # ll = .5*sigma2*D + .5*tr(ZZ) + .5*sigma2 * X%*%t(X) - # 1/sigma2 * t(Z)%*%t(W_new)%*%t(X) + .5*sigma2 * tr(t(W_new)%*%W_new%*%ZZ) # ll = -sum(ll) # more straightforward ll = dnorm(X, mean = t(W_new %*% Z), sd = sqrt(sigma2), log = TRUE) ll = -sum(ll) it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(ll_old-ll)) &lt;= tol } W = pracma::orth(W_new) # for orthonormal basis of W evs = eigen(cov(X %*% W)) evecs = evs$vectors W = W %*% evecs Z = X %*% W Xrecon = Z %*% t(W) reconerr = sum((Xrecon-X)^2) if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, Xrecon = Xrecon, reconerr = reconerr, ll = ll, sigma2 = sigma2 ) } Estimation Run the PCA. fit_em_miss = em_ppca_miss( X = X_miss, n_comp = 2, tol = 1e-8, maxit = 100 ) Iterations of EM: 1... 5... 10... 15... 19... str(fit_em_miss) List of 6 $ scores : num [1:50, 1:2] 3.79 -1.07 0.86 2.35 0.24 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings: num [1:8, 1:2] 0.133 -0.299 0.422 -0.431 0.464 ... $ Xrecon : num [1:50, 1:8] 0.414 1.998 0.448 -0.2 1.521 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ reconerr: num 130 $ ll : num 368 $ sigma2 : num 0.475 Comparison Extract reconstructed values and loadings for comparison. Xrecon = fit_em_miss$Xrecon loadings_em = fit_em_miss$loadings scores_em = fit_em_miss$scores Compare to standard pca on full data set if desired. standard_pca = prin_comp(scale(state.x77)) scores_standard_pca = standard_pca$scores[,1:2] loadings_standard_pca = standard_pca$loadings[,1:2] Xrecon_standard_pca = scores_standard_pca%*%t(loadings_standard_pca) Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) fit_pcam = pca( X_miss, nPcs = 2, threshold = 1e-8, method = &#39;ppca&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(fit_pcam) scores_pcam = scores(fit_pcam) Compare loadings and scores. round(cbind(loadings_pcam, loadings_em, loadings_standard_pca), 3) PC1 PC2 Comp.1 Comp.2 Population -0.128 -0.416 0.133 0.396 0.126 0.411 Income 0.305 -0.492 -0.299 0.537 -0.299 0.519 Illiteracy -0.434 -0.046 0.422 0.076 0.468 0.053 Life Exp 0.432 0.077 -0.431 -0.080 -0.412 -0.082 Murder -0.464 -0.284 0.464 0.290 0.444 0.307 HS Grad 0.424 -0.288 -0.433 0.318 -0.425 0.299 Frost 0.346 0.170 -0.353 -0.185 -0.357 -0.154 Area 0.041 -0.620 -0.037 0.569 -0.033 0.588 sum((abs(loadings_pcam) - abs(loadings_em)) ^ 2) [1] 0.00738241 cbind(scores_pcam, data.frame(EM = scores_em)) %&gt;% head() PC1 PC2 EM.1 EM.2 Alabama -3.7959034 0.2274074 3.7935243 -0.2316100 Alaska 1.0806631 -5.4908857 -1.0714535 5.4128832 Arizona -0.8672862 -0.7758984 0.8599622 0.8425754 Arkansas -2.3599676 1.2461202 2.3527582 -1.3002423 California -0.1899324 -4.0171723 0.2399266 3.7656674 Colorado 1.4732282 -0.4045470 -1.4689157 0.4135982 Compare reconstructed data sets. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_pcam[NAindex]-X[NAindex])^2) [1] 0.5047378 mean(abs(Xrecon_pcam - Xrecon)) [1] 0.02893291 Visualize Visualize as before Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20ppca.R Original code for the missing example found at (https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20ppca%20with%20missing.R State Space Model The following regards chapter 11 in Statistical Modeling and Computation, the first example for an unobserved components model. The data regards inflation based on the U.S. consumer price index (\\(\\textrm{infl} = 400*log(cpi_t/cpi_{t-1})\\)), from the second quarter of 1947 to the second quarter of 2011. You can acquire the data here or in Datasets repo. Just note that it has 2 mystery columns and one mystery row presumably supplied by Excel. You can also get the CPI data yourself at the Bureau of Labor Statistics in a frustrating fashion, or in a much easier fashion here. For the following I use n instead of t or T because those are transpose and TRUE in R. The model is basically y = τ + ϵ, with ϵ ~ N(0, σ2), and τ = τn-1 + υ_n with υ ~ N(0, ω2). Thus each y is associated with a latent variable that follows a random walk over time. ω2 serves as a smoothing parameter, which itself may be estimated but which is fixed in the following. See the text for more details. Data Setup library(tidyverse) d = read_csv( &#39;https://raw.githubusercontent.com/m-clark/Datasets/master/us%20cpi/USCPI.csv&#39;, col_names = FALSE ) inflation = as.matrix(d$X1) summary(inflation) V1 Min. :-9.557 1st Qu.: 1.843 Median : 3.248 Mean : 3.634 3rd Qu.: 4.819 Max. :15.931 Function EM function for a state space model. em_state_space &lt;- function( params, y, omega2_0, omega2, tol = .00001, maxits = 100, showits = FALSE ) { # Arguments # params: starting parameters (variance as &#39;sigma2&#39;), # y: data, # tol: tolerance, # omega2: latent variance (2_0) is a noisier starting variance # maxits: maximum iterations # showits: whether to show iterations # Not really needed here, but would be a good idea generally to take advantage # of sparse representation for large data # require(spam) # see usage below # Starting points n = length(y) sigma2 = params$sigma2 # Other initializations H = diag(n) for (i in 1:(ncol(H) - 1)) { H[i + 1, i] = -1 } Omega2 = spam::as.spam(diag(omega2, n)) Omega2[1, 1] = omega2_0 H = spam::as.spam(H) HinvOmega2H = t(H) %*% spam::chol2inv(spam::chol(Omega2)) %*% H # tau ~ N(0, HinvOmmega2H^-1) it = 0 converged = FALSE if (showits) # Show iterations cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { sigma2Old = sigma2[1] Sigma2invOld = diag(n)/sigma2Old K = HinvOmega2H + Sigma2invOld # E tau = solve(K, y/sigma2Old) # tau|y, sigma2_{n-1}, omega2 ~ N(0, K^-1) K_inv_tr = sum(1/eigen(K)$values) sigma2 = 1/n * (K_inv_tr + crossprod(y-tau)) # M converged = max(abs(sigma2 - sigma2Old)) &lt;= tol it = it + 1 # if showits true, &amp; it =1 or divisible by 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) } Kfinal = HinvOmega2H + diag(n) / sigma2[1] taufinal = solve(K, (y / sigma2[1])) list(sigma2 = sigma2, tau = taufinal) } Estimation ss_mod_1 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = 1^2 ) 5... 10... 15... 20... 25... 30... ss_mod_.5 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = .5^2 ) 5... 10... 15... 20... # more smooth ss_mod_.1 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = .1^2 ) 5... 10... ss_mod_1$sigma2 [,1] [1,] 2.765182 ss_mod_.5$sigma2 [,1] [1,] 4.404707 ss_mod_.1$sigma2 [,1] [1,] 7.489429 Visualization library(lubridate) series = ymd( paste0( rep(1947:2014, e = 4), &#39;-&#39;, c(&#39;01&#39;, &#39;04&#39;, &#39;07&#39;, &#39;10&#39;), &#39;-&#39;, &#39;01&#39;) ) The following corresponds to Fig. 11.1 in the text. We’ll also compare to generalized additive model via geom_smooth (greenish line). We can see the .1 model (light blue) is overly smooth. library(tidyverse) data.frame( series = series[1:length(inflation)], inflation = inflation, Mod_1 = ss_mod_1$tau, Mod_.5 = ss_mod_.5$tau, Mod_.1 = ss_mod_.1$tau ) %&gt;% ggplot(aes(x = series, y = inflation)) + geom_point(color = &#39;gray50&#39;, alpha = .25) + geom_line(aes(y = Mod_1), color = &#39;#ff5500&#39;) + geom_line(aes(y = Mod_.5), color = &#39;#9E1B34&#39;) + geom_line(aes(y = Mod_.1), color = &#39;#00aaff&#39;) + geom_smooth( formula = y ~ s(x, bs = &#39;gp&#39;, k = 50), se = FALSE, color = &#39;#1b9e86&#39;, method = &#39;gam&#39;, size = .5 ) + scale_x_date(date_breaks = &#39;10 years&#39;) + labs(x = &#39;&#39;) Source Original code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20for%20state%20space%20unobserved%20components.R "],["gradient-descent.html", "Gradient Descent Data Setup Function Estimation Comparison Source", " Gradient Descent The following demonstration regards Gradient descent for a standard linear regression model. Data Setup Create some basic data for standard regression. library(tidyverse) set.seed(8675309) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) # model matrix Function (Batch) Gradient Descent Algorithm. The function takes arguments starting points for the parameters to be estimated, a tolerance or maximum iteration value to provide a stopping point, stepsize (or starting stepsize for adaptive approach), whether to print out iterations, and whether to plot the loss over each iteration. gd &lt;- function( par, X, y, tolerance = 1e-3, maxit = 1000, stepsize = 1e-3, adapt = FALSE, verbose = TRUE, plotLoss = TRUE ) { # initialize beta = par; names(beta) = colnames(X) loss = crossprod(X %*% beta - y) tol = 1 iter = 1 while(tol &gt; tolerance &amp;&amp; iter &lt; maxit){ LP = X %*% beta grad = t(X) %*% (LP - y) betaCurrent = beta - stepsize * grad tol = max(abs(betaCurrent - beta)) beta = betaCurrent loss = append(loss, crossprod(LP - y)) iter = iter + 1 if (adapt) stepsize = ifelse( loss[iter] &lt; loss[iter - 1], stepsize * 1.2, stepsize * .8 ) if (verbose &amp;&amp; iter %% 10 == 0) message(paste(&#39;Iteration:&#39;, iter)) } if (plotLoss) plot(loss, type = &#39;l&#39;, bty = &#39;n&#39;) list( par = beta, loss = loss, RSE = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), iter = iter, fitted = LP ) } Estimation Set starting values. init = rep(0, 3) For any particular data you’d have to fiddle with the stepsize, which could be assessed via cross-validation, or alternatively one can use an adaptive approach, a simple one of which is implemented in this function. fit_gd = gd( init, X = X, y = y, tolerance = 1e-8, stepsize = 1e-4, adapt = TRUE ) str(fit_gd) List of 5 $ par : num [1:3, 1] 0.985 0.487 0.218 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ loss : num [1:70] 2315 2315 2075 1918 1760 ... $ RSE : num [1, 1] 1.03 $ iter : num 70 $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ... Comparison We can compare to standard linear regression. Intercept x1 x2 gd 0.985 0.487 0.218 lm 0.985 0.487 0.218 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gradient_descent.R "],["stochastic-gradient-descent.html", "Stochastic Gradient Descent Data Setup Function Estimation Comparison Visualize Estimates Data Set Shift SGD Variants Source", " Stochastic Gradient Descent Here we have ‘online’ learning via stochastic gradient descent. See the standard gradient descent chapter. In the following, we have basic data for standard regression, but in this ‘online’ learning case, we can assume each observation comes to us as a stream over time rather than as a single batch, and would continue coming in. Note that there are plenty of variations of this, and it can be applied in the batch case as well. Currently no stopping point is implemented in order to trace results over all data points/iterations. On revisiting this much later, I thought it useful to add that I believe this was motivated by the example in Murphy’s Probabilistic Machine Learning text. Data Setup Create some data for a standard linear regression. library(tidyverse) set.seed(1234) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) Function The estimating function using the adagrad approach. sgd &lt;- function( par, # parameter estimates X, # model matrix y, # target variable stepsize = 1, # the learning rate stepsize_tau = 0, # if &gt; 0, a check on the LR at early iterations average = FALSE # a variation of the approach ){ # initialize beta = par names(beta) = colnames(X) betamat = matrix(0, nrow(X), ncol = length(beta)) # Collect all estimates fits = NA # Collect fitted values at each point loss = NA # Collect loss at each point s = 0 # adagrad per parameter learning rate adjustment eps = 1e-8 # a smoothing term to avoid division by zero for (i in 1:nrow(X)) { Xi = X[i, , drop = FALSE] yi = y[i] LP = Xi %*% beta # matrix operations not necessary, grad = t(Xi) %*% (LP - yi) # but makes consistent with standard gd func s = s + grad^2 # adagrad approach # update beta = beta - stepsize/(stepsize_tau + sqrt(s + eps)) * grad if (average &amp; i &gt; 1) { beta = beta - 1/i * (betamat[i - 1, ] - beta) # a variation } betamat[i,] = beta fits[i] = LP loss[i] = (LP - yi)^2 grad_old = grad } LP = X %*% beta lastloss = crossprod(LP - y) list( par = beta, # final estimates par_chain = betamat, # estimates at each iteration RMSE = sqrt(sum(lastloss)/nrow(X)), fitted = LP ) } Estimation Set starting values. starting_values = rep(0, 3) For any particular data you might have to fiddle with the stepsize, perhaps choosing one based on cross-validation with old data. fit_sgd = sgd( starting_values, X = X, y = y, stepsize = .1, stepsize_tau = .5, average = FALSE ) str(fit_sgd) List of 4 $ par : num [1:3, 1] 1.024 0.537 0.148 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ par_chain: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ... $ RMSE : num 1.01 $ fitted : num [1:1000, 1] 0.198 1.218 1.379 -0.141 1.358 ... fit_sgd$par [,1] Intercept 1.0241049 x1 0.5368198 x2 0.1478470 Comparison We can compare to standard linear regression. # summary(lm(y ~ x1 + x2)) coef1 = coef(lm(y ~ x1 + x2)) Intercept x1 x2 fit_sgd 1.024 0.537 0.148 lm 1.030 0.518 0.163 Visualize Estimates Data Set Shift This data includes a shift of the previous data, where the data fundamentally changes at certain times. Data Setup We’ll add data with different underlying generating processes. set.seed(1234) n2 = 1000 x1.2 = rnorm(n2) x2.2 = rnorm(n2) y2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2) X2 = rbind(X, cbind(1, x1.2, x2.2)) coef2 = coef(lm(y2 ~ x1.2 + x2.2)) y2 = c(y, y2) n3 = 1000 x1.3 = rnorm(n3) x2.3 = rnorm(n3) y3 = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3) coef3 = coef(lm(y3 ~ x1.3 + x2.3)) X3 = rbind(X2, cbind(1, x1.3, x2.3)) y3 = c(y2, y3) Estimation We’ll use the same function as before. fit_sgd_shift = sgd( starting_values, X = X3, y = y3, stepsize = 1, stepsize_tau = 0, average = FALSE ) str(fit_sgd_shift) List of 4 $ par : num [1:3, 1] 0.821 -0.223 0.211 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ par_chain: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ... $ RMSE : num 1.57 $ fitted : num [1:3000, 1] 0.836 0.823 0.254 1.479 0.874 ... Comparison Compare with lm result for each data part. Intercept x1 x2 lm_part1 1.030 0.518 0.163 lm_part2 -0.970 0.268 -0.287 lm_part3 1.045 -0.236 0.242 sgd_part1 1.086 0.513 0.146 sgd_part2 -0.925 0.295 -0.294 sgd_part3 0.821 -0.223 0.211 Visualize Estimates Visualize estimates across iterations. SGD Variants The above uses the Adagrad approach for stochastic gradient descent, but there are many variations. A good resource can be found here, as well as this post covering more recent developments. We will compare the Adagrad, RMSprop, Adam, and Nadam approaches. Data Setup For this demo we’ll bump the sample size. I’ve also made the coefficients a little different. library(tidyverse) set.seed(1234) n = 10000 x1 = rnorm(n) x2 = rnorm(n) X = cbind(Intercept = 1, x1, x2) true = c(Intercept = 1, x1 = 1, x2 = -.75) y = X %*% true + rnorm(n) Function For this we’ll add a functional component to the primary function. We create a function factory update_ff that, based on the input will create an appropriate update step (update) for use each iteration. This is mostly is just a programming exercise, but might allow you to add additional components arguments or methods more easily. sgd &lt;- function( par, # parameter estimates X, # model matrix y, # target variable stepsize = 1e-2, # the learning rate; suggest 1e-3 for non-adagrad methods type = &#39;adagrad&#39;, # one of adagrad, rmsprop, adam or nadam average = FALSE, # a variation of the approach ... # arguments to pass to an updating function, e.g. gamma in rmsprop ){ # initialize beta = par names(beta) = colnames(X) betamat = matrix(0, nrow(X), ncol = length(beta)) # Collect all estimates v = rep(0, length(beta)) # gradient variance (sum of squares) m = rep(0, length(beta)) # average of gradients for n/adam eps = 1e-8 # a smoothing term to avoid division by zero grad_old = rep(0, length(beta)) update_ff &lt;- function(type, ...) { # if stepsize_tau &gt; 0, a check on the LR at early iterations adagrad &lt;- function(grad, stepsize_tau = 0) { v &lt;&lt;- v + grad^2 stepsize/(stepsize_tau + sqrt(v + eps)) * grad } rmsprop &lt;- function(grad, grad_old, gamma = .9) { v = gamma * grad_old^2 + (1 - gamma) * grad^2 stepsize / sqrt(v + eps) * grad } adam &lt;- function(grad, b1 = .9, b2 = .999) { m &lt;&lt;- b1 * m + (1 - b1) * grad v &lt;&lt;- b2 * v + (1 - b2) * grad^2 if (type == &#39;adam&#39;) # dividing v and m by 1 - b*^i is the &#39;bias correction&#39; stepsize/(sqrt(v / (1 - b2^i)) + eps) * (m / (1 - b1^i)) else # nadam stepsize/(sqrt(v / (1 - b2^i)) + eps) * (b1 * m + (1 - b1)/(1 - b1^i) * grad) } switch( type, adagrad = function(grad, ...) adagrad(grad, ...), rmsprop = function(grad, ...) rmsprop(grad, grad_old, ...), adam = function(grad, ...) adam(grad, ...), nadam = function(grad, ...) adam(grad, ...) ) } update = update_ff(type, ...) for (i in 1:nrow(X)) { Xi = X[i, , drop = FALSE] yi = y[i] LP = Xi %*% beta # matrix operations not necessary, grad = t(Xi) %*% (LP - yi) # but makes consistent with standard gd func # update beta = beta - update(grad, ...) if (average &amp; i &gt; 1) { beta = beta - 1/i * (betamat[i - 1, ] - beta) # a variation } betamat[i,] = beta grad_old = grad } LP = X %*% beta lastloss = crossprod(LP - y) list( par = beta, # final estimates par_chain = betamat, # estimates at each iteration RMSE = sqrt(sum(lastloss)/nrow(X)), fitted = LP ) } Estimation We’ll now use all four methods for estimation. starting_values = rep(0, ncol(X)) # starting_values = runif(3, min = -1) fit_adagrad = sgd( starting_values, X = X, y = y, stepsize = .1 # suggestion is .01 for many settings, but this works better here ) fit_rmsprop = sgd( starting_values, X = X, y = y, stepsize = 1e-3, type = &#39;rmsprop&#39; ) fit_adam = sgd( starting_values, X = X, y = y, stepsize = 1e-3, type = &#39;adam&#39; ) fit_nadam = sgd( starting_values, X = X, y = y, stepsize = 1e-3, type = &#39;nadam&#39; ) Comparison We’ll compare our results to standard linear regression and the true values. fit Intercept x1 x2 fit_adagrad 1.0439 0.9748 -0.7459 fit_rmsprop 1.0355 0.9802 -0.7160 fit_adam 1.0429 0.9733 -0.7458 fit_nadam 1.0430 0.9735 -0.7459 fit_lm 1.0042 0.9851 -0.7510 true 1.0000 1.0000 -0.7500 Visualize Estimates We can visualize the route of estimation for each technique. While Adagrad works well for this particular problem, in standard machine learning contexts with possibly millions of parameters, and possibly massive data, it would quickly get to a point where it is no longer updating (the denominator continues to grow). These other techniques are attempts to get around the limitations of Adagrad. Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/stochastic_gradient_descent.R "],["metropolis-hastings.html", "Metropolis Hastings Data Setup Functions Estimation Comparison Source", " Metropolis Hastings The following demonstrates a random walk Metropolis-Hastings algorithm using the data and model from prior sections of the document. I had several texts open while cobbling together this code (noted below), and some oriented towards the social sciences. Some parts of the code reflect information and code examples found therein, and follows Lynch’s code a bit more. References: Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Gill, Jeff. 2008. Bayesian Methods : A Social and Behavioral Sciences Approach. Second. Jackman, Simon. 2009. Bayesian Analysis for the Social Sciences. Lynch, Scott M. 2007. Introduction to Applied Bayesian Statistics and Estimation for Social Scientists. Data Setup Here we create some data based on a standard linear regression. library(tidyverse) # set seed for replicability set.seed(8675309) # create a N x k matrix of covariates N = 250 K = 3 covariates = replicate(K, rnorm(n = N)) colnames(covariates) = c(&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;) # create the model matrix with intercept X = cbind(Intercept = 1, covariates) # create a normally distributed variable that is a function of the covariates coefs = c(5, .2, -1.5, .9) sigma = 2 mu = X %*% coefs y = rnorm(N, mu, sigma) # same as # y = 5 + .2*X1 - 1.5*X2 + .9*X3 + rnorm(N, mean = 0, sd = 2) # Run lm for later comparison; but go ahead and examine now if desired fit_lm = lm(y ~ ., data = data.frame(X[, -1])) # summary(fit_lm) Functions The primary functions that we need to specify regard the posterior distribution, an update step for beta coefficients, and an update step for the variance estimate. We assume a normal distribution for the β coefficients, inverse gamma on σ2. log_posterior &lt;- function(x, y, b, s2) { # Args: X is the model matrix; y the target vector; b and s2 the parameters # to be estimated beta = b sigma = sqrt(s2) sigma2 = s2 mu = X %*% beta # priors are b0 ~ N(0, sd = 10), sigma2 ~ invGamma(.001, .001) priorbvarinv = diag(1/100, 4) prioralpha = priorbeta = .001 if (is.nan(sigma) | sigma&lt;=0) { # scale parameter must be positive return(-Inf) } # Note that you will not find the exact same presentation across texts and # other media for the log posterior in this conjugate setting. In the end # they are conceptually still (log) prior + (log) likelihood (See commented &#39;else&#39;) else { -.5*nrow(X)*log(sigma2) - (.5*(1/sigma2) * (crossprod(y-mu))) + -.5*ncol(X)*log(sigma2) - (.5*(1/sigma2) * (t(beta) %*% priorbvarinv %*% beta)) + -(prioralpha + 1)*log(sigma2) + log(sigma2) - priorbeta/sigma2 } # else { # ll = mvtnorm::dmvnorm(y, mean=mu, sigma=diag(sigma2, length(y)), log=T) # priorb = mvtnorm::dmvnorm(beta, mean=rep(0, length(beta)), sigma=diag(100, length(beta)), log=T) # priors2 = dgamma(1/sigma2, prioralpha, priorbeta, log=T) # logposterior = ll + priorb + priors2 # logposterior # } } Update functions. # update step for regression coefficients update_coef &lt;- function(i, x, y, b, s2) { # Args are the same as above but with additional i iterator argument. b[i, ] = MASS::mvrnorm(1, mu = b[i-1, ], Sigma = b_var_scale) # proposal/jumping distribution # Compare to past- does it increase the posterior probability? post_diff = log_posterior(x = x, y = y, b = b[i, ], s2 = s2[i-1]) - log_posterior(x = x, y = y, b = b[i-1, ], s2 = s2[i-1]) # Acceptance phase unidraw = runif(1) accept = unidraw &lt; min(exp(post_diff), 1) # accept if so if (accept) b[i,] else b[i-1,] } # update step for sigma2 update_s2 &lt;- function(i, x, y, b, s2) { s2_candidate = rnorm(1, s2[i-1], sd = sigma_scale) if (s2_candidate &lt; 0) { accept = FALSE } else { s2_diff = log_posterior(x = x, y = y, b = b[i, ], s2 = s2_candidate) - log_posterior(x = x, y = y, b = b[i, ], s2 = s2[i - 1]) unidraw = runif(1) accept = unidraw &lt; min(exp(s2_diff), 1) } ifelse(accept, s2_candidate, s2[i - 1]) } Estimation Now we can set things up for the MCMC chain. Aside from the typical MCMC setup and initializing the parameter matrices to hold the draws from the posterior, we also require scale parameters to use for the jumping/proposal distribution. While this code regards only one chain, though a simple loop or any number of other approaches would easily extend it to two or more. # Setup, starting values etc. nsim = 5000 warmup = 1000 thin = 10 b = matrix(0, nsim, ncol(X)) # initialize beta update matrix s2 = rep(1, nsim) # initialize sigma vector For the following, this c_ term comes from BDA3 12.2 and will produce an acceptance rate of .44 in 1 dimension and declining from there to about .23 in high dimensions. For the sigma_scale, the magic number comes from starting with a value of one and fiddling from there to get around .44. c_ = 2.4/sqrt(ncol(b)) b_var = vcov(fit_lm) b_var_scale = b_var * c_^2 sigma_scale = .9 We can now run and summarize the model with tools from the coda package. # Run for (i in 2:nsim) { b[i, ] = update_coef( i = i, y = y, x = X, b = b, s2 = s2 ) s2[i] = update_s2( i = i, y = y, x = X, b = b, s2 = s2 ) } # calculate acceptance rates b_acc_rate = mean(diff(b[(warmup+1):nsim,]) != 0) s2_acc_rate = mean(diff(s2[(warmup+1):nsim]) != 0) b_acc_rate s2_acc_rate # get final chain library(coda) b_mcmc = as.mcmc(b[seq(warmup + 1, nsim, by = thin),]) s2_mcmc = as.mcmc(s2[seq(warmup + 1, nsim, by = thin)]) # get summaries summary(b_mcmc) summary(s2_mcmc) b_acc_rate = mean(diff(b[(warmup+1):nsim,]) != 0) s2_acc_rate = mean(diff(s2[(warmup+1):nsim]) != 0) b_acc_rate s2_acc_rate 0.298 0.43 Summarize results. The following table is uses rstan’s monitor function to produce typical Stan output. parameter mean sd 2.5% 97.5% n_eff Rhat Bulk_ESS Tail_ESS beta.1 4.900 0.135 4.638 5.162 258 1.007 252 235 beta.2 0.083 0.136 -0.192 0.354 289 1.003 294 425 beta.3 -1.468 0.122 -1.706 -1.232 304 1.001 306 427 beta.4 0.831 0.120 0.598 1.061 354 1.000 359 518 sigmasq 4.091 0.377 3.431 4.880 843 1.001 832 964 Comparison We can compare to the lm result or rstanarm. fit_rstan = rstanarm::stan_glm(y ~ ., data = data.frame(X[, -1])) parameter fit lm rstanarm (Intercept) 4.900 4.898 4.898 X1 0.083 0.084 0.088 X2 -1.468 -1.469 -1.468 X3 0.831 0.820 0.821 sigma_sq 4.091 4.084 4.110 Source Original demo here: https://m-clark.github.io/bayesian-basics/appendix.html#metropolis-hastings-example "],["hamiltonian-monte-carlo.html", "Hamiltonian Monte Carlo Data Setup Functions Estimation Comparison Source", " Hamiltonian Monte Carlo The following demonstrates Hamiltonian Monte Carlo, the technique that Stan uses, and which is a different estimation approach than the Gibbs sampler in BUGS/JAGS. If you are interested in the details enough to be reading this, I highly recommend Betancourt’s conceptual introduction to HMC. This example is largely based on the code in the appendix of Gelman. Gelman et al. 2013. Bayesian Data Analysis. 3rd ed. Data Setup As with the Metropolis Hastings chapter, we use some data for a standard linear regression. library(tidyverse) # set seed for replicability set.seed(8675309) # create a N x k matrix of covariates N = 250 K = 3 covariates = replicate(K, rnorm(n = N)) colnames(covariates) = c(&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;) # create the model matrix with intercept X = cbind(Intercept = 1, covariates) # create a normally distributed variable that is a function of the covariates coefs = c(5, .2, -1.5, .9) sigma = 2 mu = X %*% coefs y = rnorm(N, mu, sigma) # same as # y = 5 + .2*X1 - 1.5*X2 + .9*X3 + rnorm(N, mean = 0, sd = 2) # Run lm for later comparison; but go ahead and examine now if desired fit_lm = lm(y ~ ., data = data.frame(X[, -1])) # summary(fit_lm) Functions First we start with the log posterior function. log_posterior &lt;- function(X, y, th) { # Args # X: the model matrix # y: the target vector # th: theta, the current parameter estimates beta = th[-length(th)] # reg coefs to be estimated sigma = th[length(th)] # sigma to be estimated sigma2 = sigma^2 mu = X %*% beta # priors are b0 ~ N(0, sd=10), sigma2 ~ invGamma(.001, .001) priorbvarinv = diag(1/100, 4) prioralpha = priorbeta = .001 if (is.nan(sigma) | sigma&lt;=0) { # scale parameter must be positive, so post return(-Inf) # density is zero if it jumps below zero } # log posterior in this conjugate setting. conceptually it&#39;s (log) prior + # (log) likelihood. (See commented &#39;else&#39; for alternative) else { -.5*nrow(X)*log(sigma2) - (.5*(1/sigma2) * (crossprod(y-mu))) + -.5*ncol(X)*log(sigma2) - (.5*(1/sigma2) * (t(beta) %*% priorbvarinv %*% beta)) + -(prioralpha + 1)*log(sigma2) + log(sigma2) - priorbeta/sigma2 } # else { # ll = mvtnorm::dmvnorm(y, mean=mu, sigma=diag(sigma2, length(y)), log=T) # priorb = mvtnorm::dmvnorm(beta, mean=rep(0, length(beta)), sigma=diag(100, length(beta)), log=T) # priors2 = dgamma(1/sigma2, prioralpha, priorbeta, log=T) # logposterior = ll + priorb + priors2 # logposterior # } } The following is the numerical gradient function as given in BDA3 p. 602. It has the same arguments as the log posterior function. gradient_theta &lt;- function(X, y, th) { d = length(th) e = .0001 diffs = numeric(d) for (k in 1:d) { th_hi = th th_lo = th th_hi[k] = th[k] + e th_lo[k] = th[k] - e diffs[k] = (log_posterior(X, y, th_hi) - log_posterior(X, y, th_lo)) / (2 * e) } diffs } The following is a function for a single HMC iteration. ϵ and L are drawn randomly at each iteration to explore other areas of the posterior (starting with epsilon0 and L0); The mass matrix M, expressed as a vector, is a bit of a magic number in this setting. It regards the mass of a particle whose position is represented by \\(\\theta\\), and momentum by \\(\\phi\\). See the sampling section of the Stan manual for more detail. hmc_iteration &lt;- function(X, y, th, epsilon, L, M) { # Args # epsilon: the stepsize # L: the number of leapfrog steps # M: a diagonal mass matrix # initialization M_inv = 1/M d = length(th) phi = rnorm(d, 0, sqrt(M)) th_old = th log_p_old = log_posterior(X, y, th) - .5*sum(M_inv * phi^2) phi = phi + .5 * epsilon * gradient_theta(X, y, th) for (l in 1:L) { th = th + epsilon*M_inv*phi phi = phi + ifelse(l == L, .5, 1) * epsilon * gradient_theta(X, y, th) } # here we get into standard MCMC stuff, jump or not based on a draw from a # proposal distribution phi = -phi log_p_star = log_posterior(X, y, th) - .5*sum(M_inv * phi^2) r = exp(log_p_star - log_p_old) if (is.nan(r)) r = 0 p_jump = min(r, 1) if (runif(1) &lt; p_jump) { th_new = th } else { th_new = th_old } # returns estimates and acceptance rate list(th = th_new, p_jump = p_jump) } Main HMC function. hmc_run &lt;- function(starts, iter, warmup, epsilon_0, L_0, M, X, y) { # # Args: # starts: starting values # iter: total number of simulations for each chain (note chain is based on the dimension of starts) # warmup: determines which of the initial iterations will be ignored for inference purposes # epsilon0: the baseline stepsize # L0: the baseline number of leapfrog steps # M: is the mass vector chains = nrow(starts) d = ncol(starts) sims = array(NA, c(iter, chains, d), dimnames = list(NULL, NULL, colnames(starts))) p_jump = matrix(NA, iter, chains) for (j in 1:chains) { th = starts[j,] for (t in 1:iter) { epsilon = runif(1, 0, 2*epsilon_0) L = ceiling(2*L_0*runif(1)) temp = hmc_iteration(X, y, th, epsilon, L, M) p_jump[t,j] = temp$p_jump sims[t,j,] = temp$th th = temp$th } } # acceptance rate acc = round(colMeans(p_jump[(warmup + 1):iter,]), 3) message(&#39;Avg acceptance probability for each chain: &#39;, paste0(acc[1],&#39;, &#39;,acc[2]), &#39;\\n&#39;) list(sims = sims, p_jump = p_jump) } Estimation With the primary functions in place, we set the starting values and choose other settings for for the HMC process. The coefficient starting values are based on random draws from a uniform distribution, while \\(\\sigma\\) is set to a value of one in each case. As in the other examples we’ll have 5000 total draws with warm-up set to 2500. I don’t have any thinning option here, but that could be added or simply done as part of the coda package preparation. # Starting values and mcmc settings parnames = c(paste0(&#39;beta[&#39;, 1:4, &#39;]&#39;), &#39;sigma&#39;) d = length(parnames) chains = 2 theta_start = t(replicate(chains, c(runif(d-1, -1, 1), 1))) colnames(theta_start) = parnames nsim = 1000 wu = 500 We can fiddle with these sampling parameters to get a desirable acceptance rate of around .80. The following work well with the data we have. stepsize = .08 nLeap = 10 vars = rep(1, 5) mass_vector = 1 / vars We are now ready to run the model. On my machine and with the above settings, it took a couple seconds. Once complete we can use the coda package if desired as we have done before. # Run the model fit_hmc = hmc_run( starts = theta_start, iter = nsim, warmup = wu, epsilon_0 = stepsize, L_0 = nLeap, M = mass_vector, X = X, y = y ) # str(fit_hmc, 1) Using coda, we can get some nice summary information. Results not shown. library(coda) theta = as.mcmc.list(list(as.mcmc(fit_hmc$sims[(wu+1):nsim, 1,]), as.mcmc(fit_hmc$sims[(wu+1):nsim, 2,]))) # summary(theta) fit_summary = summary(theta)$statistics[,&#39;Mean&#39;] beta_est = fit_summary[1:4] sigma_est = fit_summary[5] # log_posterior(X, y, fit_summary) Instead we can use rstan’s monitor function on fit_hmc$sims to produce typical Stan output. Table 1: log posterior = -301.716 parameter mean sd 2.5% 97.5% n_eff Rhat Bulk_ESS Tail_ESS beta[1] 4.898 0.128 4.645 5.139 505 1.006 521 305 beta[2] 0.077 0.133 -0.192 0.334 560 1.012 566 240 beta[3] -1.472 0.122 -1.710 -1.224 628 1.010 630 457 beta[4] 0.825 0.111 0.612 1.042 618 1.000 630 569 sigma 2.013 0.090 1.844 2.204 682 1.004 693 425 Comparison Our estimates look pretty good, and inspection of the diagnostics would show good mixing and convergence as well. At this point we can compare it to the Stan output. For the following, I use the same inverse gamma prior and tweaked the control options for a little bit more similarity, but that’s not necessary. data { // Data block int&lt;lower = 1&gt; N; // Sample size int&lt;lower = 1&gt; K; // Dimension of model matrix matrix [N, K] X; // Model Matrix vector[N] y; // Target variable } parameters { // Parameters block; declarations only vector[K] beta; // Coefficient vector real&lt;lower = 0&gt; sigma; // Error scale } model { // Model block vector[N] mu; mu = X * beta; // Creation of linear predictor // priors beta ~ normal(0, 10); sigma ~ inv_gamma(.001, .001); // changed to gamma a la code above // likelihood y ~ normal(mu, sigma); } Here are the results. Table 2: log posterior = -1270.794 term estimate std.error conf.low conf.high beta[1] 1.421 3.336 -1.761 5.095 beta[2] 1.075 0.818 -0.121 1.710 beta[3] 0.220 1.474 -1.667 1.480 beta[4] -0.094 0.786 -0.738 1.013 sigma 2.034 0.064 1.889 2.199 And finally, the standard least squares fit (Residual standard error = sigma). Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.898 0.1284 38.13 3.026e-105 X1 0.08408 0.1296 0.6488 0.5171 X2 -1.469 0.1261 -11.64 3.049e-25 X3 0.8196 0.1207 6.793 8.207e-11 Fitting linear model: y ~ . Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 250 2.021 0.4524 0.4458 Source Original demo here: https://m-clark.github.io/bayesian-basics/appendix.html#hamiltonian-monte-carlo-example "],["supplemental.html", "Supplemental Other Languages", " Supplemental Other Languages When doing some of these models and algorithms, I had some other code to work with in another language, or, at the time, just wanted to try it in that language. There is not a whole lot here, but it still may be useful to some. Refer to the corresponding chapter of R code for context. Python Demos Linear Regression Data Setup import numpy as np import pandas as pd from scipy.stats import norm from scipy.optimize import minimize np.random.seed(123) # ensures replication # predictors and response # increasing N will get estimated values closer to the known parameters N = 1000 # sample size k = 2 # number of desired predictors X = np.matrix(np.random.normal(size = N * k)).reshape(N, k) y = -.5 + .2*X[:, 0] + .1*X[:, 1] + np.random.normal(scale = .5, size = N).reshape(N, 1) dfXy = pd.DataFrame(np.column_stack([X, y]), columns = [&#39;X1&#39;, &#39;X2&#39;, &#39;y&#39;]) Functions A maximum likelihood approach. def lm_ml(par, X, y): # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par[1:].reshape(X.shape[1], 1) # coefficients sigma = par[0] # error sd # N = X.shape[0] # linear predictor LP = X * beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = norm.logpdf(y, loc = mu, scale = sigma) # log likelihood; or use norm.logpdf # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form L = -np.sum(L) # optim by default is minimization, and we want to maximize the likelihood return(L) An approach via least squares loss function. def lm_ls(par, X, y): # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par.reshape(3, 1) # coefficients N = X.shape[0] p = X.shape[1] # linear predictor LP = X * beta # linear predictor mu = LP # identity link in the glm sense # # # squared error loss # return(np.sum(np.square(y - mu))) Estimation X_mm = np.column_stack([np.repeat(1, N).reshape(N, 1), X]) # you may get warnings, they can be ignored fit_ml = minimize( fun = lm_ml, x0 = [1, 0, 0, 0], args = (X_mm, y), bounds = ((0, None), (None, None), (None, None), (None, None)), method = &#39;L-BFGS-B&#39;, tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) # can use least_squares directly # from scipy.optimize import least_squares fit_ls = minimize( lm_ls, x0 = np.array([0, 0, 0]), args = (X_mm, y), tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) Comparison import statsmodels.formula.api as smf model_sm_ols = smf.ols(&#39;y ~ X1 + X2&#39;, data = dfXy) fit_sm_ols = model_sm_ols.fit() pd.DataFrame( [ np.append(fit_ml.x[0]**2, fit_ml.x[1:]), np.append(fit_ls.fun/(N - X_mm.shape[1] - 1), fit_ls.x), np.append(fit_sm_ols.scale, fit_sm_ols.params) ], columns = [&#39;sigma&#39;,&#39;Int&#39;, &#39;b_X1&#39;, &#39;b_X2&#39;], index = [&#39;ML&#39;, &#39;OLS&#39;, &#39;SM&#39;] ) Logistic Regression Data Setup import numpy as np import pandas as pd from scipy.stats import logistic, binom from scipy.optimize import minimize np.random.seed(123) # ensures replication # predictors and response # increasing N will get estimated values closer to the known parameters N = 2500 # sample size k = 2 # number of desired predictors X = np.matrix(np.random.normal(size = N * k)).reshape(N, k) eta = -.5 + .2*X[:, 0] + .1*X[:, 1] pi = 1/(1 + np.exp(-eta)) y = np.random.binomial(1, p = pi, size = (N, 1)) dfXy = pd.DataFrame(np.column_stack([X, y]), columns = [&#39;X1&#39;, &#39;X2&#39;, &#39;y&#39;]) Functions A maximum likelihood approach. def logreg_ml(par, X, y): # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par.reshape(X.shape[1], 1) # coefficients # N = X.shape[0] # linear predictor LP = np.dot(X, beta) # linear predictor mu = 1/(1 + np.exp(-LP)) # logit link # calculate likelihood L = binom.logpmf(y, 1, mu) # log likelihood # L = np.multiply(y, np.log(mu)) + np.multiply(1 - y, np.log(1 - mu)) # alternate log likelihood form L = -np.sum(L) # optim by default is minimization, and we want to maximize the likelihood return(L) Another approach via exponential loss function. def logreg_exp(par, X, y): # Arguments # par: parameters to be estimated # X: predictor matrix with intercept column # y: target # setup beta = par.reshape(X.shape[1], 1) # coefficients # linear predictor LP = np.dot(X, beta) # linear predictor # calculate exponential loss function (convert y to -1:1 from 0:1) L = np.sum(np.exp(-np.multiply(np.where(y, 1, -1), LP) * .5)) return(L) Estimation Setup for use with optim. X_mm = np.column_stack([np.repeat(1, N).reshape(N, 1), X]) fit_ml = minimize( fun = logreg_ml, x0 = [0, 0, 0], args = (X_mm, y), method = &#39;BFGS&#39;, tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) fit_exp = minimize( fun = logreg_exp, x0 = [0, 0, 0], args = (X_mm, y), method = &#39;BFGS&#39;, tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) pars_ml = fit_ml.x pars_exp = fit_exp.x Comparison import statsmodels.formula.api as smf model_sm_glm = smf.logit(&#39;y ~ X1 + X2&#39;, data = dfXy) fit_sm_glm = model_sm_glm.fit() pd.DataFrame( [ pars_ml, pars_exp, fit_sm_glm.params ], columns = [&#39;Int&#39;, &#39;b_X1&#39;, &#39;b_X2&#39;], index = [&#39;fit_ml&#39;, &#39;fit_exp&#39;, &#39;SM&#39;] ) Quantile Regression See the R chapter for details. Data Setup import pandas as pd import numpy as np # originally from quantreg package engel = pd.read_csv(&#39;data/engel.csv&#39;) X = np.column_stack([ np.ones((engel.shape[0],1)), np.asarray(engel[&#39;income&#39;]) ]) y = np.asarray(engel[&#39;foodexp&#39;]) def qreg(par, X, y, tau): lp = np.dot(X, par) res = y - lp loss = np.where(res &lt; 0 , -(1 - tau)*res, tau*res) return(np.sum(loss)) Estimation We’ll estimate the median to start. # from scipy.stats import logistic, binom from scipy.optimize import minimize minimize( fun = qreg, x0 = [0, 0], args = (X, y, .5), method = &#39;BFGS&#39;, tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) Other quantiles Now we will add additional quantiles to estimate. pandas as pd # quantiles qs = [.05, .1, .25, .5, .75, .9, .95] fit = [] for tau in qs: init = minimize( fun = qreg, x0 = [0, 0], args = (X, y, tau), method = &#39;BFGS&#39;, tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) fit.append( pd.DataFrame(init.x.reshape(1,2), columns = [&#39;int&#39;, &#39;slope&#39;]) ) fit_qr = pd.concat(fit) from plotnine import * ( ggplot(aes(x = &#39;income&#39;, y = &#39;foodexp&#39;), data = engel) + geom_point() + geom_abline(aes(intercept = &#39;int&#39;, slope = &#39;slope&#39;, color = &#39;int&#39;), data = fit_qr) ) Stochastic Gradient Descent Here we use the adagrad approach as in the R demo. The dataset shift and other variants are not demonstrated. Data Setup Create some data for a standard linear regression. import numpy as np np.random.seed(1234) n = 10000 x1 = np.random.normal(size = n) x2 = np.random.normal(size = n) y = 1 + .5*x1 + .2*x2 + np.random.normal(size = n) X = np.column_stack([np.ones(n), x1, x2]) Function The estimating function using the adagrad approach. def sgd(par, X, y, stepsize, stepsize_tau = 0, average = False): &quot;&quot;&quot; Estimate a linear regression via stochastic gradient descent par: parameter estimates X: model matrix y: target variable stepsize: the learning rate stepsize_tau: if &gt; 0, a check on the LR at early iterations average: an alternative method Returns a dictionary of the parameter estimates, the estimates for each iteration, the loss for each iteration, the root mean square error, and the fitted values using the final parameter estimates &quot;&quot;&quot; beta = par betamat = np.zeros_like(X) fits = np.zeros_like(y) # if you want these across iterations loss = np.zeros_like(y) s = 0 for i in np.arange(X.shape[0]): Xi = X[i] yi = y[i] LP = np.dot(Xi, beta) grad = np.dot(Xi.T, LP - yi) s = s + grad**2 beta -= stepsize * grad/(stepsize_tau + np.sqrt(s)) if average and i &gt; 1: beta -= 1/i * (betamat[i - 1] - beta) betamat[i] = beta fits[i] = LP loss[i] = (LP - yi)**2 LP = np.dot(X, beta) lastloss = np.dot(LP - y, LP - y) return({ &#39;par&#39;: beta, # final estimates &#39;par_chain&#39;: betamat, # all estimates &#39;loss&#39;: loss, # observation level loss &#39;RMSE&#39;: np.sqrt(np.sum(lastloss)/X.shape[0]), &#39;fitted&#39;: LP }) Set starting values and estimate. For any particular data you might have to fiddle with the stepsize, perhaps choosing one based on cross-validation with old data. init = np.zeros(3) fit_sgd = sgd( init, X, y, stepsize = .1, stepsize_tau = .5 ) fit_sgd[&#39;par&#39;].round(4) fit_sgd[&#39;RMSE&#39;].round(4) array([1.0172, 0.4942, 0.1937]) 0.9978 Comparison We’ll compare our results to standard linear regression. import pandas as pd import statsmodels.formula.api as smf df = pd.DataFrame({&#39;x1&#39;: x1, &#39;x2&#39;: x2, &#39;y&#39;: y}) lm_fit = smf.ols(&#39;y ~ x1 + x2&#39;, df).fit() lm_fit.params.round(4) lm_fit.scale.round(4) Intercept 1.0037 x1 0.5032 x2 0.2000 dtype: float64 0.9955 Nelder-Mead import copy &#39;&#39;&#39; Francois Chollet&#39;s Nelder-Mead in Python. https://github.com/fchollet/nelder-mead/blob/master/nelder_mead.py Pure Python/Numpy implementation of the Nelder-Mead algorithm. Reference: https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method &#39;&#39;&#39; def nelder_mead( f, x_start, step = 0.1, no_improve_thr = 10e-6, no_improv_break = 10, max_iter = 0, alpha = 1., gamma = 2., rho = 0.5, sigma = 0.5 ): &#39;&#39;&#39; @param f (function): function to optimize, must return a scalar score and operate over a numpy array of the same dimensions as x_start @param x_start (numpy array): initial position @param step (float): look-around radius in initial step @no_improv_thr, no_improv_break (float, int): break after no_improv_break iterations with an improvement lower than no_improv_thr @max_iter (int): always break after this number of iterations. Set it to 0 to loop indefinitely. @alpha, gamma, rho, sigma (floats): parameters of the algorithm (see Wikipedia page for reference) return: tuple (best parameter array, best score) &#39;&#39;&#39; # init dim = len(x_start) prev_best = f(x_start) no_improv = 0 res = [[x_start, prev_best]] for i in range(dim): x = copy.copy(x_start) x[i] = x[i] + step score = f(x) res.append([x, score]) # simplex iter iters = 0 while 1: # order res.sort(key=lambda x: x[1]) best = res[0][1] # break after max_iter if max_iter and iters &gt;= max_iter: return res[0] iters += 1 # break after no_improv_break iterations with no improvement if iters//10 == 0: print(&#39;...best so far:&#39;, best) if best &lt; prev_best - no_improve_thr: no_improv = 0 prev_best = best else: no_improv += 1 if no_improv &gt;= no_improv_break: return res[0] # centroid x0 = [0.] * dim for tup in res[:-1]: for i, c in enumerate(tup[0]): x0[i] += c / (len(res)-1) # reflection xr = x0 + alpha*(x0 - res[-1][0]) rscore = f(xr) if res[0][1] &lt;= rscore &lt; res[-2][1]: del res[-1] res.append([xr, rscore]) continue # expansion if rscore &lt; res[0][1]: xe = x0 + gamma*(x0 - res[-1][0]) escore = f(xe) if escore &lt; rscore: del res[-1] res.append([xe, escore]) continue else: del res[-1] res.append([xr, rscore]) continue # contraction xc = x0 + rho*(x0 - res[-1][0]) cscore = f(xc) if cscore &lt; res[-1][1]: del res[-1] res.append([xc, cscore]) continue # reduction x1 = res[0][0] nres = [] for tup in res: redx = x1 + sigma*(tup[0] - x1) score = f(redx) nres.append([redx, score]) res = nres if __name__ == &quot;__main__&quot;: # test import math import numpy as np def f(x): return math.sin(x[0]) * math.cos(x[1]) * (1. / (abs(x[2]) + 1)) nelder_mead(f, np.array([0., 0., 0.])) HMM #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; From the wikipedia page with slight modification https://en.wikipedia.org/wiki/Viterbi_algorithm#Example &quot;&quot;&quot; def viterbi(obs, states, start_p, trans_p, emit_p): V = [{}] for st in states: V[0][st] = {&quot;prob&quot;: start_p[st] * emit_p[st][obs[0]], &quot;prev&quot;: None} # Run Viterbi when t &gt; 0 for t in range(1, len(obs)): V.append({}) for st in states: max_tr_prob = max(V[t-1][prev_st][&quot;prob&quot;]*trans_p[prev_st][st] for prev_st in states) for prev_st in states: if V[t-1][prev_st][&quot;prob&quot;] * trans_p[prev_st][st] == max_tr_prob: max_prob = max_tr_prob * emit_p[st][obs[t]] V[t][st] = {&quot;prob&quot;: max_prob, &quot;prev&quot;: prev_st} break for line in dptable(V): print(line) opt = [] # The highest probability max_prob = max(value[&quot;prob&quot;] for value in V[-1].values()) previous = None # Get most probable state and its backtrack for st, data in V[-1].items(): if data[&quot;prob&quot;] == max_prob: opt.append(st) previous = st break # Follow the backtrack till the first observation for t in range(len(V) - 2, -1, -1): opt.insert(0, V[t + 1][previous][&quot;prev&quot;]) previous = V[t + 1][previous][&quot;prev&quot;] print(&#39;The steps of states are &#39; + &#39; &#39;.join(opt) + &#39; with highest probability of %s&#39; % max_prob) def dptable(V): # Print a table of steps from dictionary yield &quot; &quot;.join((&quot;%12d&quot; % i) for i in range(len(V))) for state in V[0]: yield &quot;%.7s: &quot; % state + &quot; &quot;.join(&quot;%.7s&quot; % (&quot;%f&quot; % v[state][&quot;prob&quot;]) for v in V) # The function viterbi takes the following arguments: obs is the sequence of # observations, e.g. [&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;]; states is the set of hidden # states; start_p is the start probability; trans_p are the transition # probabilities; and emit_p are the emission probabilities. For simplicity of # code, we assume that the observation sequence obs is non-empty and that # trans_p[i][j] and emit_p[i][j] is defined for all states i,j. # In the running example, the forward/Viterbi algorithm is used as follows: obs = (&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) states = (&#39;Healthy&#39;, &#39;Fever&#39;) start_p = {&#39;Healthy&#39;: 0.6, &#39;Fever&#39;: 0.4} trans_p = { &#39;Healthy&#39; : {&#39;Healthy&#39;: 0.7, &#39;Fever&#39;: 0.3}, &#39;Fever&#39; : {&#39;Healthy&#39;: 0.4, &#39;Fever&#39;: 0.6} } emit_p = { &#39;Healthy&#39; : {&#39;normal&#39;: 0.5, &#39;cold&#39;: 0.4, &#39;dizzy&#39;: 0.1}, &#39;Fever&#39; : {&#39;normal&#39;: 0.1, &#39;cold&#39;: 0.3, &#39;dizzy&#39;: 0.6} } viterbi(obs,states,start_p,trans_p,emit_p) Julia Demos I haven’t played with Julia in a very long time, but briefly hacked the old code to get something that worked. As Julia has gone though notable changes, it’s doubtful these are very good as far as Julia programming standards go, though conceptually they may still provide some utility. Perhaps at some point I’ll reteach myself the basics and come back to these. In any case the code did run on a Jupyter notebook. Mixed Models One-factor ##################### ### Main function ### ##################### using LinearAlgebra using Statistics function one_factor_re_loglike(par::Vector) d, ni = size(y) mu = par[1] sigma2_mu = par[2] sigma2 = par[3] Sigmai = sigma2*I(ni) + sigma2_mu*ones(ni, ni) l = -(ni*d)/2*log(2*pi) - d/2*log(det(Sigmai)) for i in 1:d yi = y[i,:] l = l - .5(yi .- mu)&#39; * (Sigmai\\(yi .- mu)) end l = -l[1] return l end ################### ### Data set up ### ################### y = [22.6 20.5 20.8 22.6 21.2 20.5 17.3 16.2 16.6 21.4 23.7 23.2 20.9 22.2 22.6 14.5 10.5 12.3 20.8 19.1 21.3 17.4 18.6 18.6 25.1 24.8 24.9 14.9 16.3 16.6] ################################ ### Starting values and test ### ################################ using Statistics mu0 = mean(y) sigma2_mu0 = var(mean(y, dims = 2)) sigma20 = mean(var(y, dims = 2)) theta0 = [mu0, sigma2_mu0, sigma20] ### test one_factor_re_loglike(theta0) ########### ### Run ### ########### using Optim res = optimize(one_factor_re_loglike, theta0, LBFGS()) res * Status: success * Candidate solution Final objective value: 6.226441e+01 * Found with Algorithm: L-BFGS * Convergence measures |x - x&#39;| = 2.93e-08 ≰ 0.0e+00 |x - x&#39;|/|x&#39;| = 1.49e-09 ≰ 0.0e+00 |f(x) - f(x&#39;)| = 1.42e-14 ≰ 0.0e+00 |f(x) - f(x&#39;)|/|f(x&#39;)| = 2.28e-16 ≰ 0.0e+00 |g(x)| = 1.17e-09 ≤ 1.0e-08 * Work counters Seconds run: 1 (vs limit Inf) Iterations: 7 f(x) calls: 22 ∇f(x) calls: 22 Optim.minimizer(res) 3-element Array{Float64,1}: 19.599999980440952 12.193999992338886 1.1666666662195693 Optim.minimum(res) 62.30661224610756 Two-factor using LinearAlgebra using Statistics function sfran2_loglike(par::Vector) n = length(y) mu = par[1] sigma2_alpha = exp(par[2]) sigma2_gamma = exp(par[3]) sigma2 = exp(par[4]) Sigma = sigma2*I(n) + sigma2_alpha*(Xalpha * Xalpha&#39;) + sigma2_gamma * (Xgamma * Xgamma&#39;) l = -n/2*log(2*pi) - sum(log.(diag(cholesky(Sigma).L))) - .5*(y .- mu)&#39; * (Sigma\\(y .- mu)) l = -l[1] return l end ################## ### Data setup ### ################## y = [1.39,1.29,1.12,1.16,1.52,1.62,1.88,1.87,1.24,1.18, .95,.96,.82,.92,1.18,1.20,1.47,1.41,1.57,1.65] ################################ ### Starting values and test ### ################################ yhat = mean(reshape(y, 4, 5), 1) theta0 = [mean(y), log(var(yhat)), log(var(y)/3), log(var(y)/3)] sfran2_loglike(theta0) ########### ### Run ### ########### using Optim res = optimize(sfran2_loglike, theta0, method = :l_bfgs) res * Status: success * Candidate solution Final objective value: -1.199315e+01 * Found with Algorithm: L-BFGS * Convergence measures |x - x&#39;| = 6.60e-09 ≰ 0.0e+00 |x - x&#39;|/|x&#39;| = 1.08e-09 ≰ 0.0e+00 |f(x) - f(x&#39;)| = 5.33e-15 ≰ 0.0e+00 |f(x) - f(x&#39;)|/|f(x&#39;)| = 4.44e-16 ≰ 0.0e+00 |g(x)| = 7.02e-10 ≤ 1.0e-08 * Work counters Seconds run: 0 (vs limit Inf) Iterations: 9 f(x) calls: 23 ∇f(x) calls: 23 exp.(Optim.minimizer(res)) 4-element Array{Float64,1}: 3.7434213772629223 0.053720000000540405 0.031790000003692476 0.002290000000530042 -2*Optim.minimum(res) 23.98630759443859 Matlab Demos I don’t code in Matlab, nor have any particular desire to, so this is provided here just for reference. Mixed Models One-factor % matlab from Statistical Modeling and Computation (2014 p 311). See the % associated twofactorRE.R file for details. function one_factor_re_loglike(mu, sigma2_mu, sigma2, y) [d ni] = size(y); Sigmai = sigma2*eye(ni) + sigma2_mu*ones(ni,ni); l = -(ni*d) / 2*log(2*pi) - d / 2*log(det(Sigmai)); for i=1:d yi = y(i, :)&#39;; l = l - .5*(yi - mu)&#39; * (Sigmai\\(yi - mu)); end end y = [22.6 20.5 20.8; 22.6 21.2 20.5; 17.3 16.2 16.6; 21.4 23.7 23.2; 20.9 22.2 22.6; 14.5 10.5 12.3; 20.8 19.1 21.3; 17.4 18.6 18.6; 25.1 24.8 24.9; 14.9 16.3 16.6]; f = @(theta) -one_factor_re_loglike(theta(1), theta(2), theta(3), y); ybar = mean(y, 2); theta0 = [mean(ybar) var(ybar) mean(var(y, 0, 2))]; thetahat = fminsearch(f, theta0); Two-factor % matlab from Statistical Modeling and Computation (2014 p 314). See the % associated twofactorRE.R file for details. function sfran2_loglike(mu, eta_alpha, eta_gamma, eta, y, Xalpha, Xgamma) sigma2_alpha = exp(eta_alpha); sigma2_gamma = exp(eta_gamma); sigma2 = exp(eta); n = length(y); Sigma = sigma2*speye(n) + sigma2_alpha * (Xalpha * Xalpha&#39;) + sigma2_gamma * (Xgamma*Xgamma&#39;); l = -n/2 * log(2*pi) - sum(log(diag(chol(Sigma)))) - .5*(y - mu)&#39; * (Sigma\\(y - mu)); end y = [1.39 1.29 1.12 1.16 1.52 1.62 1.88 1.87 1.24 1.18 .95 .96 .82 .92 1.18 1.20 1.47 1.41 1.57 1.65]; Xalpha = kron(speye(5), ones(4,1)); Xgamma = kron(speye(10), ones(2,1)); f = @(theta) -sfran_loglike(theta(1), theta(2), theta(3), theta(4), y, Xalpha, Xgamma); yhat = mean(reshape(y, 4, 5)); theta0 = [mean(y) log(var(yhat)) log(var(y)/3) log(var(y)/3)]; thetahat = fminsearch(f, theta0) Gaussian Processes Any updates on the following can be found at the repo. function S = gaussSample(arg1, arg2, arg3) % Returns n samples (in the rows) from a multivariate Gaussian distribution % % Examples: % S = gaussSample(mu, Sigma, 10) % S = gaussSample(model, 100) % S = gaussSample(struct(&#39;mu&#39;,[0], &#39;Sigma&#39;, eye(1)), 3) % This file is from pmtk3.googlecode.com switch nargin case 3, mu = arg1; Sigma = arg2; n = arg3; case 2, model = arg1; mu = model.mu; Sigma = model.Sigma; n = arg2; case 1, model = arg1; mu = model.mu; Sigma = model.Sigma; n = 1; otherwise error(&#39;bad num args&#39;) end A = chol(Sigma, &#39;lower&#39;); Z = randn(length(mu), n); S = bsxfun(@plus, mu(:), A*Z)&#39;; end %% Visualize the effect of change the hyper-params for a 1d GP regression % based on demo_gpr by Carl Rasmussen % %% Generate data % This file is from pmtk3.googlecode.com n = 20; rand(&#39;state&#39;,18); randn(&#39;state&#39;,20); covfunc = {&#39;covSum&#39;, {&#39;covSEiso&#39;,&#39;covNoise&#39;}}; loghyper = [log(1.0); log(1.0); log(0.1)]; x = 15*(rand(n,1)-0.5); y = chol(feval(covfunc{:}, loghyper, x))&#39;*randn(n,1); % Cholesky decomp. xstar = linspace(-7.5, 7.5, 201)&#39;; hyps = [log(1), log(1), log(0.1);... log(0.3),log(1.08),log(0.00005);... log(3),log(1.16),log(0.89)]; %% compute post pred and plot marginals for i=1:size(hyps,1) loghyper = hyps(i,:)&#39;; [mu, S2] = gpr(loghyper, covfunc, x, y, xstar); S2 = S2 - exp(2*loghyper(3)); % remove observation noise figure; f = [mu+2*sqrt(S2);flipdim(mu-2*sqrt(S2),1)]; fill([xstar; flipdim(xstar,1)], f, [7 7 7]/8, &#39;EdgeColor&#39;, [7 7 7]/8); hold on plot(xstar,mu,&#39;k-&#39;,&#39;LineWidth&#39;,2); plot(x, y, &#39;k+&#39;, &#39;MarkerSize&#39;, 17); axis([-8 8 -3 3]) printPmtkFigure(sprintf(&#39;gprDemoChangeHparams%d&#39;, i)); end %% Reproduce figure 2.2 from GP book % %% % This file is from pmtk3.googlecode.com setSeed(0); L = 1; xs = (-5:0.2:5)&#39;; ns = length(xs); keps = 1e-8; muFn = @(x) 0*x(:).^2; Kfn = @(x,z) 1*exp(-sq_dist(x&#39;/L,z&#39;/L)/2); % plot sampled functions from the prior figure; hold on for i=1:3 model = struct(&#39;mu&#39;, muFn(xs), &#39;Sigma&#39;, Kfn(xs, xs) + 1e-15*eye(size(xs, 1))); fs = gaussSample(model, 1); plot(xs, fs, &#39;k-&#39;, &#39;linewidth&#39;, 2) end printPmtkFigure(&#39;gprDemoNoiseFreePrior&#39;) % generate noise-less training data Xtrain = [-4, -3, -2, -1, 1]&#39;; ftrain = sin(Xtrain); % compute posterior predictive K = Kfn(Xtrain, Xtrain); % K Ks = Kfn(Xtrain, xs); %K_* Kss = Kfn(xs, xs) + keps*eye(length(xs)); % K_** (keps is essential!) Ki = inv(K); postMu = muFn(xs) + Ks&#39;*Ki*(ftrain - muFn(Xtrain)); postCov = Kss - Ks&#39;*Ki*Ks; figure; hold on % plot marginal posterior variance as gray band mu = postMu(:); S2 = diag(postCov); f = [mu+2*sqrt(S2);flipdim(mu-2*sqrt(S2),1)]; fill([xs; flipdim(xs,1)], f, [7 7 7]/8, &#39;EdgeColor&#39;, [7 7 7]/8); % plot samples from posterior predictive for i=1:3 model = struct(&#39;mu&#39;, postMu(:)&#39;, &#39;Sigma&#39;, postCov); fs = gaussSample(model, 1); plot(xs, fs, &#39;k-&#39;, &#39;linewidth&#39;, 2) h=plot(Xtrain, ftrain, &#39;kx&#39;, &#39;markersize&#39;, 12, &#39;linewidth&#39;, 3); end printPmtkFigure(&#39;gprDemoNoiseFreePost&#39;) "]]
