[["index.html", "Model Estimation by Example Demonstrations with R", " Model Estimation by Example Demonstrations with R Michael Clark m-clark.github.io "],["introduction.html", "Introduction", " Introduction This document provides ‘by-hand’ demonstrations of various models and algorithms. The goal is to take away some of the mystery of them by providing clean code examples that are easy to run and compare with other tools. The code was collected over several years without respect to any previous code, so is not exactly consistent in style. However, in general, within each demo you will find some imported/simulated data, a primary estimating function, a comparison of results with some R package, and a link to the old code that was the initial demonstration. The document has several sections: Models: More or less standard/traditional statistical models and more Bayesian: Demonstrations of models using Stan Estimation: Algorithms used in model estimation (e.g. penalized likelihood, stochastic gradient descent) Supplemental: A handful of examples in languages other than R, possibly other miscellany This code is not meant to be extensive, or used in production, and in fact, some of these would probably be considered of historical interest only. To be clear, almost everything here has a package/module that would do the same thing far better and efficiently. Note also, the document itself is also not an introduction to any of these methods, and in fact contains very little expository text, assuming the reader has some familiarity with the model and possibly some reference text. This document is merely a learning tool for those wanting to dive a little deeper. The original code for these demonstrations may be found at their first home here: https://github.com/m-clark/Miscellaneous-R-Code. Many examples require some initial data processing or visualization via ggplot2, so it’s assumed the tidyverse set of packages is loaded for all demonstrations. While I’m happy to fix any glaring errors and broken links, this is pretty much a completed document, except on the off chance I add a demo on rare occasion. This code has accumulated over years, and I just wanted it in a nicer format. Perhaps if others would like to add to it via pull requests, I would do so. Last updated: 2020-11-24. "],["linear-regression.html", "Linear Regression Data Setup Functions Estimation Comparison Source", " Linear Regression We start our demonstrations with a standard regression model via maximum likelihood or least squares loss. Also included are examples for QR decomposition and normal equations. This can serve as an entry point for those starting out in the wider world of computational statistics, as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Least squares loss is not confined to the standard regression setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. Data Setup library(tidyverse) set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X, y) Functions A maximum likelihood approach. lm_ml &lt;- function(par, X, y) { # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } An approach via least squares loss function. lm_ls &lt;- function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link # calculate least squares loss function L = crossprod(y - mu) } Estimation Setup for use with optim. X = cbind(1, X) Initial values. Note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach as it is the objective function. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = lm_ml, X = X, y = y, control = list(reltol = 1e-8) ) optlmLS = optim( par = init[-1], fn = lm_ls, X = X, y = y, control = list(reltol = 1e-8) ) pars_ml = optlmML$par pars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par) # calculate sigma2 and add Comparison Compare to lm which uses QR decomposition. modlm = lm(y ~ ., dfXy) Example of QR. # QRX = qr(X) # Q = qr.Q(QRX) # R = qr.R(QRX) # Bhat = solve(R) %*% crossprod(Q, y) # alternate: qr.coef(QRX, y) round( rbind( pars_ml, pars_LS, modlm = c(summary(modlm)$sigma^2, coef(modlm))), digits = 3 ) sigma2 intercept b1 b2 pars_ml 0.219 -0.432 0.133 0.112 pars_LS 0.226 -0.432 0.133 0.112 modlm 0.226 -0.432 0.133 0.112 The slight difference in sigma is roughly dividing by N vs. N-k-1 in the traditional least squares approach. It diminishes with increasing N as both tend toward whatever sd^2 you specify when creating the y response above. Compare to glm, which by default assumes gaussian family with identity link and uses lm.fit. modglm = glm(y ~ ., data = dfXy) summary(modglm) Call: glm(formula = y ~ ., data = dfXy) Deviance Residuals: Min 1Q Median 3Q Max -0.93651 -0.33037 -0.06222 0.31068 1.03991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.43247 0.04807 -8.997 1.97e-14 *** X1 0.13341 0.05243 2.544 0.0125 * X2 0.11191 0.04950 2.261 0.0260 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 0.2262419) Null deviance: 24.444 on 99 degrees of freedom Residual deviance: 21.945 on 97 degrees of freedom AIC: 140.13 Number of Fisher Scoring iterations: 2 Via normal equations. coefs = solve(t(X) %*% X) %*% t(X) %*% y # coefficients Compare. sqrt(crossprod(y - X %*% coefs) / (N - k - 1)) [,1] [1,] 0.4756489 summary(modlm)$sigma [1] 0.4756489 sqrt(modglm$deviance / modglm$df.residual) [1] 0.4756489 c(sqrt(pars_ml[1]), sqrt(pars_LS[1])) sigma2 sigma2 0.4684616 0.4756490 # rerun by adding 3-4 zeros to the N Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_lm.R "],["logistic-regression.html", "Logistic Regression Data Setup Functions Estimation Comparison Source", " Logistic Regression The following demo regards a standard logistic regression model via maximum likelihood or exponential loss. This can serve as an entry point for those starting out to the wider world of computational statistics as maximum likelihood is the fundamental approach used in most applied statistics, but which is also a key aspect of the Bayesian approach. Exponential loss is not confined to the standard GLM setting, but is widely used in more predictive/‘algorithmic’ approaches e.g. in machine learning and elsewhere. This follows the linear regression model approach. Data Setup Predictors and target. library(tidyverse) set.seed(1235) # ensures replication N = 10000 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) # the linear predictor lp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these y = rbinom(N, size = 1, prob = plogis(lp)) dfXy = data.frame(X, y) Functions A maximum likelihood approach. logreg_ML &lt;- function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = plogis(LP) # logit link # calculate likelihood L = dbinom(y, size = 1, prob = mu, log = TRUE) # log likelihood # L = y*log(mu) + (1 - y)*log(1-mu) # alternate log likelihood form -sum(L) # optim by default is minimization, and we want to maximize the likelihood # (see also fnscale in optim.control) } Another approach via exponential loss function. logreg_exp &lt;- function(par, X, y) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par # coefficients # linear predictor LP = X %*% beta # linear predictor # calculate exponential loss function (convert y to -1:1 from 0:1) L = sum(exp(-ifelse(y, 1, -1) * .5 * LP)) } Estimation Setup for use with optim. X = cbind(1, X) # initial values init = rep(0, ncol(X)) names(init) = c(&#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmML = optim( par = init, fn = logreg_ML, X = X, y = y, control = list(reltol = 1e-8) ) optglmClass = optim( par = init, fn = logreg_exp, X = X, y = y, control = list(reltol = 1e-15) ) pars_ML = optlmML$par pars_exp = optglmClass$par Comparison Compare to glm. modglm = glm(y ~ ., dfXy, family = binomial) rbind( pars_ML, pars_exp, pars_GLM = coef(modglm) ) intercept b1 b2 pars_ML -0.5117658 0.2378927 0.08019841 pars_exp -0.5114284 0.2368478 0.07907056 pars_GLM -0.5117321 0.2378743 0.08032617 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/standard_logistic.R "],["mixed-model-one-factor.html", "One-factor Mixed Model Data Setup Function Estimation Comparison Source", " One-factor Mixed Model The following is an approach for one factor random effects model via maximum likelihood in R (and Matlab and Julia in the Supplemental Section). It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. Unfortunately I did this before knowing they had both Matlab and R code on their website, though the R code here is a little cleaner and has comments. The data regards crop yield from 10 randomly selected locations and three collections at each. Data Setup library(tidyverse) y = matrix(c(22.6,20.5,20.8, 22.6,21.2,20.5, 17.3,16.2,16.6, 21.4,23.7,23.2, 20.9,22.2,22.6, 14.5,10.5,12.3, 20.8,19.1,21.3, 17.4,18.6,18.6, 25.1,24.8,24.9, 14.9,16.3,16.6), 10, 3, byrow = TRUE) Function one_factor_re &lt;- function(mu, sigma2_mu, sigma2){ # Args # mu: intercept # sigma2_mu: variance of intercept # sigma2: residual variance of y # I follow their notation for consistency d = nrow(y) ni = ncol(y) # covariance matrix of observations Sigmai = sigma2 * diag(ni) + sigma2_mu * matrix(1, ni, ni) # log likelihood l = rep(NA, 10) # iterate over the rows for(i in 1:d){ l[i] = .5 * t(y[i, ] - mu) %*% chol2inv(chol(Sigmai)) %*% (y[i, ] - mu) } ll = -(ni*d) / 2*log(2*pi) - d / 2*log(det(Sigmai)) - sum(l) return(-ll) } Estimation Starting values starts = list( mu = mean(y), sigma2_mu = var(rowMeans(y)), sigma2 = mean(apply(y, 1, var)) ) Estimate at starting values. one_factor_re(mu = starts[[1]], sigma2_mu = starts[[2]], sigma2 = starts[[3]]) [1] 62.30661 Package bbmle has an mle2 function for maximum likelihood estimation based on underlying R functions like optim, and produces a nice summary table. LBFGS-B is used to place lower bounds on the variance estimates. library(bbmle) fit_mle = mle2( one_factor_re , start = starts, method = &#39;L-BFGS-B&#39;, lower = c( mu = -Inf, sigma2_mu = 0, sigma2 = 0 ), trace = TRUE ) Comparison We can compare to the lme4 model result. library(lme4) library(tidyverse) d = data.frame(y) %&gt;% pivot_longer(everything(), names_to = &#39;x&#39;, values_to = &#39;value&#39;) %&gt;% arrange(x) %&gt;% group_by(x) %&gt;% mutate(group = 1:n()) fit_mer = lmer(value ~ 1 | group, data = d, REML = FALSE) summary(fit_mle) Maximum likelihood estimation Call: mle2(minuslogl = one_factor_re, start = starts, method = &quot;L-BFGS-B&quot;, trace = TRUE, lower = c(mu = -Inf, sigma2_mu = 0, sigma2 = 0)) Coefficients: Estimate Std. Error z value Pr(z) mu 19.60000 1.12173 17.4729 &lt; 2.2e-16 *** sigma2_mu 12.19400 5.62858 2.1664 0.030277 * sigma2 1.16667 0.36893 3.1623 0.001565 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: 124.5288 summary(fit_mer) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: value ~ 1 | group Data: d AIC BIC logLik deviance df.resid 130.5 134.7 -62.3 124.5 27 Scaled residuals: Min 1Q Median 3Q Max -1.9950 -0.6555 0.1782 0.4870 1.7083 Random effects: Groups Name Variance Std.Dev. group (Intercept) 12.194 3.492 Residual 1.167 1.080 Number of obs: 30, groups: group, 10 Fixed effects: Estimate Std. Error t value (Intercept) 19.600 1.122 17.47 -2 * logLik(fit_mer) &#39;log Lik.&#39; 124.5288 (df=3) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/one_factor_RE.R "],["mixed-model-two-factor.html", "Two-factor Mixed Model Data Setup Function Estimation Comparison Source", " Two-factor Mixed Model An approach for two factor random effects model via maximum likelihood in R Matlab and Julia. It’s based on Statistical Modeling and Computation (2014) Chapter 10, example 10.10. The data regards the breeding value of a set of five sires in raising pigs. Each sire is mated to a random group of dams, with the response being the average daily weight gain in pounds of two piglets in each litter. See the previous chapter for a one factor model, and two_factor_RE.m and two_factor_RE.jl for the Matlab and Julia versions of this example on the GitHub site. Note that the text has a typo on the sigma2 variance estimate (value should be .0023 not .023). Data Setup library(tidyverse) y = c(1.39,1.29,1.12,1.16,1.52,1.62,1.88,1.87,1.24,1.18, .95,.96,.82,.92,1.18,1.20,1.47,1.41,1.57,1.65) # for use in lme4, but also a more conceptual representation of the data d = expand.grid(sire = rep(1:5, 2), dam = 1:2) d = data.frame(d[order(d$sire), ], y) Function The function takes the log variances eta* as input to keep positive. two_factor_re &lt;- function(mu, eta_alpha, eta_gamma, eta) { # Args # mu: intercept # eta_alpha: random effect one # eta_gamma: random effect two # eta: residual variance of y sigma2_alpha = exp(eta_alpha) sigma2_gamma = exp(eta_gamma) sigma2 = exp(eta) n = length(y) # covariance matrix of observations Sigma = sigma2 * diag(n) + sigma2_alpha * tcrossprod(Xalpha) + sigma2_gamma * tcrossprod(Xgamma) # log likelihood ll = -n / 2 * log(2 * pi) - sum(log(diag(chol(Sigma)))) - .5 * t(y - mu) %*% chol2inv(chol(Sigma)) %*% (y - mu) return(-ll) } Estimation Starting values and test. starts = list( mu = mean(y), eta_alpha = var(tapply(y, d$sire, mean)), eta_gamma = var(y) / 3, eta = var(y) / 3 ) Xalpha = diag(5) %x% rep(1,4) Xgamma = diag(10) %x% rep(1,2) Estimate at starting values. two_factor_re( mu = starts[[1]], eta_alpha = starts[[2]], eta_gamma = starts[[3]], eta = starts[[4]] ) [,1] [1,] 26.53887 Package bbmle has an mle2 function for maximum likelihood estimation based on underlying R functions like optim, and produces a nice summary table. LBFGS-B is used to place lower bounds on the variance estimates. library(bbmle) fit_mle = mle2(two_factor_re, start = starts, method = &#39;BFGS&#39;) Comparison We can compare to the lme4 model result. ### lme4 comparison library(lme4) fit_mer = lmer(y ~ (1 | sire) + (1 | dam:sire), d, REML = FALSE) summary(fit_mle) Maximum likelihood estimation Call: mle2(minuslogl = two_factor_re, start = starts, method = &quot;BFGS&quot;) Coefficients: Estimate Std. Error z value Pr(z) mu 1.32000 0.11848 11.1410 &lt; 2.2e-16 *** eta_alpha -2.92393 0.84877 -3.4449 0.0005712 *** eta_gamma -3.44860 0.65543 -5.2616 1.428e-07 *** eta -6.07920 0.44721 -13.5935 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: -23.98631 exp(coef(fit_mle)[-1]) eta_alpha eta_gamma eta 0.05372198 0.03178996 0.00229000 summary(fit_mer) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: y ~ (1 | sire) + (1 | dam:sire) Data: d AIC BIC logLik deviance df.resid -16 -12 12 -24 16 Scaled residuals: Min 1Q Median 3Q Max -1.21052 -0.59450 0.02314 0.61984 1.10386 Random effects: Groups Name Variance Std.Dev. dam:sire (Intercept) 0.03179 0.17830 sire (Intercept) 0.05372 0.23178 Residual 0.00229 0.04785 Number of obs: 20, groups: dam:sire, 10; sire, 5 Fixed effects: Estimate Std. Error t value (Intercept) 1.3200 0.1185 11.14 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Mixed%20Models/two_factor_RE.R "],["mixed-model-ML.html", "Mixed Model via ML Introduction Maximum Likelihood Estimation Source", " Mixed Model via ML Introduction The following is based on the Wood text (Generalized Additive Models, 1st ed.) on additive models, chapter 6 in particular. It assumes familiarity with standard regression from a matrix perspective and at least passing familiarity with mixed models. The full document this chapter is based on can be found here, and contains more detail and exposition. See also the comparison of additive and mixed models here. Maximum Likelihood Estimation For this we’ll use the sleepstudy data from the lme4 package. The data has reaction times for 18 individuals over 10 days each (see the help file for the sleepstudy object for more details). Data Setup library(tidyverse) data(sleepstudy, package = &#39;lme4&#39;) X = model.matrix(~Days, sleepstudy) Z = model.matrix(~factor(sleepstudy$Subject) - 1) colnames(Z) = paste0(&#39;Subject_&#39;, unique(sleepstudy$Subject)) # for cleaner presentation later rownames(Z) = paste0(&#39;Subject_&#39;, sleepstudy$Subject) y = sleepstudy$Reaction Function The following is based on the code in Wood (6.2.2), with a couple modifications for consistent nomenclature and presentation. We use optim and a minimizing function, in this case the negative log likelihood, to estimate the parameters of interest, collectively \\(\\theta\\), in the code below. The (square root of the) variances will be estimated on the log scale. In Wood, he simply extracts the ‘fixed effects’ for the intercept and days effects using lm (6.2.3), and we’ll do the same. ll_mixed &lt;- function(y, X, Z, theta) { tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 L = chol(e) # L&#39;L = e # transform dependent linear model to independent y = backsolve(L, y, transpose = TRUE) X = backsolve(L, X, transpose = TRUE) b = coef(lm(y~X-1)) LP = X %*% b ll = -n/2*log(2*pi) -sum(log(diag(L))) - crossprod(y - LP)/2 -ll } Here is an alternative function using a multivariate normal approach that doesn’t use the transformation to independent, and might provide additional perspective. ll_mixed_mv &lt;- function(y, X, Z, theta) { tau = exp(theta[1]) sigma = exp(theta[2]) n = length(y) # evaluate covariance matrix for y e = tcrossprod(Z)*tau^2 + diag(n)*sigma^2 b = coef(lm.fit(X, y)) mu = X %*% b ll = mvtnorm::dmvnorm(y, mu, e, log = TRUE) -ll } Estimation Now we’re ready to use the optim function for estimation. A slight change to tolerance is included to get closer estimates to lme4, which we will compare shortly. paramInit = c(0, 0) names(paramInit) = c(&#39;tau&#39;, &#39;sigma&#39;) fit = optim( fn = ll_mixed, X = X, y = y, Z = Z, par = paramInit, control = list(reltol = 1e-10) ) fit_mv = optim( fn = ll_mixed_mv, X = X, y = y, Z = Z, par = paramInit, control = list(reltol = 1e-10) ) func tau sigma negLogLik X(Intercept) XDays ll_mixed 36.016 30.899 897.039 251.405 10.467 ll_mixed_mv 36.016 30.899 897.039 251.405 10.467 As we can see, both formulations produce identical results. We can now compare those results to the lme4 output for the same model, and see that we’re getting what we should. library(lme4) fit_mer = lmer(Reaction ~ Days + (1|Subject), sleepstudy, REML = FALSE) fit_mer Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: Reaction ~ Days + (1 | Subject) Data: sleepstudy AIC BIC logLik deviance df.resid 1802.0786 1814.8505 -897.0393 1794.0786 176 Random effects: Groups Name Std.Dev. Subject (Intercept) 36.01 Residual 30.90 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 We can also predict the random effects (Wood, 6.2.4), sometimes called BLUPs (Best Linear Unbiased Predictions) and after doing so again compare the results to the lme4 estimates. tau = exp(fit$par)[1] tausq = tau^2 sigma = exp(fit$par)[2] sigmasq = sigma^2 Sigma = tcrossprod(Z)*tausq/sigmasq + diag(length(y)) ranef_est = tausq * t(Z) %*% solve(Sigma) %*% resid(lm(y ~ X - 1))/sigmasq data.frame( ranef_est, lme4 = ranef(fit_mer)$Subject[[1]] ) %&gt;% kable_df(2) ranef_est lme4 Subject_308 40.64 40.64 Subject_309 -77.57 -77.57 Subject_310 -62.88 -62.88 Subject_330 4.39 4.39 Subject_331 10.18 10.18 Subject_332 8.19 8.19 Subject_333 16.44 16.44 Subject_334 -2.99 -2.99 Subject_335 -45.12 -45.12 Subject_337 71.92 71.92 Subject_349 -21.12 -21.12 Subject_350 14.06 14.06 Subject_351 -7.83 -7.83 Subject_352 36.25 36.25 Subject_369 7.01 7.01 Subject_370 -6.34 -6.34 Subject_371 -3.28 -3.28 Subject_372 18.05 18.05 Issues with ML estimation Situations arise in which using maximum likelihood for mixed models would result in notably biased estimates (e.g. small N, lots of fixed effects), and so it is typically not used. Standard software usually defaults to restricted maximum likelihood. However, our purpose here has been served, so we will not dwell further on mixed model estimation. Link with penalized regression A link exists between mixed models and a penalized likelihood approach. For a penalized approach with the the standard linear model, the objective function we want to minimize can be expressed as follows: \\[ \\lVert y- X\\beta \\rVert^2 + \\beta^\\intercal\\beta \\] The added component to the sum of the squared residuals is the penalty. By adding the sum of the squared coefficients, we end up keeping them from getting too big, and this helps to avoid overfitting. Another interesting aspect of this approach is that it is comparable to using a specific prior on the coefficients in a Bayesian framework. We can now see mixed models as a penalized technique. If we knew \\(\\sigma\\) and \\(\\psi_\\theta\\), then the predicted random effects \\(g\\) and estimates for the fixed effects \\(\\beta\\) are those that minimize the following objective function: \\[ \\frac{1}{\\sigma^2}\\lVert y - X\\beta - Zg \\rVert^2 + g^\\intercal\\psi_\\theta^{-1}g \\] Source Main doc found at https://m-clark.github.io/docs/mixedModels/mixedModelML.html "],["probit.html", "Probit &amp; Bivariate Probit Standard Probit Bivariate Probit Source", " Probit &amp; Bivariate Probit Stata users seem to be the primary audience concerned with probit models, but I thought I’d play around with one even though I’ve never had reason to use it. Stata examples come from the UCLA ATS website and the Stata manual, so one can investigate the Stata result for comparison. Standard Probit The standard probit model is identical to the logistic model but using a different link function. Function probit_ll &lt;- function(beta, X, y) { mu = X %*% beta # these produce identical results, but the second is the typical depiction ll = sum(dbinom( y, size = 1, prob = pnorm(mu), log = T )) # ll = sum(y * pnorm(mu, log = T) + (1 - y) * log(1 - pnorm(mu))) -ll } Examples Example 1 detail available here. library(tidyverse) admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) head(admit) # A tibble: 6 x 4 admit gre gpa rank &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 380 3.61 3 2 1 660 3.67 3 3 1 800 4 1 4 1 640 3.19 4 5 0 520 2.93 4 6 1 760 3 2 X = model.matrix(admit~ gre + gpa + factor(rank), admit) y = admit$admit init = rep(0, ncol(X)) fit = optim( fn = probit_ll, par = init, X = X, y = y, method = &#39;BFGS&#39; ) fit $par [1] -2.2518260555 0.0007915268 0.5453357468 -0.4211611887 -0.8285356685 -0.9457812896 $value [1] 229.6227 $counts function gradient 140 11 $convergence [1] 0 $message NULL Example 2 from Stata manual on standard probit. We have data on the make, weight, and mileage rating of 22 foreign and 52 domestic automobiles. We wish to fit a probit model explaining whether a car is foreign based on its weight and mileage.\" auto = haven::read_dta(&#39;http://www.stata-press.com/data/r13/auto.dta&#39;) head(auto) # A tibble: 6 x 12 make price mpg rep78 headroom trunk weight length turn displacement gear_ratio foreign &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; 1 AMC Concord 4099 22 3 2.5 11 2930 186 40 121 3.58 0 [Domestic] 2 AMC Pacer 4749 17 3 3 11 3350 173 40 258 2.53 0 [Domestic] 3 AMC Spirit 3799 22 NA 3 12 2640 168 35 121 3.08 0 [Domestic] 4 Buick Century 4816 20 3 4.5 16 3250 196 40 196 2.93 0 [Domestic] 5 Buick Electra 7827 15 4 4 20 4080 222 43 350 2.41 0 [Domestic] 6 Buick LeSabre 5788 18 3 4 21 3670 218 43 231 2.73 0 [Domestic] X = model.matrix(foreign~ weight + mpg, auto) y = auto$foreign init = rep(0, ncol(X)) fit = optim( fn = probit_ll, par = init, X = X, y = y ) fit $par [1] 8.277015097 -0.002335939 -0.103973147 $value [1] 26.84419 $counts function gradient 380 NA $convergence [1] 0 $message NULL Bivariate Probit For the bivariate model, we are dealing with two binary outcomes and their correlation. Here is the main function. bivariate_probit_ll &lt;- function(pars, X, y1, y2) { rho = pars[1] mu1 = X %*% pars[2:(ncol(X) + 1)] mu2 = X %*% pars[(ncol(X) + 2):length(pars)] q1 = ifelse(y1 == 1, 1,-1) q2 = ifelse(y2 == 1, 1,-1) require(mnormt) eta1 = q1 * mu1 eta2 = q2 * mu2 ll = matrix(NA, nrow = nrow(X)) for (i in 1:length(ll)) { corr = q1[i] * q2[i] * rho corr = matrix(c(1, corr, corr, 1), 2) ll[i] = log( pmnorm( x = c(eta1[i], eta2[i]), mean = c(0, 0), varcov = corr ) ) } # the loop is probably clearer, and there is no difference in time, but here&#39;s # a oneliner ll = mapply(function(e1, e2, q1, q2) log(pmnorm(x = c(e1, e2), # varcov = matrix(c(1,q1*q2*rho,q1*q2*rho,1),2))), eta1, eta2, q1, q2) -sum(ll) } Example From the Stata manual on bivariate probit: We wish to model the bivariate outcomes of whether children attend private school and whether the head of the household voted for an increase in property tax based on the other covariates. school = haven::read_dta(&#39;http://www.stata-press.com/data/r13/school.dta&#39;) head(school) # A tibble: 6 x 11 obs pub12 pub34 pub5 private years school loginc logptax vote logeduc &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 0 1 0 0 10 1 9.77 7.05 1 7.21 2 2 0 1 0 0 8 0 10.0 7.05 0 7.61 3 3 1 0 0 0 4 0 10.0 7.05 0 8.28 4 4 0 1 0 0 13 0 9.43 6.40 0 6.82 5 5 0 1 0 0 3 1 10.0 7.28 1 7.69 6 6 1 0 0 0 5 0 10.5 7.05 0 6.91 X = model.matrix(private ~ years + logptax + loginc, school) y1 = school$private y2 = school$vote init = c(0, rep(0, ncol(X)*2)) # you&#39;ll probably get a warning or two, ignore; takes a couple seconds fit = optim( fn = bivariate_probit_ll, par = init, X = X, y1 = y1, y2 = y2, method = &#39;BFGS&#39; ) loglik = fit$value rho = fit$par[1] coefs_private = fit$par[2:(ncol(X) + 1)] coefs_vote = fit$par[(ncol(X) + 2):length(init)] names(coefs_private) = names(coefs_vote) = c(&#39;Int&#39;, &#39;years&#39;, &#39;logptax&#39;, &#39;loginc&#39;) list( loglik = loglik, rho = rho, Private = coefs_private, Vote = coefs_vote ) $loglik [1] 89.25407 $rho [1] -0.2695802 $Private Int years logptax loginc -4.14327955 -0.01193699 -0.11030513 0.37459892 $Vote Int years logptax loginc -0.52933721 -0.01686685 -1.28983223 0.99840976 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/bivariateProbit.R "],["heckman-selection.html", "Heckman Selection Data Setup Two step approach Maximum Likelihood Source", " Heckman Selection This demonstration of the Heckman selection model is based on Bleven’s example here, but which is more or less the ‘classic’ example regarding women’s wages, variations of which you’ll find all over. Data Setup Description of the data: Draw 10,000 obs at random educ uniform over [0,16] age uniform over [18,64] wearnl = 4.49 + 0.08 * educ + 0.012 * age + ε Generate missing data for wearnl drawn z from standard normal [0,1]. z is actually never explained in the slides, I think it’s left out on slide 3 and just represents an additional covariate. d*=-1.5+0.15*educ+0.01*age+0.15*z+v wearnl missing if d*≤0 wearn reported if d*&gt;0 wearnl_all = wearnl with non-missing obs library(tidyverse) set.seed(123456) N = 10000 educ = sample(1:16, N, replace = T) age = sample(18:64, N, replace = T) covmat = matrix(c(.46^2, .25*.46, .25*.46, 1), ncol = 2) errors = mvtnorm::rmvnorm(N, sigma = covmat) z = rnorm(N) e = errors[, 1] v = errors[, 2] wearnl = 4.49 + .08 * educ + .012 * age + e d_star = -1.5 + 0.15 * educ + 0.01 * age + 0.15 * z + v observed_index = d_star &gt; 0 d = data.frame(wearnl, educ, age, z, observed_index) Examine linear regression approaches if desired. # lm based on full data lm_all = lm(wearnl ~ educ + age, data=d) # lm based on observed data lm_obs = lm(wearnl ~ educ + age, data=d[observed_index,]) summary(lm_all) Call: lm(formula = wearnl ~ educ + age, data = d) Residuals: Min 1Q Median 3Q Max -2.03286 -0.31240 0.00248 0.31578 1.50828 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.4691266 0.0171413 260.72 &lt;2e-16 *** educ 0.0798814 0.0010005 79.84 &lt;2e-16 *** age 0.0124381 0.0003398 36.60 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4611 on 9997 degrees of freedom Multiple R-squared: 0.4331, Adjusted R-squared: 0.433 F-statistic: 3820 on 2 and 9997 DF, p-value: &lt; 2.2e-16 summary(lm_obs) # smaller coefs, resid standard error Call: lm(formula = wearnl ~ educ + age, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75741 -0.30289 -0.00133 0.30918 1.50032 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.6713823 0.0258865 180.46 &lt;2e-16 *** educ 0.0705849 0.0014760 47.82 &lt;2e-16 *** age 0.0114758 0.0004496 25.52 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4507 on 5517 degrees of freedom Multiple R-squared: 0.3374, Adjusted R-squared: 0.3372 F-statistic: 1405 on 2 and 5517 DF, p-value: &lt; 2.2e-16 Two step approach The two-step approach first conducts a probit model regarding whether the individual is observed or not, in order to calculate the inverse mills ratio, or ‘nonselection hazard’. The second step is a standard linear model. Step 1: Probit Model probit = glm(observed_index ~ educ + age + z, data = d, family = binomial(link = &#39;probit&#39;)) summary(probit) Call: glm(formula = observed_index ~ educ + age + z, family = binomial(link = &quot;probit&quot;), data = d) Deviance Residuals: Min 1Q Median 3Q Max -2.4674 -0.9062 0.4628 0.8800 2.2674 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.519248 0.052819 -28.763 &lt;2e-16 *** educ 0.150027 0.003220 46.588 &lt;2e-16 *** age 0.010072 0.001015 9.926 &lt;2e-16 *** z 0.159292 0.013889 11.469 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 13755 on 9999 degrees of freedom Residual deviance: 11119 on 9996 degrees of freedom AIC: 11127 Number of Fisher Scoring iterations: 4 # http://www.stata.com/support/faqs/statistics/inverse-mills-ratio/ probit_lp = predict(probit) mills0 = dnorm(probit_lp)/pnorm(probit_lp) summary(mills0) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.38632 0.70027 0.75664 1.09246 1.96602 # identical formulation # probit_lp = -predict(probit) # imr = dnorm(probit_lp)/(1-pnorm(probit_lp)) imr = mills0[observed_index] summary(imr) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.07588 0.28739 0.48466 0.57015 0.77617 1.87858 Take a look at the distribution. ggplot2::qplot(imr, geom = &#39;histogram&#39;) Step 2: Estimate via Linear Regression Standard regression model using the inverse mills ratio as covariate lm_select = lm(wearnl ~ educ + age + imr, data = d[observed_index, ]) summary(lm_select) Call: lm(formula = wearnl ~ educ + age + imr, data = d[observed_index, ]) Residuals: Min 1Q Median 3Q Max -1.75994 -0.30293 -0.00186 0.31049 1.48179 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1063144 42.477 &lt;2e-16 *** educ 0.0782580 0.0052989 14.769 &lt;2e-16 *** age 0.0119700 0.0005564 21.513 &lt;2e-16 *** imr 0.0955209 0.0633557 1.508 0.132 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.4506 on 5516 degrees of freedom Multiple R-squared: 0.3377, Adjusted R-squared: 0.3373 F-statistic: 937.4 on 3 and 5516 DF, p-value: &lt; 2.2e-16 Compare to sampleSelection package. library(sampleSelection) selection_2step = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;2step&#39;) summary(selection_2step) -------------------------------------------- Tobit 2 model (sample selection model) 2-step Heckman / heckit estimation 10000 observations (4480 censored and 5520 observed) 10 free parameters (df = 9991) Probit selection equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.519248 0.052725 -28.815 &lt;2e-16 *** educ 0.150027 0.003221 46.577 &lt;2e-16 *** age 0.010072 0.001014 9.934 &lt;2e-16 *** z 0.159292 0.013937 11.430 &lt;2e-16 *** Outcome equation: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.5159161 0.1066914 42.33 &lt;2e-16 *** educ 0.0782580 0.0053181 14.71 &lt;2e-16 *** age 0.0119700 0.0005592 21.41 &lt;2e-16 *** Multiple R-Squared:0.3377, Adjusted R-Squared:0.3373 Error terms: Estimate Std. Error t value Pr(&gt;|t|) invMillsRatio 0.09552 0.06354 1.503 0.133 sigma 0.45550 NA NA NA rho 0.20970 NA NA NA -------------------------------------------- coef(lm_select)[&#39;imr&#39;] / summary(lm_select)$sigma # slightly off imr 0.2119813 coef(lm_select)[&#39;imr&#39;] / summary(selection_2step)$estimate[&#39;sigma&#39;, &#39;Estimate&#39;] imr 0.2097041 Maximum Likelihood The following likelihood function takes arguments as follows: par: the regression coefficients pertaining to the two models, the residual standard error sigma and rho for the correlation estimate X: observed data model matrix for the linear regression model Z: complete data model matrix for the probit model y: the target variable observed_index: an index denoting whether y is observed select_ll &lt;- function(par, X, Z, y, observed_index) { gamma = par[1:4] lp_probit = Z %*% gamma beta = par[5:7] lp_lm = X %*% beta sigma = par[8] rho = par[9] ll = sum(log(1-pnorm(lp_probit[!observed_index]))) + - log(sigma) + sum(dnorm(y, mean = lp_lm, sd = sigma, log = TRUE)) + sum( pnorm((lp_probit[observed_index] + rho/sigma * (y-lp_lm)) / sqrt(1-rho^2), log.p = TRUE) ) -ll } X = model.matrix(lm_select) Z = model.matrix(probit) # initial values init = c(coef(probit), coef(lm_select)[-4], 1, 0) Estimate via optim. Without bounds for sigma and rho you’ll get warnings, but does fine anyway fit_unbounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;BFGS&#39;, control = list(maxit = 1000, reltol = 1e-15), hessian = T ) fit_bounded = optim( init, select_ll, X = X[, -4], Z = Z, y = wearnl[observed_index], observed_index = observed_index, method = &#39;L-BFGS&#39;, lower = c(rep(-Inf, 7), 1e-10,-1), upper = c(rep(Inf, 8), 1), control = list(maxit = 1000, factr = 1e-15), hessian = T ) Comparison Comparison model. selection_ml = selection(observed_index ~ educ + age + z, wearnl ~ educ + age, method = &#39;ml&#39;) # summary(selection_ml) We now compare the results of the different estimation approaches. model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step probit (Intercept) -1.52026540 -1.521 -1.523 -1.519 -1.519 probit educ 0.15020205 0.150 0.150 0.150 0.150 probit age 0.01006608 0.010 0.010 0.010 0.010 probit z 0.15747033 0.158 0.158 0.159 0.159 lm (Intercept) 4.47798502 4.480 4.481 4.516 4.516 lm educ 0.08012367 0.080 0.080 0.078 0.078 lm age 0.01209129 0.012 0.012 0.012 0.012 lm sigma 0.45820605 0.458 0.458 0.451 0.456 both rho 0.25945397 0.257 0.255 0.212 0.210 model par sampselpack_ml unbounded_ml bounded_ml explicit_twostep sampselpack_2step probit (Intercept) 0.053 0.053 0.053 0.053 0.053 probit educ 0.003 0.003 0.003 0.003 0.003 probit age 0.001 0.001 0.001 0.001 0.001 probit z 0.014 0.014 0.014 0.014 0.014 lm (Intercept) 0.090 0.090 0.090 0.106 0.107 lm educ 0.004 0.004 0.005 0.005 0.005 lm age 0.001 0.001 0.001 0.001 0.001 lm sigma 0.008 0.008 0.008 NA NA both rho 0.112 0.112 0.112 NA NA Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/heckman_selection.R "],["marginal-structural.html", "Marginal Structural Model Data Setup Function Estimation Comparison Source", " Marginal Structural Model This is a demonstration of a simple marginal structural model for estimation of so-called ‘causal’ effects using inverse probability weighting. Example data is from, and comparison made to, the ipw package. See more here. Data Setup This example is from the helpfile at ?ipwpoint. library(tidyverse) library(ipw) set.seed(16) n = 1000 simdat = data.frame(l = rnorm(n, 10, 5)) a_lin = simdat$l - 10 pa = plogis(a_lin) simdat = simdat %&gt;% mutate( a = rbinom(n, 1, prob = pa), y = 10 * a + 0.5 * l + rnorm(n, -10, 5) ) ipw_result = ipwpoint( exposure = a, family = &quot;binomial&quot;, link = &quot;logit&quot;, numerator = ~ 1, denominator = ~ l, data = simdat ) summary(ipw_result$ipw.weights) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.4810 0.5127 0.5285 0.9095 0.6318 74.6994 ipwplot(ipw_result$ipw.weights) We create the weights as follows using the probabilities from a logistic regression. ps_num = fitted(glm(a ~ 1, data = simdat, family = &#39;binomial&#39;)) ps_num[simdat$a == 0] = 1 - ps_num[simdat$a == 0] ps_den = fitted(glm(a ~ l, data = simdat, family = &#39;binomial&#39;)) ps_den[simdat$a == 0] = 1 - ps_den[simdat$a == 0] wts = ps_num / ps_den Compare the weights. rbind(summary(wts), summary(ipw_result$ipw.weights)) Min. 1st Qu. Median Mean 3rd Qu. Max. [1,] 0.481 0.5127181 0.5284768 0.9094652 0.631794 74.6994 [2,] 0.481 0.5127181 0.5284768 0.9094652 0.631794 74.6994 Add inverse probability weights to the data if desired. simdat = simdat %&gt;% mutate(sw = ipw_result$ipw.weights) Function Create the likelihood function for using the weights. msm_ll &lt;- function( par, # parameters to be estimated; first is taken to be sigma X, # model matrix y, # target variable wts # estimated weights ) { beta = par[-1] lp = X %*% beta sigma = exp(par[1]) # exponentiated value to stay positive ll = dnorm(y, mean = lp, sd = sigma, log = TRUE) -sum(ll * wts) # weighted likelihood # same as # ll = dnorm(y, mean = lp, sd = sigma)^wts # -sum(log(ll)) } Estimation We want to estimate the marginal structural model for the causal effect of a on y corrected for confounding by l, using inverse probability weighting with robust standard error from the survey package. Create the matrices for estimation, estimate the model, and extract results. X = cbind(1, simdat$a) y = simdat$y fit = optim( par = c(sigma = 0, intercept = 0, b = 0), fn = msm_ll, X = X, y = y, wts = wts, hessian = TRUE, method = &#39;BFGS&#39;, control = list(abstol = 1e-12) ) dispersion = exp(fit$par[1])^2 beta = fit$par[-1] Now we compute the standard errors. The following uses the survey package raw version to get the appropriate standard errors, which the ipw approach uses. glm_basic = glm(y ~ a, data = simdat, weights = wts) # to get unscaled cov res = resid(glm_basic, type = &#39;working&#39;) # residuals glm_vcov_unsc = summary(glm_basic)$cov.unscaled # weighted vcov unscaled by dispersion solve(crossprod(qr(X))) estfun = X * res * wts x = estfun %*% glm_vcov_unsc Comparison library(&quot;survey&quot;) fit_msm = svyglm( y ~ a, design = svydesign(~ 1, weights = ~ sw, data = simdat) ) summary(fit_msm) Call: svyglm(formula = y ~ a, design = svydesign(~1, weights = ~sw, data = simdat)) Survey design: svydesign(~1, weights = ~sw, data = simdat) Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -4.375 1.142 -3.832 0.000135 *** a 10.647 1.190 8.948 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 29.58889) Number of Fisher Scoring iterations: 2 Now get the standard errors. se = sqrt(diag(crossprod(x) * n/(n-1))) # a robust standard error se_robust = sqrt(diag(sandwich::sandwich(glm_basic))) # an easier way to get it se_msm = sqrt(diag(vcov(fit_msm))) # extract from msm model Compare standard errors. tibble(se, se_robust, se_msm) # A tibble: 2 x 3 se se_robust se_msm &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1.14 1.14 1.14 2 1.19 1.19 1.19 Inspect the general fit and compare with the other. (#tab:msm-comparison- show)msm_ll Estimate init_se se_robust t p dispersion -4.378 0.247 1.141 -3.834 0 29.563 10.649 0.361 1.189 8.950 0 29.563 (#tab:msm-comparison- show)svyglm term estimate std.error statistic p.value (Intercept) -4.375 1.142 -3.832 0 a 10.647 1.190 8.948 0 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ipw.R "],["tobit.html", "Tobit Censoring with an Upper Limit Censoring with a Lower Limit Source", " Tobit The following is a simple demonstration of tobit regression via maximum likelihood. The issue is one where data is censored such that while we observe the value, it is not the true value, which would extend beyond the range of the observed data. This is very commonly seen in cases where the dependent variable has been given some arbitrary cutoff at the lower or upper end of the range, often resulting in floor or ceiling effects respectively. The conceptual idea is that we are interested in modeling the underlying latent variable that would not have such restriction if it was actually observed. Censoring with an Upper Limit Data Setup Data regards academic aptitude (GRE scores) with will be modeled using reading and math test scores, as well as the type of program the student is enrolled in (academic, general, or vocational). See this for an applied example and more detail. library(tidyverse) acad_apt = read_csv(&quot;https://stats.idre.ucla.edu/stat/data/tobit.csv&quot;) %&gt;% mutate(prog = factor(prog, labels = c(&#39;acad&#39;, &#39;general&#39;, &#39;vocational&#39;))) Setup data and initial values. initmod = lm(apt ~ read + math + prog, data = acad_apt) X = model.matrix(initmod) init = c(coef(initmod), log_sigma = log(summary(initmod)$sigma)) Function tobit_ll &lt;- function(par, X, y, ul = -Inf, ll = Inf) { # this function only takes a lower OR upper limit # parameters sigma = exp(par[length(par)]) beta = par[-length(par)] # create indicator depending on chosen limit if (!is.infinite(ll)) { limit = ll indicator = y &gt; ll } else { limit = ul indicator = y &lt; ul } # linear predictor lp = X %*% beta # log likelihood ll = sum(indicator * log((1/sigma)*dnorm((y-lp)/sigma)) ) + sum((1-indicator) * log(pnorm((lp-limit)/sigma, lower = is.infinite(ll)))) -ll } Estimation Estimate via optim. res = optim( par = init, tobit_ll, y = acad_apt$apt, X = X, ul = 800, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) # this would be more akin to the default Stata default approach # optim( # par = init, # tobit_ll, # y = acad_apt$apt, # X = X, # ul = 800, # control = list(maxit = 16000, reltol = 1e-15) # ) Comparison Compare to AER package tobit function. library(survival) aer_mod = AER::tobit( apt ~ read + math + prog, data = acad_apt, left = -Inf, right = 800 ) rbind( tobit = c( res$par[1:5], sigma = exp(res$par[6]), logLike = -res$value ), AER = c(coef(aer_mod), aer_mod$scale, logLik(aer_mod)) ) %&gt;% kable_df() (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 AER is actually just using survreg from the survival package. Survival models are usually for modeling time to some event, e.g. death in medical studies, and the censoring comes from the fact that the observed event does not occur for some people. Like our tobit function, an indicator is needed to denote who is or isn’t censored. In survival models, the indicator is for the event itself, and means they are NOT censored. So we’ll reverse the indicator used in the tobit function for survreg. surv_mod = survreg(Surv(apt, apt &lt; 800, type = &#39;right&#39;) ~ read + math + prog, data = acad_apt, dist = &#39;gaussian&#39;) Compare all results. (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit_ll 209.566 2.698 5.914 -12.716 -46.143 65.677 -1041.063 AER 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 survival 209.566 2.698 5.914 -12.715 -46.144 65.677 -1041.063 Censoring with a Lower Limit Create a censored data situation for the low end. The scale itself would be censored for anyone scoring a 200, but that basically doesn’t happen. In this data, 15 are less than a score of 500, so we’ll do that. acad_apt = acad_apt %&gt;% mutate(apt2 = apt, apt2 = if_else(apt2 &lt; 500, 500, apt2)) Estimate and use AER for comparison. res = optim( par = init, tobit, y = acad_apt$apt2, X = X, ll = 400, method = &#39;BFGS&#39;, control = list(maxit = 2000, reltol = 1e-15) ) aer_mod = AER::tobit(apt2 ~ read + math + prog, data = acad_apt, left = 400) Comparison (Intercept) read math proggeneral progvocational sigma.log_sigma logLike tobit_ll 270.408 2.328 5.086 -11.331 -38.606 57.024 -1092.483 AER 270.409 2.328 5.085 -11.331 -38.606 57.024 -1092.483 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/tobit.R "],["cox.html", "Cox Survival Standard Proportional Hazards Time-varying coefficients Stratified Cox Model Source", " Cox Survival Some simple demonstrations of a standard Cox, Cox with time-varying covariates and a stratified Cox. Standard Proportional Hazards Data Setup set.seed(12) dur = 1:10 kittyblarg = rnorm(10) # something happened to kitty! kittyhappy = rep(0:1, times = 5) # is kitty happy? kittydied = sample(0:1, 10, replace = TRUE) # kitty died! oh no! d = data.frame(kittyblarg, kittyhappy, dur, kittydied) # Inspect d kittyblarg kittyhappy dur kittydied 1 -1.4805676 0 1 0 2 1.5771695 1 2 1 3 -0.9567445 0 3 0 4 -0.9200052 1 4 1 5 -1.9976421 0 5 1 6 -0.2722960 1 6 1 7 -0.3153487 0 7 0 8 -0.6282552 1 8 1 9 -0.1064639 0 9 0 10 0.4280148 1 10 1 Function Create a the (partial) likelihood function to feed to optim. cox_pl &lt;- function(pars, preds, died, t) { # Arguments- # pars: coefficients of interest # preds: predictor matrix # died: death # t: time b = pars X = as.matrix(preds[order(t), ]) died2 = died[order(t)] LP = X%*%b # Linear predictor # initialize log likelihood due to looping, not necessary ll = numeric(nrow(X)) rows = 1:nrow(preds) for (i in rows){ riskset = ifelse(rows &lt; i, FALSE, TRUE) # identify risk set ll[i] = died2[i]*(LP[i] - log(sum(exp(LP[riskset]))) ) # log likelihood } -sum(ll) } Estimation Estimate with optim. initial_values = c(0, 0) fit = optim( par = initial_values, fn = cox_pl, preds = d[, c(&#39;kittyblarg&#39;, &#39;kittyhappy&#39;)], died = d[, &#39;kittydied&#39;], t = dur, method = &quot;BFGS&quot;, hessian = T ) fit $par [1] -0.5827125 1.3803731 $value [1] 7.783878 $counts function gradient 14 5 $convergence [1] 0 $message NULL $hessian [,1] [,2] [1,] 1.9913282 0.4735968 [2,] 0.4735968 0.7780126 Comparison Extract results. B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se # create a summary table result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE)*2, pnorm(Z, lower = TRUE)*2) ) Compare to survival package. library(survival) cox_model = coxph(Surv(dur, kittydied) ~ kittyblarg + kittyhappy) B exp se Z p coxph.kittyblarg -0.583 0.558 0.766 -0.760 0.447 coxph.kittyhappy 1.380 3.976 1.226 1.126 0.260 cox_pl.kittyblarg -0.583 0.558 0.766 -0.760 0.447 cox_pl.kittyhappy 1.380 3.976 1.226 1.126 0.260 Time-varying coefficients Note that technically nothing new is going on here relative to the previous model. See the vignette for the survival package for further details. Data Setup In the following we’ll first create some noisy time points. set.seed(123) t1 = rep(NA, 20) t2 = rep(NA, 20) t1[seq(1, 20, by = 2)] = 1:10 t2[seq(1, 20, by = 2)] = t1[seq(1, 20, by = 2)] + sample(1:5, 10, replace = TRUE) + abs(rnorm(10)) t1[seq(2, 20, by = 2)] = t2[seq(1, 20, by = 2)] t2[seq(2, 20, by = 2)] = t1[seq(2, 20, by = 2)] + sample(1:5) + abs(rnorm(10)) kitty = rep(1:10, e = 2) kittyblarg = t2 + rnorm(20, sd = 5) kittyhappy = rep(0:1, times = 5, e = 2) die = 0:1 cens = c(0, 0) kittydied = ifelse(runif(20)&gt;=.5, die, cens) d = data.frame(kitty, kittyblarg, kittyhappy, t1, t2, kittydied) # Inspect the Surv object if desired # Surv(t1,t2, kittydied) # Inspect the data d kitty kittyblarg kittyhappy t1 t2 kittydied 1 1 -1.870759 0 1.000000 4.686853 0 2 1 4.904677 0 4.686853 7.902718 0 3 2 4.798609 1 2.000000 5.445662 0 4 2 15.214255 1 5.445662 10.780575 0 5 3 5.467102 0 3.000000 6.224082 0 6 3 9.958737 0 6.224082 8.309781 1 7 4 -9.776800 1 4.000000 6.359814 0 8 4 4.586278 1 6.359814 8.445237 0 9 5 9.833514 0 5.000000 8.400771 0 10 5 7.368822 0 8.400771 13.471382 1 11 6 13.283435 1 6.000000 11.110683 0 12 6 18.256961 1 11.110683 14.256076 0 13 7 10.736186 0 7.000000 11.555841 0 14 7 23.935980 0 11.555841 17.721386 1 15 8 6.114988 1 8.000000 10.786913 0 16 8 14.573972 1 10.786913 12.605429 1 17 9 13.516008 0 9.000000 11.497850 0 18 9 9.750603 0 11.497850 14.182787 1 19 10 8.371929 1 10.000000 14.966617 0 20 10 19.430893 1 14.966617 19.286674 1 Function cox_pl_tv &lt;- function(pars, preds, died, t1, t2, data) { # Same arguments as before though will take a data object # plus variable names via string input. Also requires beginning # and end time point (t1, t2) dat = data[,c(preds, died, t1, t2)] dat = dat[order(dat$t2), ] b = pars X = as.matrix(dat[, preds]) died2 = dat[, died] # linear predictor LP = X%*%b # log likelihood ll = numeric(nrow(X)) rows = 1:nrow(dat) for (i in rows){ st_i = dat$t2[i] # if they have already died/censored (row &lt; i) or if the initial time is # greater than current end time (t1 &gt; st_i), they are not in the risk set, # else they are. riskset = ifelse(rows &lt; i | dat$t1 &gt; st_i, FALSE, TRUE) ll[i] = died2[i]*(LP[i] - log(sum(exp(LP[riskset]))) ) } -sum(ll) } Estimation Estimate with optim. initial_values = c(0, 0) fit = optim( par = initial_values, fn = cox_pl_tv, preds = c(&#39;kittyblarg&#39;, &#39;kittyhappy&#39;), died = &#39;kittydied&#39;, data = d, t1 = &#39;t1&#39;, t2 = &#39;t2&#39;, method = &quot;BFGS&quot;, hessian = TRUE ) # fit Comparison Extract results. B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE) * 2, pnorm(Z, lower = TRUE) * 2) ) Compare to survival package. cox_model_tv = coxph( Surv(t1, t2, kittydied) ~ kittyblarg + kittyhappy, method = &#39;breslow&#39;, control = coxph.control(iter.max = 1000) ) # cox_model_tv # cox_model_tv$loglik[2] B exp se Z p coxph.kittyblarg -0.092 0.912 0.107 -0.853 0.394 coxph.kittyhappy -1.513 0.220 1.137 -1.331 0.183 cox_pl.kittyblarg -0.092 0.912 0.107 -0.853 0.394 cox_pl.kittyhappy -1.513 0.220 1.137 -1.331 0.183 Stratified Cox Model Data Setup data(ovarian, package = &#39;survival&#39;) Function Requires cox_pl function above though one could extend to cox_pl_tv. cox_pl_strat &lt;- function(pars, preds, died, t, strata) { strat = as.factor(strata) d = data.frame(preds, died, t, strat) dlist = split(d, strata) neglls = map_dbl( dlist, function(x) cox_pl( pars = pars, preds = x[, colnames(preds)], died = x$died, t = x$t ) ) sum(neglls) } Estimation Estimate with optim. initial_values = c(0, 0) fit = optim( par = initial_values, fn = cox_pl_strat, preds = ovarian[, c(&#39;age&#39;, &#39;ecog.ps&#39;)], died = ovarian$fustat, t = ovarian$futime, strata = ovarian$rx, method = &quot;BFGS&quot;, hessian = TRUE ) # fit Comparison B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se result_tbl = data.frame( B, exp = exp(B), se, Z, p = ifelse(Z &gt; 0, pnorm(Z, lower = FALSE) * 2, pnorm(Z, lower = TRUE)*2) ) cox_strata_model = coxph( Surv(futime, fustat) ~ age + ecog.ps + strata(rx), data = ovarian ) # cox_strata_model # cox_strata_model$loglik[2] B exp se Z p coxph.age 0.139 1.149 0.048 2.885 0.004 coxph.ecog.ps -0.097 0.908 0.630 -0.154 0.878 cox_pl.age 0.139 1.149 0.048 2.885 0.004 cox_pl.ecog.ps -0.097 0.908 0.630 -0.154 0.878 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/survivalCox.R "],["hurdle.html", "Hurdle Model Poisson Negative Binomial Source", " Hurdle Model Hurdle models are applied to situations in which target data has relatively many of one value, usually zero, to go along with the other observed values. They are two-part models, a logistic model for whether an observation is zero or not, and a count model for the other part. The key distinction from the usual ‘zero-inflated’ count models, is that the count distribution does not contribute to the excess zeros. While the typical application is count data, the approach can be applied to any distribution in theory. Poisson Data Setup Here we import a simple data set. The example comes from the Stata help file for zinb command. One can compare results with hnblogit command in Stata. library(tidyverse) fish = haven::read_dta(&quot;http://www.stata-press.com/data/r11/fish.dta&quot;) Function The likelihood function is of two parts, one a logistic model, the other, a poisson count model. hurdle_poisson_ll &lt;- function(y, X, par) { # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] poispars = par[grep(&#39;pois&#39;, names(par))] # Logit model part Xlogit = X ylogit = ifelse(y == 0, 0, 1) LPlogit = Xlogit %*% logitpars mulogit = plogis(LPlogit) # Calculate the likelihood logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) ) # Poisson part Xpois = X[y &gt; 0, ] ypois = y[y &gt; 0] mupois = exp(Xpois %*% poispars) # Calculate the likelihood loglik0 = -mupois loglikpois = -sum(dpois(ypois, lambda = mupois, log = TRUE)) + sum(log(1 - exp(loglik0))) # combine likelihoods loglik = loglikpois + logliklogit loglik } Get some starting values from glm For these functions, and create a named vector for them. init_mod = glm( count ~ persons + livebait, data = fish, family = poisson, x = TRUE, y = TRUE ) starts = c(logit = coef(init_mod), pois = coef(init_mod)) Estimation Use optim. to estimate parameters. I fiddle with some options to reproduce the hurdle function as much as possible. fit = optim( par = starts, fn = hurdle_poisson_ll, X = init_mod$x, y = init_mod$y, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit Extract the elements from the output to create a summary table. B = fit$par se = sqrt(diag(solve(fit$hessian))) Z = B/se p = ifelse(Z &gt;= 0, pnorm(Z, lower = FALSE)*2, pnorm(Z)*2) summary_table = round(data.frame(B, se, Z, p), 3) list(summary = summary_table, ll = fit$value) $summary B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.761 0.078 logit.livebait 0.711 0.403 1.766 0.077 pois.(Intercept) -2.057 0.341 -6.035 0.000 pois.persons 0.750 0.043 17.378 0.000 pois.livebait 1.851 0.307 6.023 0.000 $ll [1] 882.2514 Comparison Compare to hurdle from pscl package. library(pscl) fit_pscl = hurdle( count ~ persons + livebait, data = fish, zero.dist = &quot;binomial&quot;, dist = &quot;poisson&quot; ) coef B se Z p pscl X.Intercept. -2.057 0.341 -6.035 0.000 pscl persons 0.750 0.043 17.378 0.000 pscl livebait 1.851 0.307 6.023 0.000 pscl X.Intercept..1 -1.417 0.491 -2.888 0.004 pscl persons.1 0.206 0.117 1.762 0.078 pscl livebait.1 0.711 0.403 1.765 0.077 hurdle_poisson_ll logit.(Intercept) -1.417 0.491 -2.888 0.004 hurdle_poisson_ll logit.persons 0.206 0.117 1.761 0.078 hurdle_poisson_ll logit.livebait 0.711 0.403 1.766 0.077 hurdle_poisson_ll pois.(Intercept) -2.057 0.341 -6.035 0.000 hurdle_poisson_ll pois.persons 0.750 0.043 17.378 0.000 hurdle_poisson_ll pois.livebait 1.851 0.307 6.023 0.000 Negative Binomial Function The likelihood function. hurdle_nb_ll &lt;- function(y, X, par) { # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] NegBinpars = par[grep(&#39;NegBin&#39;, names(par))] theta = exp(par[grep(&#39;theta&#39;, names(par))]) # Logit model part Xlogit = X ylogit = ifelse(y == 0, 0, 1) LPlogit = Xlogit%*%logitpars mulogit = plogis(LPlogit) # Calculate the likelihood logliklogit = -sum( ylogit*log(mulogit) + (1 - ylogit)*log(1 - mulogit) ) #NB part XNB = X[y &gt; 0, ] yNB = y[y &gt; 0] muNB = exp(XNB %*% NegBinpars) # Calculate the likelihood loglik0 = dnbinom(0, mu = muNB, size = theta, log = TRUE) loglik1 = dnbinom(yNB, mu = muNB, size = theta, log = TRUE) loglikNB = -( sum(loglik1) - sum(log(1 - exp(loglik0))) ) # combine likelihoods loglik = loglikNB + logliklogit loglik } Estimation starts = c( logit = coef(init_mod), NegBin = coef(init_mod), theta = 1 ) optNB1 = optim( par = starts, fn = hurdle_nb_ll, X = init_mod$x, y = init_mod$y, control = list(maxit = 5000, reltol = 1e-12), method = &quot;BFGS&quot;, hessian = TRUE ) # optNB1 B = optNB1$par se = sqrt(diag(solve(optNB1$hessian))) Z = B/se p = ifelse(Z &gt;= 0, pnorm(Z, lower = FALSE)*2, pnorm(Z)*2) summary_table = round(data.frame(B, se, Z, p), 3) list(summary = summary_table, ll = optNB1$value) $summary B se Z p logit.(Intercept) -1.417 0.491 -2.888 0.004 logit.persons 0.206 0.117 1.762 0.078 logit.livebait 0.711 0.403 1.765 0.077 NegBin.(Intercept) -3.461 0.869 -3.984 0.000 NegBin.persons 0.941 0.153 6.154 0.000 NegBin.livebait 1.985 0.639 3.109 0.002 theta -1.301 0.576 -2.257 0.024 $ll [1] 439.3686 Comparison fit_pscl = hurdle( count ~ persons + livebait, data = fish, zero.dist = &quot;binomial&quot;, dist = &quot;negbin&quot; ) # summary(fit_pscl)$coefficients # summary_table coef B se Z p pscl X.Intercept. -3.461 0.869 -3.984 0.000 pscl persons 0.941 0.153 6.154 0.000 pscl livebait 1.985 0.639 3.109 0.002 pscl Log.theta. -1.301 0.576 -2.257 0.024 pscl X.Intercept..1 -1.417 0.491 -2.888 0.004 pscl persons.1 0.206 0.117 1.762 0.078 pscl livebait.1 0.711 0.403 1.765 0.077 hurdle_nb_ll logit.(Intercept) -1.417 0.491 -2.888 0.004 hurdle_nb_ll logit.persons 0.206 0.117 1.762 0.078 hurdle_nb_ll logit.livebait 0.711 0.403 1.765 0.077 hurdle_nb_ll NegBin.(Intercept) -3.461 0.869 -3.984 0.000 hurdle_nb_ll NegBin.persons 0.941 0.153 6.154 0.000 hurdle_nb_ll NegBin.livebait 1.985 0.639 3.109 0.002 hurdle_nb_ll theta -1.301 0.576 -2.257 0.024 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hurdle.R "],["zi.html", "Zero-Inflated Model Poisson Negative Binomial Supplemental Example Source", " Zero-Inflated Model Log likelihood function to estimate parameters for a Zero-inflated Poisson model. With examples and comparison to pscl package output. Also includes approach based on Hilbe GLM text. Zero-inflated models are applied to situations in which target data has relatively many of one value, usually zero, to go along with the other observed values. They are two-part models, a logistic model for whether an observation is zero or not, and a count model for the other part. The key distinction from hurdle count models is that the count distribution contributes to the excess zeros. While the typical application is count data, the approach can be applied to any distribution in theory. Poisson Data Setup Get the data. library(tidyverse) fish = haven::read_dta(&quot;http://www.stata-press.com/data/r11/fish.dta&quot;) Function The log likelihood function. zip_ll &lt;- function(y, X, par) { # arguments are response y, predictor matrix X, and parameter named starting points of &#39;logit&#39; and &#39;pois&#39; # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] poispars = par[grep(&#39;pois&#39;, names(par))] # Logit part; in this function Xlogit = Xpois but one could split X argument into Xlogi and Xpois for example Xlogit = X LPlogit = Xlogit %*% logitpars logi0 = plogis(LPlogit) # alternative 1/(1+exp(-LPlogit)) # Poisson part Xpois = X mupois = exp(Xpois %*% poispars) # LLs logliklogit = log( logi0 + exp(log(1 - logi0) - mupois) ) loglikpois = log(1 - logi0) + dpois(y, lambda = mupois, log = TRUE) # Hilbe formulation # logliklogit = log(logi0 + (1 - logi0)*exp(- mupois) ) # loglikpois = log(1-logi0) -mupois + log(mupois)*y #not necessary: - log(gamma(y+1)) y0 = y == 0 # 0 values yc = y &gt; 0 # Count part loglik = sum(logliklogit[y0]) + sum(loglikpois[yc]) -loglik } Estimation Get starting values or simply do zeros. # for zip: need &#39;logit&#39;, &#39;pois&#39; initial_model = glm( count ~ persons + livebait, data = fish, x = TRUE, y = TRUE, &quot;poisson&quot; ) # starts = c(logit = coef(initial_model), pois = coef(initial_model)) starts = c(rep(0, 3), rep(0, 3)) names(starts) = c(paste0(&#39;pois.&#39;, names(coef(initial_model))), paste0(&#39;logit.&#39;, names(coef(initial_model)))) Estimate with optim. fit = optim( par = starts , fn = zip_ll, X = initial_model$x, y = initial_model$y, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit Comparison Extract for clean display. B = fit$par se = sqrt(diag(solve((fit$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 Results from pscl. library(pscl) fit_pscl = zeroinfl(count ~ persons + livebait, data = fish, dist = &quot;poisson&quot;) Compare. coef B se Z p pscl X.Intercept. -2.006 0.324 -6.196 0.000 pscl persons 0.747 0.043 17.516 0.000 pscl livebait 1.809 0.292 6.195 0.000 pscl X.Intercept..1 0.303 0.674 0.449 0.654 pscl persons.1 -0.069 0.129 -0.537 0.591 pscl livebait.1 -0.031 0.558 -0.056 0.956 zip_poisson_ll pois.(Intercept) -2.006 0.324 -6.196 0.000 zip_poisson_ll pois.persons 0.747 0.043 17.516 0.000 zip_poisson_ll pois.livebait 1.809 0.292 6.195 0.000 zip_poisson_ll logit.(Intercept) 0.303 0.674 0.449 0.654 zip_poisson_ll logit.persons -0.069 0.129 -0.537 0.591 zip_poisson_ll logit.livebait -0.031 0.558 -0.056 0.956 Negative Binomial Function zinb_ll &lt;- function(y, X, par) { # arguments are response y, predictor matrix X, and parameter named starting points of &#39;logit&#39;, &#39;negbin&#39;, and &#39;theta&#39; # Extract parameters logitpars = par[grep(&#39;logit&#39;, names(par))] negbinpars = par[grep(&#39;negbin&#39;, names(par))] theta = exp(par[grep(&#39;theta&#39;, names(par))]) # Logit part; in this function Xlogit = Xnegbin but one could split X argument into Xlogit and Xnegbin for example Xlogit = X LPlogit = Xlogit %*% logitpars logi0 = plogis(LPlogit) # Negbin part Xnegbin = X munb = exp(Xnegbin %*% negbinpars) # LLs logliklogit = log( logi0 + exp(log(1 - logi0) + suppressWarnings(dnbinom(0, size = theta, mu = munb, log = TRUE))) ) logliknegbin = log(1 - logi0) + suppressWarnings(dnbinom(y, size = theta, mu = munb, log = TRUE)) # Hilbe formulation # theta part # alpha = 1/theta # m = 1/alpha # p = 1/(1 + alpha*munb) # logliklogit = log( logi0 + (1 - logi0)*(p^m) ) # logliknegbin = log(1-logi0) + log(gamma(m+y)) - log(gamma(m)) + m*log(p) + y*log(1-p) # gamma(y+1) not needed y0 = y == 0 # 0 values yc = y &gt; 0 # Count part loglik = sum(logliklogit[y0]) + sum(logliknegbin[yc]) -loglik } Estimation Get starting values or simply do zeros. # for zinb: &#39;logit&#39;, &#39;negbin&#39;, &#39;theta&#39; initial_model = model.matrix(count ~ persons + livebait, data = fish) # to get X matrix startlogi = glm(count == 0 ~ persons + livebait, data = fish, family = &quot;binomial&quot;) startcount = glm(count ~ persons + livebait, data = fish, family = &quot;poisson&quot;) starts = c( negbin = coef(startcount), logit = coef(startlogi), theta = 1 ) # starts = c(negbin = rep(0, 3), # logit = rep(0, 3), # theta = log(1)) Estimate with optim. fit_nb = optim( par = starts , fn = zinb_ll, X = initial_model, y = fish$count, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit_nb Comparison Extract for clean display. B = fit_nb$par se = sqrt(diag(solve((fit_nb$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 Results from pscl. # pscl results library(pscl) fit_pscl = zeroinfl(count ~ persons + livebait, data = fish, dist = &quot;negbin&quot;) Compare. coef B se Z p pscl X.Intercept. -2.803 0.558 -5.026 0.000 pscl persons 0.849 0.124 6.833 0.000 pscl livebait 1.791 0.511 3.504 0.000 pscl Log.theta. -0.969 0.302 -3.206 0.001 pscl X.Intercept..1 -4.276 4.278 -1.000 0.318 pscl persons.1 0.560 0.517 1.084 0.279 pscl livebait.1 1.168 3.661 0.319 0.750 zinb_ll negbin.(Intercept) -2.803 0.558 -5.026 0.000 zinb_ll negbin.persons 0.849 0.124 6.834 0.000 zinb_ll negbin.livebait 1.791 0.511 3.504 0.000 zinb_ll logit.(Intercept) -4.276 4.278 -1.000 0.318 zinb_ll logit.persons 0.560 0.517 1.084 0.279 zinb_ll logit.livebait 1.168 3.661 0.319 0.750 zinb_ll theta -0.969 0.302 -3.206 0.001 Supplemental Example This supplemental example uses the bioChemists data. It contains a sample of 915 biochemistry graduate students with the following information: art: count of articles produced during last 3 years of Ph.D. fem: factor indicating gender of student, with levels Men and Women mar: factor indicating marital status of student, with levels Single and Married kid5: number of children aged 5 or younger phd: prestige of Ph.D. department ment: count of articles produced by Ph.D. mentor during last 3 years data(&quot;bioChemists&quot;, package = &quot;pscl&quot;) initial_model = model.matrix(art ~ fem + mar + kid5 + phd + ment, data = bioChemists) # to get X matrix startlogi = glm(art==0 ~ fem + mar + kid5 + phd + ment, data = bioChemists, family = &quot;binomial&quot;) startcount = glm(art ~ fem + mar + kid5 + phd + ment, data = bioChemists, family = &quot;quasipoisson&quot;) starts = c( negbin = coef(startcount), logit = coef(startlogi), theta = summary(startcount)$dispersion ) # starts = c(negbin = rep(0, 6), # logit = rep(0, 6), # theta = 1) fit_nb_pub = optim( par = starts , fn = zinb_ll, X = initial_model, y = bioChemists$art, method = &quot;BFGS&quot;, control = list(maxit = 5000, reltol = 1e-12), hessian = TRUE ) # fit_nb_pub B = fit_nb_pub$par se = sqrt(diag(solve((fit_nb_pub$hessian)))) Z = B/se p = pnorm(abs(Z), lower = FALSE)*2 library(pscl) fit_pscl = zeroinfl(art ~ . | ., data = bioChemists, dist = &quot;negbin&quot;) summary(fit_pscl)$coefficients $count Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.4167465901 0.143596450 2.90220678 3.705439e-03 femWomen -0.1955076374 0.075592558 -2.58633447 9.700275e-03 marMarried 0.0975826042 0.084451953 1.15548073 2.478936e-01 kid5 -0.1517320709 0.054206071 -2.79917119 5.123397e-03 phd -0.0006997593 0.036269674 -0.01929323 9.846072e-01 ment 0.0247861500 0.003492672 7.09661548 1.278491e-12 Log(theta) 0.9763577454 0.135469554 7.20721163 5.710921e-13 $zero Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.19160645 1.3227962 -0.1448496 0.884829645 femWomen 0.63587048 0.8488959 0.7490559 0.453823498 marMarried -1.49943716 0.9386562 -1.5974296 0.110169987 kid5 0.62840922 0.4427746 1.4192531 0.155825245 phd -0.03773288 0.3080059 -0.1225070 0.902497523 ment -0.88227364 0.3162186 -2.7900755 0.005269575 round(data.frame(B, se, Z, p), 4) B se Z p negbin.(Intercept) 0.4167 0.1436 2.9021 0.0037 negbin.femWomen -0.1955 0.0756 -2.5863 0.0097 negbin.marMarried 0.0976 0.0845 1.1555 0.2479 negbin.kid5 -0.1517 0.0542 -2.7992 0.0051 negbin.phd -0.0007 0.0363 -0.0195 0.9845 negbin.ment 0.0248 0.0035 7.0967 0.0000 logit.(Intercept) -0.1916 1.3229 -0.1449 0.8848 logit.femWomen 0.6359 0.8490 0.7491 0.4538 logit.marMarried -1.4995 0.9387 -1.5974 0.1102 logit.kid5 0.6284 0.4428 1.4192 0.1558 logit.phd -0.0377 0.3080 -0.1225 0.9025 logit.ment -0.8823 0.3162 -2.7901 0.0053 theta 0.9763 0.1355 7.2071 0.0000 Source Original code for zip_ll found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/poiszeroinfl.R Original code for zinb_ll found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/NBzeroinfl.R "],["naive-bayes.html", "Naive Bayes Initialization Comparison Source", " Naive Bayes Initialization Demo for binary data. First we generate some data. We have several binary covariates and a binary target variable y. library(tidyverse) set.seed(123) x = matrix(sample(0:1, 50, replace = TRUE), ncol = 5) xf = map(data.frame(x), factor) y = sample(0:1, 10, prob = c(.25, .75), replace = TRUE) Comparison We can use e1071 for comparison. library(e1071) m = naiveBayes(xf, y) m Naive Bayes Classifier for Discrete Predictors Call: naiveBayes.default(x = xf, y = y) A-priori probabilities: y 0 1 0.3 0.7 Conditional probabilities: X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Using base R for our model, we can easily obtain the ‘predictions’… map(xf, function(var) t(prop.table(table(&#39; &#39; = var, y), margin = 2))) $X1 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X2 y 0 1 0 0.6666667 0.3333333 1 0.4285714 0.5714286 $X3 y 0 1 0 0.6666667 0.3333333 1 0.5714286 0.4285714 $X4 y 0 1 0 1.0000000 0.0000000 1 0.4285714 0.5714286 $X5 y 0 1 0 0.3333333 0.6666667 1 0.8571429 0.1428571 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/naivebayes.R "],["multinomial.html", "Multinomial Standard (Categorical) Model Alternative specific and constant variables Source", " Multinomial For more detail on these types of models, see my document. In general we can use multinomial models for multi-category target variables, or more generally, multi-count data. Standard (Categorical) Model Data Setup First, lets get some data. 200 entering high school students make program choices: general program, vocational program, and academic program. We will be interested in their choice, using their writing score as a proxy for scholastic ability and their socioeconomic status, a categorical variable of low, middle, and high values. library(haven) library(tidyverse) library(mlogit) program = read_dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;) %&gt;% as_factor() %&gt;% mutate(prog = relevel(prog, ref = &quot;academic&quot;)) head(program[, 1:5]) # A tibble: 6 x 5 id female ses schtyp prog &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 45 female low public vocation 2 108 male middle public general 3 15 male high public vocation 4 67 male low public vocation 5 153 male middle public vocation 6 51 female high public general # convert to long form for mlogit programLong = program %&gt;% select(id, prog, ses, write) %&gt;% mlogit.data( data = , shape = &#39;wide&#39;, choice = &#39;prog&#39;, id.var = &#39;id&#39; ) head(programLong) ~~~~~~~ first 10 observations out of 600 ~~~~~~~ id prog ses write chid alt idx 1 1 FALSE low 44 11 academic 11:emic 2 1 FALSE low 44 11 general 11:eral 3 1 TRUE low 44 11 vocation 11:tion 4 2 FALSE middle 41 9 academic 9:emic 5 2 FALSE middle 41 9 general 9:eral 6 2 TRUE middle 41 9 vocation 9:tion 7 3 TRUE low 65 159 academic 159:emic 8 3 FALSE low 65 159 general 159:eral 9 3 FALSE low 65 159 vocation 159:tion 10 4 TRUE low 50 30 academic 30:emic ~~~ indexes ~~~~ chid id alt 1 11 1 academic 2 11 1 general 3 11 1 vocation 4 9 2 academic 5 9 2 general 6 9 2 vocation 7 159 3 academic 8 159 3 general 9 159 3 vocation 10 30 4 academic indexes: 1, 1, 2 We go ahead and run a model via mlogit for later comparison. fit_mlogit = mlogit(prog ~ 1| write + ses, data = programLong) mlogit_coefs = coef(fit_mlogit)[c(1,5,7,3,2,6,8,4)] # reorder Function Multinomial model via maximum likelihood multinom_ml &lt;- function(par, X, y) { levs = levels(y) ref = levs[1] # reference level (category label 1) y0 = y == ref y1 = y == levs[2] # category 2 y2 = y == levs[3] # category 3 beta = matrix(par, ncol = 2) # more like mnlogit package depiction in its function # V1 = X %*% beta[ ,1] # V2 = X %*% beta[ ,2] # ll = sum(-log(1 + exp(V1) + exp(V2))) + sum(V1[y1], V2[y2]) V = X %*% beta # a vectorized approach baseProbVec = 1 / (1 + rowSums(exp(V))) # reference group probabilities loglik = sum(log(baseProbVec)) + crossprod(c(V), c(y1, y2)) loglik } fit = optim( runif(8,-.1, .1), multinom_ml, X = model.matrix(prog ~ ses + write, data = program), y = program$prog, control = list( maxit = 1000, reltol = 1e-12, ndeps = rep(1e-8, 8), trace = TRUE, fnscale = -1, type = 3 ), method = &#39;BFGS&#39; ) initial value 638.963532 iter 10 value 180.008322 final value 179.981726 converged # fit$par Comparison An initial comparison. fit_coefs mlogit_coefs (Intercept):general 2.8522 2.8522 sesmiddle:general -0.5333 -0.5333 seshigh:general -1.1628 -1.1628 write:general -0.0579 -0.0579 (Intercept):vocation 5.2182 5.2182 sesmiddle:vocation 0.2914 0.2914 seshigh:vocation -0.9827 -0.9827 write:vocation -0.1136 -0.1136 The following uses dmultinom for the likelihood, similar to other modeling demonstrations in this document. X = model.matrix(prog ~ ses + write, data = program) y = program$prog pars = matrix(fit$par, ncol = 2) V = X %*% pars acadprob = 1 / (1+rowSums(exp(V))) fitnonacad = exp(V) * matrix(rep(acadprob, 2), ncol = 2) fits = cbind(acadprob, fitnonacad) yind = model.matrix( ~ -1 + prog, data = program) # because dmultinom can&#39;t take matrix for prob ll = 0 for (i in 1:200){ ll = ll + dmultinom(yind[i, ], size = 1, prob = fits[i, ], log = TRUE) } ll [1] -179.9817 fit$value [1] -179.9817 logLik(fit_mlogit) &#39;log Lik.&#39; -179.9817 (df=8) Alternative specific and constant variables Now we add alternative specific and alternative constant variables to the previous individual specific covariates.. In this example, price is alternative invariant (Z) income is individual/alternative specific (X), and catch is alternative specific (Y). We can use the fish data from the mnlogit package. library(mnlogit) data(Fish) head(Fish) mode income alt price catch chid 1.beach FALSE 7083.332 beach 157.930 0.0678 1 1.boat FALSE 7083.332 boat 157.930 0.2601 1 1.charter TRUE 7083.332 charter 182.930 0.5391 1 1.pier FALSE 7083.332 pier 157.930 0.0503 1 2.beach FALSE 1250.000 beach 15.114 0.1049 2 2.boat FALSE 1250.000 boat 10.534 0.1574 2 fm = formula(mode ~ price | income | catch) fit_mnlogit = mnlogit(fm, Fish) # fit_mnlogit = mlogit(fm, Fish) # summary(fit_mnlogit) The likelihood function. multinom_ml &lt;- function(par, X, Y, Z, respVec, choice) { # Args- # X dim nrow(Fish)/K x p + 1 (intercept) # Z, Y nrow(N); Y has alt specific coefs; then for Z ref group dropped so nrow = nrow*(K-1)/K # for ll everything through previous X the same # then calc probmat for Y and Z, add to X probmat, and add to base N = sum(choice) K = length(unique(respVec)) levs = levels(respVec) xpar = matrix(par[1:6], ncol = K-1) ypar = matrix(par[7:10], ncol = K) zpar = matrix(par[length(par)], ncol = 1) # Calc X Vx = X %*% xpar # Calc Y (mnlogit finds N x 1 results by going through 1:N, N+1:N*2 etc; then # makes 1 vector, then subtracts the first 1:N from whole vector, then makes # Nxk-1 matrix with N+1:end values (as 1:N are just zero)); creating the # vector and rebuilding the matrix is unnecessary though Vy = sapply(1:K, function(alt) Y[respVec == levs[alt], , drop = FALSE] %*% ypar[alt]) Vy = Vy[,-1] - Vy[,1] # Calc Z Vz = Z %*% zpar Vz = matrix(Vz, ncol = 3) # all Vs must fit into N x K -1 matrix where N is nobs (i.e. individuals) V = Vx + Vy + Vz ll0 = crossprod(c(V), choice[-(1:N)]) baseProbVec &lt;- 1 / (1 + rowSums(exp(V))) loglik = sum(log(baseProbVec)) + ll0 loglik # note fitted values via # fitnonref = exp(V) * matrix(rep(baseProbVec, K-1), ncol = K-1) # fitref = 1-rowSums(fitnonref) # fits = cbind(fitref, fitnonref) } inits = runif(11, -.1, .1) mdat = mnlogit(fm, Fish)$model # this data already ordered! As X has a constant value across alternatives, the coefficients regard the selection of the alternative relative to reference. X = cbind(1, mdat[mdat$`_Alt_Indx_` == &#39;beach&#39;, &#39;income&#39;]) dim(X) [1] 1182 2 head(X) [,1] [,2] [1,] 1 7083.332 [2,] 1 1250.000 [3,] 1 3750.000 [4,] 1 2083.333 [5,] 1 4583.332 [6,] 1 4583.332 Y will use the complete data to start. Coefficients will be differences from the reference alternative coefficient. Y = as.matrix(mdat[, &#39;catch&#39;, drop = FALSE]) dim(Y) [1] 4728 1 Z are difference scores from reference group. Z = as.matrix(mdat[mdat$`_Alt_Indx_` != &#39;beach&#39;, &#39;price&#39;, drop = FALSE]) Z = Z - mdat[mdat$`_Alt_Indx_` == &#39;beach&#39;, &#39;price&#39;] dim(Z) [1] 3546 1 respVec = mdat$`_Alt_Indx_` # first 10 should be 0 0 1 0 1 0 0 0 1 1 after beach dropped multinom_ml(inits, X, Y, Z, respVec, choice = mdat$mode) [,1] [1,] -162384.5 fit = optim( par = rep(0, 11), multinom_ml, X = X, Y = Y, Z = Z, respVec = respVec, choice = mdat$mode, control = list( maxit = 1000, reltol = 1e-12, ndeps = rep(1e-8, 11), trace = TRUE, fnscale = -1, type = 3 ), method = &#39;BFGS&#39; ) initial value 1638.599935 iter 10 value 1253.603448 iter 20 value 1199.143447 final value 1199.143445 converged Comparison Compare fits. fit_coefs mnlogit_coefs 0.842 0.842 0.000 0.000 2.155 2.155 0.000 0.000 1.043 1.043 0.000 0.000 3.118 3.118 2.542 2.542 0.759 0.759 2.851 2.851 -0.025 -0.025 fit_ll mnlogit_ll -1199.143 -1199.143 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/multinomial.R "],["ordinal.html", "Ordinal Data Function Estimation Comparison Source", " Ordinal The following demonstrates a standard cumulative link ordinal regression model via maximum likelihood. Default is with probit link function. Alternatively you can compare it with a logit link, which will result in values roughly 1.7*parameters estimates from the probit. Data This data generation is from the probit perspective, where the underlying continuous latent variable is normally distributed. library(tidyverse) set.seed(808) N = 1000 # Sample size x = cbind(x1 = rnorm(N), x2 = rnorm(N)) # predictor variables beta = c(1,-1) # coefficients y_star = rnorm(N, mean = x %*% beta) # the underlying latent variable y_1 = y_star &gt; -1.5 # -1.50 first cutpoint y_2 = y_star &gt; .75 # 0.75 second cutpoint y_3 = y_star &gt; 1.75 # 1.75 third cutpoint y = y_1 + y_2 + y_3 + 1 # target table(y) y 1 2 3 4 175 495 182 148 d = data.frame(x, y = factor(y)) Function ll_ord &lt;- function(par, X, y, probit = TRUE) { K = length(unique(y)) # number of classes K ncuts = K-1 # number of cutpoints/thresholds cuts = par[(1:ncuts)] # cutpoints beta = par[-(1:ncuts)] # regression coefficients lp = X %*% beta # linear predictor ll = rep(0, length(y)) # log likelihood pfun = ifelse(probit, pnorm, plogis) # which link to use for(k in 1:K){ if (k==1) { ll[y==k] = pfun((cuts[k] - lp[y==k]), log = TRUE) } else if (k &lt; K) { ll[y==k] = log(pfun(cuts[k] - lp[y==k]) - pfun(cuts[k-1] - lp[y==k])) } else { ll[y==k] = log(1 - pfun(cuts[k-1] - lp[y==k])) } } -sum(ll) } Estimation init = c(-1, 1, 2, 0, 0) # initial values fit_probit = optim( init, ll_ord, y = y, X = x, probit = TRUE, control = list(reltol = 1e-10) ) fit_logit = optim( init, ll_ord, y = y, X = x, probit = FALSE, control = list(reltol = 1e-10) ) Comparison We can compare our results with the ordinal package. library(ordinal) fit_ordpack_probit = clm(y ~ x1 + x2, data = d, link = &#39;probit&#39;) fit_ordpack_logit = clm(y ~ x1 + x2, data = d, link = &#39;logit&#39;) method cut_1 cut_2 cut_3 beta1 beta2 probit ll_ord -1.609 0.731 1.798 1.019 -1.051 probit ordpack -1.609 0.731 1.798 1.019 -1.051 logit ll_ord -2.872 1.284 3.168 1.806 -1.877 logit ordpack -2.872 1.284 3.168 1.806 -1.877 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ordinal_regression.R "],["markov.html", "Markov Model Data Setup Function Estimation Comparison Source", " Markov Model Here we demonstrate a Markov model. We start by showing how to create some data and estimate such a model via the markovchain package. You may want to play with it to get a better feel for how it works, as we will use it for comparison later. library(tidyverse) library(markovchain) A = matrix(c(.7, .3, .9, .1), nrow = 2, byrow = TRUE) dtmcA = new( &#39;markovchain&#39;, transitionMatrix = A, states = c(&#39;a&#39;, &#39;b&#39;), name = &#39;MarkovChain A&#39; ) dtmcA MarkovChain A A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.7 0.3 b 0.9 0.1 plot(dtmcA) transitionProbability(dtmcA, &#39;b&#39;, &#39;b&#39;) [1] 0.1 initialState = c(0, 1) steps = 4 finalState = initialState * dtmcA ^ steps #using power operator finalState a b [1,] 0.7488 0.2512 steadyStates(dtmcA) a b [1,] 0.75 0.25 observed_states = sample(c(&#39;a&#39;, &#39;b&#39;), 50, c(.7, .3), replace = TRUE) createSequenceMatrix(observed_states) a b a 24 11 b 11 3 markovchainFit(observed_states) $estimate MLE Fit A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.6857143 0.3142857 b 0.7857143 0.2142857 $standardError a b a 0.1399708 0.09476071 b 0.2369018 0.12371791 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b a 0.4113764 0.1285581 b 0.3213953 0.0000000 $upperEndpointMatrix a b a 0.9600522 0.5000133 b 1.0000000 0.4567684 $logLikelihood [1] -29.06116 Data Setup Data Functions A recursive function to take a matrix power. mat_power &lt;- function(M, N) { if (N == 1) return(M) M %*% mat_power(M, N - 1) } A function to create a sequence. create_sequence &lt;- function(states, len, tmat) { # states: number of states # len: length of sequence # tmat: the transition matrix states_numeric = length(unique(states)) out = numeric(len) out[1] = sample(states_numeric, 1, prob = colMeans(tmat)) # initial state for (i in 2:len){ out[i] = sample(states_numeric, 1, prob = tmat[out[i - 1], ]) } states[out] } # example test_matrix = matrix(rep(2, 4), nrow = 2) test_matrix [,1] [,2] [1,] 2 2 [2,] 2 2 mat_power(test_matrix, 2) [,1] [,2] [1,] 8 8 [2,] 8 8 # transition matrix A = matrix(c(.7, .3, .4, .6), nrow = 2, byrow = TRUE) mat_power(A, 10) [,1] [,2] [1,] 0.5714311 0.4285689 [2,] 0.5714252 0.4285748 Two states Demo Note that a notably long sequence is needed to get close to recovering the true transition matrix. A = matrix(c(.7, .3, .9, .1), nrow = 2, byrow = TRUE) observed_states = create_sequence(c(&#39;a&#39;, &#39;b&#39;), 500, tmat = A) createSequenceMatrix(observed_states) a b a 288 100 b 101 10 prop.table(createSequenceMatrix(observed_states), 1) a b a 0.7422680 0.25773196 b 0.9099099 0.09009009 fit = markovchainFit(observed_states) fit $estimate MLE Fit A 2 - dimensional discrete Markov Chain defined by the following states: a, b The transition matrix (by rows) is defined as follows: a b a 0.7422680 0.25773196 b 0.9099099 0.09009009 $standardError a b a 0.04373856 0.02577320 b 0.09053942 0.02848899 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b a 0.6565420 0.20721741 b 0.7324559 0.03425269 $upperEndpointMatrix a b a 0.8279941 0.3082465 b 1.0000000 0.1459275 $logLikelihood [1] -255.0253 # log likelihood sum(createSequenceMatrix(observed_states) * log(fit$estimate@transitionMatrix)) [1] -255.0253 Three states demo A = matrix( c(.70, .20, .10, .20, .40, .40, .05, .05, .90), nrow = 3, byrow = TRUE ) observed_states = create_sequence(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), 500, tmat = A) createSequenceMatrix(observed_states) a b c a 92 20 11 b 11 24 22 c 20 13 286 prop.table(createSequenceMatrix(observed_states), 1) a b c a 0.74796748 0.16260163 0.08943089 b 0.19298246 0.42105263 0.38596491 c 0.06269592 0.04075235 0.89655172 markovchainFit(observed_states) $estimate MLE Fit A 3 - dimensional discrete Markov Chain defined by the following states: a, b, c The transition matrix (by rows) is defined as follows: a b c a 0.74796748 0.16260163 0.08943089 b 0.19298246 0.42105263 0.38596491 c 0.06269592 0.04075235 0.89655172 $standardError a b c a 0.07798100 0.03635883 0.02696443 b 0.05818640 0.08594701 0.08228800 c 0.01401923 0.01130267 0.05301421 $confidenceLevel [1] 0.95 $lowerEndpointMatrix a b c a 0.59512750 0.09133962 0.03658157 b 0.07893918 0.25259956 0.22468337 c 0.03521872 0.01859952 0.79264575 $upperEndpointMatrix a b c a 0.90080746 0.23386364 0.1422802 b 0.30702573 0.58950571 0.5472465 c 0.09017313 0.06290518 1.0000000 $logLikelihood [1] -277.6268 Function Now we create a function to calculate the (negative) log likelihood. markov_ll &lt;- function(par, x) { # par should be the c(A) of transition probabilities A nstates = length(unique(x)) # create transition matrix par = matrix(par, ncol = nstates) par = t(apply(par, 1, function(x) x / sum(x))) # create seq matrix seq_mat = table(x[-length(x)], x[-1]) # calculate log likelihood ll = sum(seq_mat * log(par)) -ll } A = matrix( c(.70, .20, .10, .40, .20, .40, .10, .15, .75), nrow = 3, byrow = TRUE ) observed_states = create_sequence(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), 1000, tmat = A) Estimation Note that initial state values will be transformed to rowsum to one, so the specific initial values don’t matter (i.e. they don’t have to be probabilities). With the basic optim approach, sometimes log(0) will occur and produce a warning. Can be ignored, or use LFBGS as demonstrated at the end. initpar = rep(1, 9) fit = optim( par = initpar, fn = markov_ll, x = observed_states, method = &#39;BFGS&#39;, control = list(reltol = 1e-12) ) # get estimates on prob scale est_mat = matrix(fit$par, ncol = 3) est_mat = t(apply(est_mat, 1, function(x) x / sum(x))) Comparison Compare with markovchain package. fit_compare = markovchainFit(observed_states) # compare log likelihood c(-fit$value, fit_compare$logLikelihood) [1] -815.6466 -815.6466 # compare estimated transition matrix list( `Estimated via optim` = est_mat, `markovchain Package` = fit_compare$estimate@transitionMatrix, `Analytical Solution` = prop.table( table(observed_states[-length(observed_states)], observed_states[-1]) , 1) ) %&gt;% purrr::map(round, 3) $`Estimated via optim` [,1] [,2] [,3] [1,] 0.674 0.242 0.084 [2,] 0.462 0.126 0.412 [3,] 0.106 0.151 0.743 $`markovchain Package` a b c a 0.674 0.242 0.084 b 0.462 0.126 0.412 c 0.106 0.151 0.743 $`Analytical Solution` a b c a 0.674 0.242 0.084 b 0.462 0.126 0.412 c 0.106 0.151 0.743 Visualize. plot( new( &#39;markovchain&#39;, transitionMatrix = est_mat, states = c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;), name = &#39;Estimated Markov Chain&#39; ) ) If you don’t want warnings due to zeros use constraints (?constrOptim). fit = optim( par = initpar, fn = markov_ll, x = observed_states, method = &#39;L-BFGS&#39;, lower = rep(1e-20, length(initpar)), control = list(pgtol = 1e-12) ) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/markov_model.R "],["hmm.html", "Hidden Markov Model Data Setup Function Estimation Supplemental demo Source", " Hidden Markov Model This function duplicates hmm_viterbi.py, which comes from the Viterbi algorithm wikipedia page (at least as it was when I stumbled across it). This first function is just to provide R code that is similar, in case anyone is interested in a more direct comparison, but the original used lists of tuples and thus was very inefficient R-wise, and provided output that wasn’t succinct. The second function takes a vectorized approach and returns a matrix in a much more straightforward fashion. Both will provide the same result as the Python code. See The Markov Model chapter also. Data Setup library(tidyverse) obs = c(&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) # observed state states = c(&#39;Healthy&#39;, &#39;Fever&#39;) # latent states start_p = c(&#39;Healthy&#39; = 0.6, &#39;Fever&#39; = 0.4) # starting probabilities # transition matrix trans_p = list( &#39;Healthy&#39; = c(&#39;Healthy&#39; = 0.7, &#39;Fever&#39; = 0.3), &#39;Fever&#39; = c(&#39;Healthy&#39; = 0.4, &#39;Fever&#39; = 0.6) ) # emission matrix emit_p = list( &#39;Healthy&#39; = c(&#39;normal&#39; = 0.5, &#39;cold&#39; = 0.4, &#39;dizzy&#39; = 0.1), &#39;Fever&#39; = c(&#39;normal&#39; = 0.1, &#39;cold&#39; = 0.3, &#39;dizzy&#39; = 0.6) ) Function This first function takes a Python-esque approach in the manner of the original, allowing for easier comparison. viterbi &lt;- function(obs, states, start_p, trans_p, emit_p) { V = vector(&#39;list&#39;, length(obs)) for (st in seq_along(states)) { V[[1]][[states[st]]] = list(&quot;prob&quot; = start_p[st] * emit_p[[st]][obs[1]], &quot;prev&quot; = NULL) } for (t in 2:length(obs)) { for (st in seq_along(states)) { max_tr_prob = numeric() for (prev_st in states) { max_tr_prob[prev_st] = V[[t-1]][[prev_st]][[&quot;prob&quot;]] * trans_p[[prev_st]][[st]] } max_tr_prob = max(max_tr_prob) for (prev_st in states) { flag = V[[t-1]][[prev_st]][[&quot;prob&quot;]] * trans_p[[prev_st]][[st]] == max_tr_prob if (flag) { max_prob = max_tr_prob * emit_p[[st]][obs[t]] V[[t]][[states[st]]] = list(&#39;prob&#39; = max_prob, &#39;prev&#39; = prev_st) } } } } # I don&#39;t bother duplicating the text output code of the original df_out = rbind( Healthy = sapply(V, function(x) x$Healthy$prob), Fever = sapply(V, function(x) x$Fever$prob) ) colnames(df_out) = obs print(df_out) m = paste0( &#39;The steps of states are: &#39;, paste(rownames(df_out)[apply(df_out, 2, which.max)], collapse = &#39; &#39;), paste(&#39;\\nHighest probability: &#39;, max(df_out[, ncol(df_out)])) ) message(m) V } This approach is much more R-like. viterbi_2 &lt;- function(obs, states, start_p, trans_mat, emit_mat) { prob_mat = matrix(NA, nrow = length(states), ncol = length(obs)) colnames(prob_mat) = obs rownames(prob_mat) = states prob_mat[,1] = start_p * emit_mat[,1] for (t in 2:length(obs)) { prob_tran = prob_mat[,t-1] * trans_mat max_tr_prob = apply(prob_tran, 2, max) prob_mat[,t] = max_tr_prob * emit_mat[, obs[t]] } print(prob_mat) m = paste0( &#39;The steps of states are: &#39;, paste(states[apply(prob_mat, 2, which.max)], collapse = &#39; &#39;), paste(&#39;\\nHighest probability: &#39;, max(prob_mat[, ncol(prob_mat)])) ) message(m) } Estimation First we demo the initial function. test = viterbi( obs, states, start_p, trans_p, emit_p ) normal cold dizzy Healthy 0.30 0.084 0.00588 Fever 0.04 0.027 0.01512 # test set.seed(123) obs = sample(obs, 6, replace = TRUE) test = viterbi( obs, states, start_p, trans_p, emit_p ) dizzy dizzy dizzy cold dizzy cold Healthy 0.06 0.0096 0.003456 0.00497664 0.0003483648 0.0003224863 Fever 0.24 0.0864 0.031104 0.00559872 0.0020155392 0.0003627971 # test Now the vectorized approach. set.seed(123) obs = c(&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) obs = sample(obs, 6, replace = T) # need matrices now emit_mat = do.call(rbind, emit_p) trans_mat = do.call(rbind, trans_p) viterbi_2( obs, states, start_p, trans_mat, emit_mat ) dizzy dizzy dizzy cold dizzy cold Healthy 0.30 0.021 0.00216 0.0031104 0.000217728 0.0002015539 Fever 0.04 0.054 0.01944 0.0034992 0.001259712 0.0002267482 Supplemental demo This example comes from the hidden markov model wikipedia page. states = c(&#39;Rainy&#39;, &#39;Sunny&#39;) observations = c(&#39;walk&#39;, &#39;shop&#39;, &#39;clean&#39;) start_probability = c(&#39;Rainy&#39; = 0.6, &#39;Sunny&#39; = 0.4) transition_probability = rbind( &#39;Rainy&#39; = c(&#39;Rainy&#39; = 0.7, &#39;Sunny&#39; = 0.3), &#39;Sunny&#39; = c(&#39;Rainy&#39; = 0.4, &#39;Sunny&#39; = 0.6) ) emission_probability = rbind( &#39;Rainy&#39; = c(&#39;walk&#39; = 0.1, &#39;shop&#39; = 0.4, &#39;clean&#39; = 0.5), &#39;Sunny&#39; = c(&#39;walk&#39; = 0.6, &#39;shop&#39; = 0.3, &#39;clean&#39; = 0.1) ) viterbi_2( observations, states, start_probability, transition_probability, emission_probability ) walk shop clean Rainy 0.06 0.0384 0.013440 Sunny 0.24 0.0432 0.002592 Source Original code for R found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hmm_viterbi.R Original code for Python found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/hmm_viterbi.py "],["quantile-regression.html", "Quantile Regression Data Setup Function Estimation Comparison Visualize Source", " Quantile Regression Data Setup We’ll use the quantreg package for comparison, and the classic data set on Belgian household income and food expenditure. Scale income if you want a meaningful ‘centercept’. library(tidyverse) library(quantreg) data(engel) # engel$income = scale(engel$income) X = cbind(1, engel$income) colnames(X) = c(&#39;Intercept&#39;, &#39;income&#39;) Function Loss function. It really is this simple. qreg &lt;- function(par, X, y, tau) { lp = X%*%par res = y - lp loss = ifelse(res &lt; 0 , -(1 - tau)*res, tau*res) sum(loss) } Estimation We’ll estimate the median to start. Compare optim output with quantreg package. optim( par = c(intercept = 0, income = 0), fn = qreg, X = X, y = engel$foodexp, tau = .5 )$par intercept income 81.4853550 0.5601706 rq(foodexp ~ income, tau = .5, data = engel) Call: rq(formula = foodexp ~ income, tau = 0.5, data = engel) Coefficients: (Intercept) income 81.4822474 0.5601806 Degrees of freedom: 235 total; 233 residual Other quantiles Now we will add additional quantiles to estimate. # quantiles qs = c(.05, .1, .25, .5, .75, .9, .95) fit_rq = coef(rq(foodexp ~ income, tau = qs, data = engel)) fit = map_df(qs, function(tau) data.frame(t( optim( par = c(intercept = 0, income = 0), fn = qreg, X = X, y = engel$foodexp, tau = tau )$par ))) Comparison Compare results. coef tau= 0.05 tau= 0.10 tau= 0.25 tau= 0.50 tau= 0.75 tau= 0.90 tau= 0.95 fit_rq X.Intercept. 124.880 110.142 95.484 81.482 62.397 67.351 64.104 fit_rq income 0.343 0.402 0.474 0.560 0.644 0.686 0.709 fit_est intercept 124.881 110.142 95.484 81.485 62.400 67.332 64.143 fit_est income.1 0.343 0.402 0.474 0.560 0.644 0.686 0.709 Visualize Let’s visualize the results. Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/quantile_regression.Rmd "],["cubic-spline.html", "Cubic Spline Model Data Setup Functions Example 1 Example 2 Source", " Cubic Spline Model See Wood (2017) Generalized Additive Models or my document for an introduction to generalized additive models. Data Setup The data regards engine wear index versus engine capacity for 19 Volvo car engines used. The idea is that a larger car engine will wear out less quickly than a smaller one (from Wood GAM 2e chapter 4). library(tidyverse) data(engine, package = &#39;gamair&#39;) size = engine$size wear = engine$wear x = size - min(size) x = x / max(x) d = data.frame(wear, x) Functions Cubic spline function, rk refers to reproducing kernel. If I recall correctly, the function code is actually based on the first edition of Wood’s text. rk &lt;- function(x, z) { ((z - 0.5)^2 - 1/12) * ((x - 0.5)^2 - 1/12)/4 - ((abs(x - z) - 0.5)^4 - (abs(x - z) - 0.5)^2 / 2 + 7/240) / 24 } Generate the model matrix. splX &lt;- function(x, knots) { q = length(knots) + 2 # number of parameters n = length(x) # number of observations X = matrix(1, n, q) # initialized model matrix X[ ,2] = x # set second column to x X[ ,3:q] = outer(x, knots, FUN = rk) # remaining to cubic spline basis X } splS &lt;- function(knots) { q = length(knots) + 2 S = matrix(0, q, q) # initialize matrix S[3:q, 3:q] = outer(knots, knots, FUN = rk) # fill in non-zero part S } Matrix square root function. Note that there are various packages with their own. mat_sqrt &lt;- function(S) { d = eigen(S, symmetric = TRUE) rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors) rS } Penalized fitting function. prs_fit &lt;- function(y, x, knots, lambda) { q = length(knots) + 2 # dimension of basis n = length(x) # number of observations Xa = rbind(splX(x, knots), mat_sqrt(splS(knots))*sqrt(lambda)) # augmented model matrix y[(n + 1):(n+q)] = 0 # augment the data vector lm(y ~ Xa - 1) # fit and return penalized regression spline } Example 1 We start with an unpenalized approach. knots = 1:4/5 X = splX(x, knots) # generate model matrix fit_lm = lm(wear ~ X - 1) # fit model xp = 0:100/100 # x values for prediction Xp = splX(xp, knots) # prediction matrix Visualize. ggplot(aes(x = x, y = wear), data = data.frame(x, wear)) + geom_point(color = &quot;#FF5500&quot;) + geom_line(aes(x = xp, y = Xp %*% coef(fit_lm)), data = data.frame(xp, Xp), color = &quot;#00AAFF&quot;) + labs(x = &#39;Scaled Engine size&#39;, y = &#39;Wear Index&#39;) Example 2 Now we add the lambda penalty and compare fits at different values of lambda. knots = 1:7/8 d2 = data.frame(x = xp) lambda = c(.1, .01, .001, .0001, .00001, .000001) rmse = vector(&#39;numeric&#39;, length(lambda)) idx = 0 for (i in lambda) { # fit penalized regression fit_penalized = prs_fit( y = wear, x = x, knots = knots, lambda = i ) # spline choosing lambda Xp = splX(xp, knots) # matrix to map parameters to fitted values at xp LP = Xp %*% coef(fit_penalized) d2[, paste0(&#39;lambda = &#39;, i)] = LP[, 1] r = resid(fit_penalized) idx = 1 + idx rmse[idx] = sqrt(mean(r^2)) } Visualize. I add the root mean square error for model comparison. d3 = d2 %&gt;% pivot_longer(cols = -x, names_to = &#39;lambda&#39;, values_to = &#39;value&#39;) %&gt;% mutate(lambda = fct_inorder(lambda), rmse = round(rmse[lambda], 3)) d3 = d2 %&gt;% pivot_longer(cols = -x, names_to = &#39;lambda&#39;, values_to = &#39;value&#39;) %&gt;% mutate(lambda = fct_inorder(lambda), rmse = round(rmse[lambda], 3)) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/cubicsplines.R "],["gaussian-process.html", "Gausian Processes Noise-Free Demonstration Noisy Demonstration Source", " Gausian Processes Noise-Free Demonstration We’ll start with the ‘Noise-free’ gaussian process. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006. See those sources for more detail. Murphy’s original Matlab code can be found here, though the relevant files are housed alongside this code in my original repo (*.m files). The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which y = sin(x). It will reproduce figure 15.2 in Murphy 2012 and 2.2 in Rasmussen and Williams 2006. Data Setup library(tidyverse) l = 1 # for l, sigma_f, see note at covariance function sigma_f = 1 k_eps = 1e-8 # see note at K_starstar n_prior = 5 # number of prior draws n_post_pred = 5 # number of posterior predictive draws Generate noise-less training data. X_train = c(-4, -3, -2, -1, 1) y_train = sin(X_train) n_train = length(X_train) X_test = seq(-5, 5, .2) n_test = length(X_test) Functions The mean function. In this case the mean equals 0. gp_mu &lt;- function(x) { map_dbl(x, function(x) x = 0) } The covariance function. Here it is the squared exponential kernel. l is the horizontal scale, sigma_f is the vertical scale. gp_K &lt;- function(x, l = 1, sigma_f = 1){ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) } Visualize the prior distribution Data setup for the prior and plot. x_prior = seq(-5, 5, .2) y_prior = MASS::mvrnorm( n = n_prior, mu = gp_mu(x_prior), Sigma = gp_K(x_prior, l = l, sigma_f = sigma_f) ) prior_data = data.frame( x = x_prior, y = t(y_prior), sd = apply(y_prior, 2, sd)) %&gt;% pivot_longer(-c(x, sd), names_to = &#39;variable&#39;) g1 = ggplot(aes(x = x, y = value), data = prior_data) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;, alpha = .5) + labs(title = &#39;Prior&#39;) g1 Generate the posterior predictive distribution Create K, K*, and K** matrices as defined in the texts. K = gp_K(X_train, l = l, sigma_f = sigma_f) K_ = gp_K(c(X_train, X_test), l = l, sigma_f = sigma_f) # initial matrix K_star = K_[1:n_train, (n_train+1):ncol(K_)] # dim = N x N* tK_star = t(K_star) # dim = N* x N K_starstar = K_[(n_train+1):nrow(K_), (n_train+1):ncol(K_)] + # dim = N* x N* k_eps * diag(n_test) # the k_eps part is for positive definiteness Kinv = solve(K) Calculate posterior mean and covariance. post_mu = gp_mu(X_test) + t(K_star) %*% Kinv %*% (y_train - gp_mu(X_train)) post_K = K_starstar - t(K_star) %*% Kinv %*% K_star s2 = diag(post_K) # R = chol(post_K) # L = t(R) # L is used in alternative formulation below based on gaussSample.m Generate draws from posterior predictive. y_pp = data.frame( t(MASS::mvrnorm(n_post_pred, mu = post_mu, Sigma = post_K)) ) # alternative if using R and L above # y_pp = data.frame(replicate(n_post_pred, post_mu + L %*% rnorm(post_mu))) Visualize the Posterior Predictive Distribution Reshape data for plotting and create the plot. pp_data = data.frame( x = X_test, y = y_pp, se_lower = post_mu - 2 * sqrt(s2), se_upper = post_mu + 2 * sqrt(s2) ) %&gt;% pivot_longer(starts_with(&#39;y&#39;), names_to = &#39;variable&#39;) g2 = ggplot(aes(x = x, y = value), data = pp_data) + geom_ribbon(aes(ymin = se_lower, ymax = se_upper, group = variable), fill = &#39;gray92&#39;) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;) + geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + labs(title = &#39;Posterior Predictive&#39;) g2 Plot prior and posterior predictive together. library(patchwork) g1 + g2 Noisy Demonstration ‘Noisy’ gaussian process demo. The matrix labeling is in keeping with Murphy 2012 and Rasmussen and Williams 2006. See those sources for more detail. Murphy’s original Matlab code can be found here, though the relevant files are housed alongside this code in my original repo (*.m files). The goal of this code is to plot samples from the prior and posterior predictive of a gaussian process in which y = sin(x) + noise. It will reproduce an example akin to figure 15.3 in Murphy 2012. Data Setup l = 1 # for l, sigma_f, sigma_n, see note at covariance function sigma_f = 1 sigma_n = .25 k_eps = 1e-8 # see note at Kstarstar n_prior = 5 # number of prior draws n_post_pred = 5 # number of posterior predictive draws X_train = 15 * (runif(20) - .5) n_train = length(X_train) # kept sine function for comparison to noise free result y_train = sin(X_train) + rnorm(n = n_train, sd = .1) X_test = seq(-7.5, 7.5, length = 200) n_test = length(X_test) Functions The mean function. In this case the mean equals 0. gp_mu &lt;- function(x) { map_dbl(x, function(x) x = 0) } The covariance function. Here it is the squared exponential kernel. l is the horizontal scale, sigma_f is the vertical scale, and, unlike the previous function, sigma_n the noise. gp_K &lt;- function( x, y = NULL, l = 1, sigma_f = 1, sigma_n = .5 ) { if(!is.null(y)){ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) + sigma_n*diag(length(x)) } else{ sigma_f * exp( -(1/(2 * l^2)) * as.matrix(dist(x, upper = TRUE, diag = TRUE) ^ 2) ) } } Visualize the prior distribution Data setup. x_prior = seq(-5, 5, .2) y_prior = MASS::mvrnorm( n = n_prior, mu = gp_mu(x_prior), Sigma = gp_K( x_prior, l = l, sigma_f = sigma_f, sigma_n = sigma_n ) ) Plot. prior_data = data.frame( x = x_prior, y = t(y_prior), sd = apply(y_prior, 2, sd)) %&gt;% pivot_longer(-c(x, sd), names_to = &#39;variable&#39;) g1 = ggplot(aes(x = x, y = value), data = prior_data) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;, alpha = .5) + labs(title = &#39;Prior&#39;) g1 Generate the posterior predictive distribution Create Ky, K*, and K** matrices as defined in the texts. Ky = gp_K( x = X_train, y = y_train, l = l, sigma_f = sigma_f, sigma_n = sigma_n ) # initial matrix K_ = gp_K( c(X_train, X_test), l = l, sigma_f = sigma_f, sigma_n = sigma_n ) Kstar = K_[1:n_train, (n_train+1):ncol(K_)] # dim = N x N* tKstar = t(Kstar) # dim = N* x N Kstarstar = K_[(n_train+1):nrow(K_), (n_train+1):ncol(K_)] + # dim = N* x N* k_eps*diag(n_test) # the k_eps part is for positive definiteness Kyinv = solve(Ky) Calculate posterior mean and covariance. post_mu = gp_mu(X_test) + tKstar %*% Kyinv %*% (y_train - gp_mu(X_train)) post_K = Kstarstar - tKstar %*% Kyinv %*% Kstar s2 = diag(post_K) # R = chol(post_K) # L = t(R) # L is used in alternative formulation below based on gaussSample.m Generate draws from posterior predictive. y_pp = data.frame(t(MASS::mvrnorm(n_post_pred, mu = post_mu, Sigma = post_K))) # alternative # y_pp = data.frame(replicate(n_post_pred, post_mu + L %*% rnorm(post_mu))) Visualize the Posterior Predictive Distribution Reshape data for plotting and create the plot. pp_data = data.frame( x = X_test, y = y_pp, fmean = post_mu, se_lower = post_mu - 2 * sqrt(s2), se_upper = post_mu + 2 * sqrt(s2) ) %&gt;% pivot_longer(starts_with(&#39;y&#39;), names_to = &#39;variable&#39;) g2 = ggplot(aes(x = x, y = value), data = pp_data) + geom_ribbon(aes(ymin = se_lower, ymax = se_upper, group = variable), fill = &#39;gray92&#39;) + geom_line(aes(group = variable), color = &#39;#FF550080&#39;) + geom_point(aes(x = X_train, y = y_train), data = data.frame(X_train, y_train)) + labs(title = &#39;Posterior Predictive&#39;) g2 Plot prior and posterior predictive together. library(patchwork) g1 + g2 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoiseFree.R (noise-free) https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gp%20Examples/gaussianprocessNoisey.R (noisy) "],["elm.html", "Extreme Learning Machine Data Setup Function Estimation Comparison Supplemental Example Source", " Extreme Learning Machine A very simple implementation of an extreme learning machine for regression. See elmNN and ELMR for some R package implementations. I add comparison to generalized additive models (elm/neural networks and GAMs are adaptive basis function models). http://www.extreme-learning-machines.org G.B. Huang, Q.Y. Zhu and C.K. Siew, Extreme Learning Machine: Theory and Applications. Data Setup One variable, complex function. library(tidyverse) library(mgcv) set.seed(123) n = 5000 x = runif(n) # x = rnorm(n) mu = sin(2*(4*x-2)) + 2* exp(-(16^2) * ((x-.5)^2)) y = rnorm(n, mu, .3) d = data.frame(x, y) qplot(x, y, color = I(&#39;#ff55001A&#39;)) Motorcycle accident data. data(&#39;mcycle&#39;, package = &#39;MASS&#39;) times = matrix(mcycle$times, ncol = 1) accel = mcycle$accel Function elm &lt;- function(X, y, n_hidden = NULL, active_fun = tanh) { # X: an N observations x p features matrix # y: the target # n_hidden: the number of hidden nodes # active_fun: activation function pp1 = ncol(X) + 1 w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden) # random weights h = active_fun(cbind(1, scale(X)) %*% w0) # compute hidden layer B = MASS::ginv(h) %*% y # find weights for hidden layer fit = h %*% B # fitted values list( fit = fit, loss = crossprod(y - fit), B = B, w0 = w0 ) } Estimation X_mat = as.matrix(x, ncol = 1) fit_elm = elm(X_mat, y, n_hidden = 100) str(fit_elm) List of 4 $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ... $ loss: num [1, 1] 442 $ B : num [1:100, 1] 217 -608 1408 -1433 -4575 ... $ w0 : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ... ggplot(aes(x, y), data = d) + geom_point(color = &#39;#ff55001A&#39;) + geom_line(aes(y = fit_elm$fit), color = &#39;#00aaff&#39;) cor(fit_elm$fit[,1], y)^2 [1] 0.8862518 fit_elm_mcycle = elm(times, accel, n_hidden = 100) cor(fit_elm_mcycle$fit[,1], accel)^2 [1] 0.8122349 Comparison We’ll compare to a generalized additive model with gaussian process approximation. fit_gam = gam(y ~ s(x, bs = &#39;gp&#39;, k = 20), data = d) summary(fit_gam)$r.sq [1] 0.8856188 d %&gt;% mutate(fit_elm = fit_elm$fit, fit_gam = fitted(fit_gam)) %&gt;% ggplot() + geom_point(aes(x, y), color = &#39;#ff55001A&#39;) + geom_line(aes(x, y = fit_elm), color = &#39;#1e90ff&#39;) + geom_line(aes(x, y = fit_gam), color = &#39;#990024&#39;) fit_gam_mcycle = gam(accel ~ s(times), data = mcycle) summary(fit_gam_mcycle)$r.sq [1] 0.7832988 mcycle %&gt;% ggplot(aes(times, accel)) + geom_point(color = &#39;#ff55001A&#39;) + geom_line(aes(y = fit_elm_mcycle$fit), color = &#39;#1e90ff&#39;) + geom_line(aes(y = fitted(fit_gam_mcycle)), color = &#39;#990024&#39;) Supplemental Example Yet another example with additional covariates. d = gamSim(eg = 7, n = 10000) Gu &amp; Wahba 4 term additive model, correlated predictors X = as.matrix(d[, 2:5]) y = d[, 1] n_nodes = c(10, 25, 100, 250, 500, 1000) The following estimation over multiple models will take several seconds. fit_elm = purrr::map(n_nodes, function(n) elm(X, y, n_hidden = n)) Now find the best fitting model. # estimate best_loss = which.min(map_dbl(fit_elm, function(x) x$loss)) fit_best = fit_elm[[best_loss]] A quick check of the fit. # str(fit_best) # qplot(fit_best$fit[, 1], y, alpha = .2) cor(fit_best$fit[, 1], y)^2 [1] 0.7241967 And compare again to mgcv. In this case, we’re comparing fit on test data of the same form. fit_gam = gam(y ~ s(x0) + s(x1) + s(x2) + s(x3), data = d) gam.check(fit_gam) Method: GCV Optimizer: magic Smoothing parameter selection converged after 15 iterations. The RMS GCV score gradient at convergence was 9.309879e-07 . The Hessian was positive definite. Model rank = 37 / 37 Basis dimension (k) checking results. Low p-value (k-index&lt;1) may indicate that k is too low, especially if edf is close to k&#39;. k&#39; edf k-index p-value s(x0) 9.00 4.71 1.00 0.48 s(x1) 9.00 4.89 1.00 0.35 s(x2) 9.00 8.96 0.99 0.20 s(x3) 9.00 1.00 1.00 0.47 summary(fit_gam)$r.sq [1] 0.6952978 test_data0 = gamSim(eg = 7) # default n = 400 Gu &amp; Wahba 4 term additive model, correlated predictors test_data = cbind(1, scale(test_data0[, 2:5])) # remember to use your specific activation function here elm_prediction = tanh(test_data %*% fit_best$w0) %*% fit_best$B gam_prediction = predict(fit_gam, newdata = test_data0) cor(data.frame(elm_prediction, gam_prediction), test_data0$y)^2 [,1] elm_prediction 0.6873090 gam_prediction 0.7185687 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/elm.R "],["rkhs.html", "Reproducing Kernel Hilbert Space Regression Data Setup Functions Estimation Comparison Example: Cubic Spline Source", " Reproducing Kernel Hilbert Space Regression This R code is based on Reproducing Kernel Hilbert Spaces for Penalized Regression: A tutorial, Nosedal-Sanchez et al. (2010), specifically, their code in the supplemental section. The original code had several issues as far as general R programming practices, and eventually appears to have been replaced in publication at some point, as did most of the corresponding supplemental text. I can no longer locate the original, so now follow the published code. The original data I was following was also replaced by the longley and mcycle data sets. To start, we will use an example for ridge regression, followed by a spline example. Data Setup library(tidyverse) data(longley) # avaiable in base R y = longley[,1] X = as.matrix(longley[,2:7]) X = apply(X, 2, scales::rescale, to = c(0, 1)) Functions Function to find the inverse of a matrix We can use base::solve, but this function avoids a computationally singular result. inverse &lt;- function(X, eps = 1e-12) { eig.X = eigen(X, symmetric = TRUE) P = eig.X[[2]] lambda = eig.X[[1]] ind = lambda &gt; eps lambda[ind] = 1/lambda[ind] lambda[!ind] = 0 P %*% diag(lambda) %*% t(P) } Reproducing Kernel rk &lt;- function(s, t) { init_len = length(s) rk = 0 for (i in 1:init_len) rk = s[i]*t[i] + rk rk } Gram matrix For the first example involving ridge regression, the gram function just produces tcrossprod(X). I generalize it in case a different kernel is desired, and add that as an additional argument. This will avoid having to redo the function later. gram &lt;- function(X, rkfunc = rk) { apply(X, 1, function(Row) apply(X, 1, function(tRow) rkfunc(Row, tRow)) ) } Ridge regression ridge &lt;- function(X, y, lambda) { Gramm = gram(X) # Gramm matrix (nxn) n = length(y) Q = cbind(1, Gramm) # design matrix S = rbind(0, cbind(0, Gramm)) M = crossprod(Q) + lambda*S M_inv = inverse(M) # inverse of M gamma_hat = crossprod(M_inv, crossprod(Q, y)) f_hat = Q %*% gamma_hat A = Q %*% M_inv %*% t(Q) tr_A = sum(diag(A)) # trace of hat matrix rss = crossprod(y - f_hat) # residual sum of squares gcv = n*rss / (n - tr_A)^2 # obtain GCV score list( f_hat = f_hat, gamma_hat = gamma_hat, beta_hat = c(gamma_hat[1], crossprod(gamma_hat[-1], X)), gcv = gcv ) } Estimation A simple direct search for the GCV optimal smoothing parameter can be made as follows: lambda = 10^seq(-6, 0, by = .1) gcv_search = map(lambda, function(lam) ridge(X, y, lam)) V = map_dbl(gcv_search, function(x) x$gcv) ridge_coefs = map_df(gcv_search, function(x) data.frame( value = x$beta_hat[-1], coef = colnames(X) ), .id = &#39;iter&#39;) %&gt;% mutate( lambda = lambda[as.integer(iter)] ) Compare with Figure 3 in the article. gcv_plot = qplot( lambda, V, geom = &#39;line&#39;, main = &#39;GCV score&#39;, ylab = &#39;GCV&#39; ) + scale_x_log10() beta_plot = ridge_coefs %&gt;% ggplot(aes(x = lambda, y = value, color = coef)) + geom_line() + scale_x_log10() + scico::scale_color_scico_d(end = .8) + labs(title = &#39;Betas Across Lambda&#39;) library(patchwork) gcv_plot + beta_plot Pick the best model and obtain the estimates. ridge_model = ridge(X, y, lambda[which.min(V)]) # fit optimal model gamma_hat = ridge_model$gamma_hat beta_0 = ridge_model$gamma_hat[1] # intercept beta_hat = crossprod(gamma_hat[-1,], X) # slope and noise term coefficients Comparison I add a comparison to glmnet, where setting alpha = 0 is equivalent to ridge regression. c(beta_0, beta_hat) [1] 82.7840043 54.1683427 5.3640251 1.3781910 -28.7948627 5.3956341 -0.6095799 ridge_glmnet = glmnet::glmnet( X, y, alpha = 0, lambda = lambda, standardize = FALSE ) cbind( est = c(beta_0, beta_hat), glmnet = coef(ridge_glmnet)[, which.max(ridge_glmnet$dev.ratio)] ) est glmnet (Intercept) 82.7840043 82.328741 GNP 54.1683427 69.783267 Unemployed 5.3640251 6.962672 Armed.Forces 1.3781910 1.245064 Population -28.7948627 -37.323055 Year 5.3956341 -1.497405 Employed -0.6095799 -1.605646 Example: Cubic Spline Data Setup For this example we’ll use the MASS::mcycle data. x = as.matrix(MASS::mcycle$times) x = scales::rescale(x, to = c(0, 1)) # rescale predictor to [0,1] y = MASS::mcycle$accel Functions Reproducing Kernel rk_spline &lt;- function(s, t) { return(.5 * min(s, t)^2 * max(s, t) - (1/6) * min(s, t)^3) } No need to redo the gram function do to previous change that accepts the kernel as an argument Smoothing Spline smoothing_spline &lt;- function(X, y, lambda) { Gramm = gram(X, rkfunc = rk_spline) # Gramm matrix (nxn) n = length(y) J = cbind(1, X) # matrix with a basis for the null space of the penalty Q = cbind(J, Gramm) # design matrix m = ncol(J) # dimension of the null space of the penalty S = matrix(0, n + m, n + m) # initialize S S[(m + 1):(n + m), (m + 1):(n + m)] = Gramm # non-zero part of S M = crossprod(Q) + lambda*S M_inv = inverse(M) # inverse of M gamma_hat = crossprod(M_inv, crossprod(Q, y)) f_hat = Q %*% gamma_hat A = Q %*% M_inv %*% t(Q) tr_A = sum(diag(A)) # trace of hat matrix rss = crossprod(y - f_hat) # residual sum of squares gcv = n * rss/(n - tr_A)^2 # obtain GCV score list( f_hat = f_hat, gamma_hat = gamma_hat, gcv = gcv ) } Estimation lambda = 10^seq(-6, 0, by = .1) gcv_search = map(lambda, function(lam) smoothing_spline(x, y, lam)) V = map_dbl(gcv_search, function(x) x$gcv) Plot of GCV. gcv_plot = qplot( lambda, V, geom = &#39;line&#39;, main = &#39;GCV score&#39;, ylab = &#39;GCV&#39; ) + scale_x_log10() # gcv_plot Comparison I’ve added comparison to an additive model using mgcv. Compare the result to Figure 2 of the Supplementary Material. spline_model = smoothing_spline(x, y, lambda[which.min(V)]) # fit optimal model gam_model = mgcv::gam(y ~ s(x)) fit_plot = MASS::mcycle %&gt;% mutate(rk_fit = spline_model$f_hat[, 1], gam_fit = fitted(gam_model)) %&gt;% arrange(times) %&gt;% pivot_longer(-c(times, accel), names_to = &#39;fit&#39;, values_to = &#39;value&#39;) %&gt;% ggplot(aes(times, accel)) + geom_point(color = &#39;#FF55001A&#39;) + geom_line(aes(y = value, color = fit)) + scico::scale_color_scico_d(palette = &#39;hawaii&#39;, begin = .2, end = .8) library(patchwork) gcv_plot + fit_plot Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/RKHSReg Current Supplemental Code You can peruse the supplemental section that shows the R code here. Original Supplemental Code This was the old original code from the supplemental section for the article, but was changed at some point (I can’t remember, it may have been at one of the author’s website). The R code on the repo follows these examples, while this document follows the currently accessible supplementary material. I used RStudio’s default cleanup to make the code a little easier to read, and maybe added a little spacing, but otherwise it is identical to what’s in the linked paper. A.1 ###### Data ######## set.seed(3) n &lt;- 20 x1 &lt;- runif(n) x2 &lt;- runif(n) X &lt;- matrix(c(x1, x2), ncol = 2) # design matrix y &lt;- 2 + 3 * x1 + rnorm(n, sd = 0.25) ##### function to find the inverse of a matrix #### my.inv &lt;- function(X, eps = 1e-12) { eig.X &lt;- eigen(X, symmetric = T) P &lt;- eig.X[[2]] lambda &lt;- eig.X[[1]] ind &lt;- lambda &gt; eps lambda[ind] &lt;- 1 / lambda[ind] lambda[!ind] &lt;- 0 ans &lt;- P %*% diag(lambda, nrow = length(lambda)) %*% t(P) return(ans) } ###### Reproducing Kernel ######### rk &lt;- function(s, t) { p &lt;- length(s) rk &lt;- 0 for (i in 1:p) { rk &lt;- s[i] * t[i] + rk } return((rk)) } ##### Gram matrix ####### get.gramm &lt;- function(X) { n &lt;- dim(X)[1] Gramm &lt;- matrix(0, n, n) #initializes Gramm array #i=index for rows #j=index for columns Gramm&lt;-as.matrix(Gramm) # Gramm matrix for (i in 1:n) { for (j in 1:n) { Gramm[i, j] &lt;- rk(X[i,], X[j,]) } } return(Gramm) } ridge.regression &lt;- function(X, y, lambda) { Gramm &lt;- get.gramm(X) #Gramm matrix (nxn) n &lt;- dim(X)[1] # n=length of y J &lt;- matrix(1, n, 1) # vector of ones dim Q &lt;- cbind(J, Gramm) # design matrix m &lt;- 1 # dimension of the null space of the penalty S &lt;- matrix(0, n + m, n + m) #initialize S S[(m + 1):(n + m), (m + 1):(n + m)] &lt;- Gramm #non-zero part of S M &lt;- (t(Q) %*% Q + lambda * S) M.inv &lt;- my.inv(M) # inverse of M gamma.hat &lt;- crossprod(M.inv, crossprod(Q, y)) f.hat &lt;- Q %*% gamma.hat A &lt;- Q %*% M.inv %*% t(Q) tr.A &lt;- sum(diag(A)) #trace of hat matrix rss &lt;- t(y - f.hat) %*% (y - f.hat) #residual sum of squares gcv &lt;- n * rss / (n - tr.A) ^ 2 #obtain GCV score return(list( f.hat = f.hat, gamma.hat = gamma.hat, gcv = gcv )) } # Plot of GCV lambda &lt;- 1e-8 V &lt;- rep(0, 40) for (i in 1:40) { V[i] &lt;- ridge.regression(X, y, lambda)$gcv #obtain GCV score lambda &lt;- lambda * 1.5 #increase lambda } index &lt;- (1:40) plot( 1.5 ^ (index - 1) * 1e-8, V, type = &quot;l&quot;, main = &quot;GCV score&quot;, lwd = 2, xlab = &quot;lambda&quot;, ylab = &quot;GCV&quot; ) # plot score i &lt;- (1:60)[V == min(V)] # extract index of min(V) opt.mod &lt;- ridge.regression(X, y, 1.5 ^ (i - 1) * 1e-8) #fit optimal model ### finding beta.0, beta.1 and beta.2 ########## gamma.hat &lt;- opt.mod$gamma.hat beta.hat.0 &lt;- opt.mod$gamma.hat[1]#intercept beta.hat &lt;- gamma.hat[2:21,] %*% X #slope and noise term coefficients #### Fitted Line Plot for Cubic Smoothing Spline #### plot(x[,1],y,xlab=&quot;x&quot;,ylab=&quot;response&quot;,main=&quot;Cubic Smoothing Spline&quot;) ; lines(x[,1],opt.mod$f.hat,type=&quot;l&quot;,lty=1,lwd=2,col=&quot;blue&quot;) ; A.2 A.2 RKHS solution applied to Cubic Smoothing Spline We consider a sample of size n = 50, (\\(y_1, y_2, y_3, ..., y_{50}\\)), from the model \\(y_i = sin(2πx_i) + ϵ_i\\) where ϵi has a N(0, 0.22) . The following code generates x and y… A simple direct search for the GCV optimal smoothing parameter can be made as follows: Now we have to find an optimal lambda using GCV… Below we give a function to find the cubic smoothing spline using the RKHS framework we discussed in Section 4.3. We also provide a graph with our estimation along with the true function and data. ###### Data ######## set.seed(3) n &lt;- 50 x &lt;- matrix(runif(n), nrow, ncol = 1) x.star &lt;- matrix(sort(x), nrow, ncol = 1) # sorted x, used by plot y &lt;- sin(2 * pi * x.star) + rnorm(n, sd = 0.2) #### Reproducing Kernel for &lt;f,g&gt;=int_0^1 f’’(x)g’’(x)dx ##### rk.1 &lt;- function(s, t) { return((1 / 2) * min(s, t) ^ 2) * (max(s, t) + (1 / 6) * (min(s, t)) ^ 3) } get.gramm.1 &lt;- function(X) { n &lt;- dim(X)[1] Gramm &lt;- matrix(0, n, n) #initializes Gramm array #i=index for rows #j=index for columns Gramm &lt;- as.matrix(Gramm) # Gramm matrix for (i in 1:n) { for (j in 1:n) { Gramm[i, j] &lt;- rk.1(X[i, ], X[j, ]) } } return(Gramm) } smoothing.spline &lt;- function(X, y, lambda) { Gramm &lt;- get.gramm.1(X) #Gramm matrix (nxn) n &lt;- dim(X)[1] # n=length of y J &lt;- matrix(1, n, 1) # vector of ones dim T &lt;- cbind(J, X) # matrix with a basis for the null space of the penalty Q &lt;- cbind(T, Gramm) # design matrix m &lt;- dim(T)[2] # dimension of the null space of the penalty S &lt;- matrix(0, n + m, n + m) #initialize S S[(m + 1):(n + m), (m + 1):(n + m)] &lt;- Gramm #non-zero part of S M &lt;- (t(Q) %*% Q + lambda * S) M.inv &lt;- my.inv(M) # inverse of M gamma.hat &lt;- crossprod(M.inv, crossprod(Q, y)) f.hat &lt;- Q %*% gamma.hat A &lt;- Q %*% M.inv %*% t(Q) tr.A &lt;- sum(diag(A)) #trace of hat matrix rss &lt;- t(y - f.hat) %*% (y - f.hat) #residual sum of squares gcv &lt;- n * rss / (n - tr.A) ^ 2 #obtain GCV score return(list( f.hat = f.hat, gamma.hat = gamma.hat, gcv = gcv )) } ### Now we have to find an optimal lambda using GCV... ### Plot of GCV lambda &lt;- 1e-8 V &lt;- rep(0, 60) for (i in 1:60) { V[i] &lt;- smoothing.spline(x.star, y, lambda)$gcv #obtain GCV score lambda &lt;- lambda * 1.5 #increase lambda } plot(1:60, V, type = &quot;l&quot;, main = &quot;GCV score&quot;, xlab = &quot;i&quot;) # plot score i &lt;- (1:60)[V == min(V)] # extract index of min(V) spline_model &lt;- smoothing.spline(x.star, y, 1.5 ^ (i - 1) * 1e-8) #fit optimal model #Graph (Cubic Spline) plot( x.star, spline_model$f.hat, type = &quot;l&quot;, lty = 2, lwd = 2, col = &quot;blue&quot;, xlab = &quot;x&quot;, ylim = c(-2.5, 1.5), xlim = c(-0.1, 1.1), ylab = &quot;response&quot;, main = &quot;Cubic Spline&quot; ) #predictions lines(x.star, sin(2 * pi * x.star), lty = 1, lwd = 2) #true legend( -0.1, -1.5, c(&quot;predictions&quot;, &quot;true&quot;), lty = c(2, 1), bty = &quot;n&quot;, lwd = c(2, 2), col = c(&quot;blue&quot;, &quot;black&quot;) ) points(x.star, y) "],["cfa.html", "Confirmatory Factor Analysis Data Setup Functions Estimation Comparison Source", " Confirmatory Factor Analysis This mostly follows Bollen (1989) for maximum likelihood estimation of a confirmatory factor analysis. In the following example we will examine a situation where there are two underlying (correlated) latent variables for 8 observed responses. The code as is will only work with this toy data set. Setup uses the psych and mvtnorm packages, and results are checked against the lavaan package. Data Setup For the data we will simulate observed variables with specific loadings on two latent constructs (factors). library(tidyverse) set.seed(123) # loading matrix lambda = matrix( c(1.0, 0.5, 0.8, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7, 0.6, 0.8), nrow = 2, byrow = TRUE ) # correlation of factors phi = matrix(c(1, .25, .25, 1), nrow = 2, byrow = TRUE) # factors and some noise factors = mvtnorm::rmvnorm(1000, mean = rep(0, 2), sigma = phi, &quot;chol&quot;) e = mvtnorm::rmvnorm(1000, sigma = diag(8)) # observed responses y = 0 + factors%*%lambda + e # Examine #dim(y) psych::describe(y) vars n mean sd median trimmed mad min max range skew kurtosis se X1 1 1000 0.05 1.44 0.05 0.05 1.42 -5.13 4.51 9.63 0.00 0.01 0.05 X2 2 1000 0.00 1.08 -0.01 0.00 1.04 -3.34 3.25 6.59 0.00 -0.06 0.03 X3 3 1000 0.01 1.27 0.05 0.01 1.17 -5.33 4.09 9.42 -0.06 0.32 0.04 X4 4 1000 0.00 1.14 -0.03 -0.01 1.13 -3.85 3.98 7.83 0.10 0.16 0.04 X5 5 1000 0.04 1.43 0.10 0.05 1.39 -4.43 5.21 9.63 -0.02 0.07 0.05 X6 6 1000 -0.02 1.22 -0.01 -0.02 1.27 -3.35 4.68 8.03 0.04 -0.10 0.04 X7 7 1000 0.02 1.14 0.02 0.01 1.10 -3.08 3.66 6.74 0.11 0.00 0.04 X8 8 1000 0.01 1.29 0.02 0.01 1.24 -3.68 4.50 8.18 -0.02 0.10 0.04 # round(cor(y), 2) # see the factor structure psych::cor.plot(cor(y)) # example exploratory fa #psych::fa(y, nfactors=2, rotate=&quot;oblimin&quot;) Functions We will have two separate estimation functions, one for the covariance matrix, and another for the correlation matrix. # measurement model, covariance approach # trace function, strangely absent from base R tr &lt;- function(mat) { sum(diag(mat), na.rm = TRUE) } cfa_cov &lt;- function (parms, data) { # Arguments- # parms: initial values (named) # data: raw data # Extract parameters by name l1 = c(1, parms[grep(&#39;l1&#39;, names(parms))]) # loadings for factor 1 l2 = c(1, parms[grep(&#39;l2&#39;, names(parms))]) # loadings for factor 2 cov0 = parms[grep(&#39;cov&#39;, names(parms))] # factor covariance, variances # Covariance matrix S = cov(data)*((nrow(data)-1)/nrow(data)) # ML covariance div by N rather than N-1, the multiplier adjusts # loading estimates lambda = cbind( c(l1, rep(0,length(l2))), c(rep(0,length(l1)), l2) ) # disturbances dist_init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist_init) # factor correlation phi_init = matrix(c(cov0[1], cov0[2], cov0[2], cov0[3]), 2, 2) #factor cov/correlation matrix # other calculations and log likelihood sigtheta = lambda%*%phi_init%*%t(lambda) + disturbs # in Bollen p + q (but for the purposes of this just p) = tr(data) pq = dim(data)[2] # a reduced version; Bollen 1989 p.107 # ll = -(log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq) # this should be the same as Mplus H0 log likelihood ll = ( (-nrow(data)*pq/2) * log(2*pi) ) - (nrow(data)/2) * ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) ) -ll } We can use the correlation matrix for standardized results. Lines correspond to those in cfa_cov. cfa_cor &lt;- function (parms, data) { l1 = parms[grep(&#39;l1&#39;, names(parms))] # loadings for factor 1 l2 = parms[grep(&#39;l2&#39;, names(parms))] # loadings for factor 2 cor0 = parms[grep(&#39;cor&#39;, names(parms))] # factor correlation S = cor(data) lambda = cbind( c(l1, rep(0,length(l2))), c(rep(0,length(l1)), l2) ) dist_init = parms[grep(&#39;dist&#39;, names(parms))] disturbs = diag(dist_init) phi_init = matrix(c(1, cor0, cor0, 1), ncol=2) sigtheta = lambda%*%phi_init%*%t(lambda) + disturbs pq = dim(data)[2] #ll = ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) - log(det(S)) - pq ) ll = ( (-nrow(data)*pq/2) * log(2*pi) ) - (nrow(data)/2) * ( log(det(sigtheta)) + tr(S%*%solve(sigtheta)) ) -ll } Estimation Corresponding to the functions, we will get results based on the covariance and correlation matrix respectively. Raw Set initial values. par_init_cov = c(rep(1, 6), rep(.05, 8), rep(.5, 3)) names(par_init_cov) = rep(c(&#39;l1&#39;,&#39;l2&#39;, &#39;dist&#39;, &#39;cov&#39;), c(3, 3, 8, 3)) Estimate and extract. fit_cov = optim( par = par_init_cov, fn = cfa_cov, data = y, method = &quot;L-BFGS-B&quot;, lower = 0 ) loadings_cov = data.frame( f1 = c(1, fit_cov$par[1:3], rep(0, 4)), f2 = c(rep(0, 4), 1, fit_cov$par[4:6]) ) disturbances_cov = fit_cov$par[7:14] Standardized par_init_cor = c(rep(1, 8), rep(.05, 8), 0) #for cor names(par_init_cor) = rep(c(&#39;l1&#39;, &#39;l2&#39;, &#39;dist&#39;, &#39;cor&#39;), c(4, 4, 8, 1)) fit_cor = optim( par = par_init_cor, fn = cfa_cor, data = y, method = &quot;L-BFGS-B&quot;, lower = 0, upper = 1 ) loadings_cor = matrix( c(fit_cor$par[1:4], rep(0, 4), rep(0, 4), fit_cor$par[5:8]), ncol = 2 ) disturbances_cor = fit_cor$par[9:16] Comparison Gather results for summarizing. results = list( raw = list( loadings = round(data.frame(loadings_cov, Variances = disturbances_cov), 3), cov.fact = round(matrix(c(fit_cov$par[c(15, 16, 16, 17)]), ncol = 2) , 3) ), standardized = list( loadings = round( data.frame( loadings_cor, Variances = disturbances_cor, Rsq = (1 - disturbances_cor) ), 3), cor.fact = round(matrix(c(1, fit_cor$par[c(17, 17)], 1), ncol = 2), 3) ), # note inclusion of intercepts for total number of par fit_lav = data.frame( ll = fit_cov$value, AIC = 2*fit_cov$value + 2 * (length(par_init_cov) + ncol(y)), BIC = 2*fit_cov$value + log(nrow(y)) * (length(par_init_cov) + ncol(y)) ) ) results $raw $raw$loadings f1 f2 Variances 1 1.000 0.000 1.073 2 0.459 0.000 0.955 3 0.836 0.000 0.908 4 0.570 0.000 0.961 5 0.000 1.000 1.047 6 0.000 0.739 0.941 7 0.000 0.575 0.972 8 0.000 0.803 1.034 $raw$cov.fact [,1] [,2] [1,] 1.006 0.185 [2,] 0.185 0.989 $standardized $standardized$loadings X1 X2 Variances Rsq 1 0.696 0.000 0.516 0.484 2 0.426 0.000 0.819 0.181 3 0.661 0.000 0.563 0.437 4 0.504 0.000 0.746 0.254 5 0.000 0.697 0.514 0.486 6 0.000 0.604 0.636 0.364 7 0.000 0.502 0.748 0.252 8 0.000 0.618 0.618 0.382 $standardized$cor.fact [,1] [,2] [1,] 1.000 0.186 [2,] 0.186 1.000 $fit_lav ll AIC BIC 1 12497.68 25045.37 25168.06 Compare with lavaan. library(lavaan) y = data.frame(y) model = &#39; F1 =~ X1 + X2 + X3 + X4 F2 =~ X5 + X6 + X7 + X8 &#39; fit_lav = cfa( model, data = y, mimic = &#39;Mplus&#39;, estimator = &#39;ML&#39; ) fit_lav_std = cfa( model, data = y, mimic = &#39;Mplus&#39;, estimator = &#39;ML&#39;, std.lv = TRUE, std.ov = TRUE ) # note that lavaan does not count the intercepts among the free params for # AIC/BIC by default, (can get its result via -2 * as.numeric(lls) + k * # attr(lls, &quot;df&quot;)), but the mimic=&#39;Mplus&#39; should have them correspond to optim&#39;s # results summary(fit_lav, fit.measures = TRUE, standardized = TRUE) lavaan 0.6-7 ended normally after 30 iterations Estimator ML Optimization method NLMINB Number of free parameters 25 Number of observations 1000 Number of missing patterns 1 Model Test User Model: Test statistic 25.586 Degrees of freedom 19 P-value (Chi-square) 0.142 Model Test Baseline Model: Test statistic 1229.322 Degrees of freedom 28 P-value 0.000 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.995 Tucker-Lewis Index (TLI) 0.992 Loglikelihood and Information Criteria: Loglikelihood user model (H0) -12497.683 Loglikelihood unrestricted model (H1) -12484.890 Akaike (AIC) 25045.366 Bayesian (BIC) 25168.060 Sample-size adjusted Bayesian (BIC) 25088.658 Root Mean Square Error of Approximation: RMSEA 0.019 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.035 P-value RMSEA &lt;= 0.05 1.000 Standardized Root Mean Square Residual: SRMR 0.018 Parameter Estimates: Standard errors Standard Information Observed Observed information based on Hessian Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all F1 =~ X1 1.000 1.003 0.696 X2 0.459 0.046 10.043 0.000 0.460 0.426 X3 0.836 0.066 12.590 0.000 0.839 0.661 X4 0.570 0.050 11.450 0.000 0.572 0.504 F2 =~ X5 1.000 0.994 0.697 X6 0.739 0.056 13.239 0.000 0.735 0.604 X7 0.575 0.048 12.071 0.000 0.572 0.502 X8 0.803 0.060 13.386 0.000 0.799 0.618 Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all F1 ~~ F2 0.185 0.046 4.014 0.000 0.186 0.186 Intercepts: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .X1 0.054 0.046 1.173 0.241 0.054 0.037 .X2 -0.004 0.034 -0.104 0.917 -0.004 -0.003 .X3 0.007 0.040 0.182 0.855 0.007 0.006 .X4 0.004 0.036 0.113 0.910 0.004 0.004 .X5 0.044 0.045 0.964 0.335 0.044 0.031 .X6 -0.019 0.038 -0.504 0.614 -0.019 -0.016 .X7 0.024 0.036 0.674 0.500 0.024 0.021 .X8 0.010 0.041 0.247 0.805 0.010 0.008 F1 0.000 0.000 0.000 F2 0.000 0.000 0.000 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .X1 1.073 0.087 12.356 0.000 1.073 0.516 .X2 0.955 0.047 20.128 0.000 0.955 0.819 .X3 0.908 0.065 13.887 0.000 0.908 0.563 .X4 0.961 0.051 18.885 0.000 0.961 0.746 .X5 1.048 0.079 13.336 0.000 1.048 0.514 .X6 0.941 0.056 16.867 0.000 0.941 0.636 .X7 0.972 0.050 19.255 0.000 0.972 0.748 .X8 1.034 0.063 16.411 0.000 1.034 0.618 F1 1.006 0.108 9.351 0.000 1.000 1.000 F2 0.989 0.100 9.853 0.000 1.000 1.000 Mplus If you have access to Mplus you can use Mplus Automation to prepare the data. The following code is in Mplus syntax and will produce the above model. library(MplusAutomation) prepareMplusData(data.frame(y), &quot;factsim.dat&quot;) MODEL: F1 BY X1-X4; F2 BY X5-X8; results: STDYX; Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/cfa.R "],["bayesian.html", "Introduction to Bayesian Methods", " Introduction to Bayesian Methods The following demonstrations will likely assume more background knowledge, but otherwise take a similar approach. For example, the model fitting functions in the previous demonstrations are now replaced with Stan code. I start with a demo followed by the simplest of models such as a t-test and linear regression to get one comfortable. For more introduction, see my document. Note that some of the old code is now easily accomplished with tools like rstanarm or brms (e.g. for standard linear and mixed models), or Stan even has built in functions (e.g. gaussian process covariance functions). As such, I didn’t copy all of my old efforts to this document. So you can take a look at the old repo for a few more demos I probably won’t include here. "],["bayesian-basics.html", "Basics Prior, likelihood, &amp; posterior distributions Prior Likelihood Posterior Posterior predictive Source", " Basics Prior, likelihood, &amp; posterior distributions The following is an attempt to provide a small example to show the connection between prior distribution, likelihood and posterior distribution. It is taken directly from my document with mostly just cleaned up code and visualization. Let’s say we want to estimate the probability that a soccer/football player will score a penalty kick in a shootout. We will employ the binomial distribution to model this. Our goal is to estimate a parameter \\(\\theta\\), the probability that the random knucklehead from your favorite football team will score the penalty in an overtime shootout. Let’s say that for this match it takes 10 shots per team before the game is decided. We can represent the following data for your team as follows, as well as set up some other things for later. shots = c(&#39;goal&#39;,&#39;goal&#39;,&#39;goal&#39;,&#39;miss&#39;,&#39;miss&#39;, &#39;goal&#39;,&#39;goal&#39;,&#39;miss&#39;,&#39;miss&#39;,&#39;goal&#39;) # convert to numeric, arbitrarily picking goal = 1, miss = 0 shots_01 = as.numeric(shots == &#39;goal&#39;) N = length(shots) # sample size n_goals = sum(shots == &#39;goal&#39;) # number of shots made n_missed = sum(shots == &#39;miss&#39;) # number of those miss Recall the binomial distribution where we specify the number of trials for a particular observation and the probability of an event. Let’s look at the distribution for a couple values for \\(\\theta\\) equal to .5 and .85, and \\(N=10\\) observations. We will repeat this 1000 times. set.seed(1234) x1 = rbinom(1000, size = 10, p = .5) x2 = rbinom(1000, size = 10, p = .85) mean(x1) [1] 5.043 qplot(x1, geom = &#39;histogram&#39;) mean(x2) [1] 8.569 qplot(x2, geom = &#39;histogram&#39;) The histograms are not shown, but we can see the means are roughly around \\(N*p\\) as we expect with the binomial. Prior For our current situation, we don’t know \\(\\theta\\) and are trying to estimate it. We will start by supplying some possible values. To keep things simple, we’ll only consider 10 values that fall between 0 and 1. theta = seq(from = 1/(N + 1), to = N/(N + 1), length = 10) For the Bayesian approach we must choose a prior distribution representing our initial beliefs about the estimates we might potentially consider. I provide three possibilities, and note that any one of them would work just fine for this situation. We’ll go with a triangular distribution, which will put most of the weight toward values around \\(.5\\). While we will only work with one prior, do play with the others. ### prior distribution # triangular as in Kruschke text example p_theta = pmin(theta, 1 - theta) # uniform # p_theta = dunif(theta) # beta prior with mean = .5 # p_theta = dbeta(theta, 10, 10) # Normalize so that values sum to 1 p_theta = p_theta / sum(p_theta) So, given some estimate of \\(\\theta\\), we have a probability of that value based on our chosen prior. Likelihood Next we will compute the likelihood of the data given some value of \\(\\theta\\). Generally, the likelihood for some target variable \\(y\\), with observed values \\(i \\dots n\\), given some (set of) parameter(s) \\(\\theta\\), can be expressed as follows: \\[p(y|\\theta) = \\prod_{i}^{n} p(y_i|\\theta)\\] Specifically, the likelihood function for the binomial can be expressed as: \\[p(y|\\theta) = {N \\choose k}\\, \\theta^k\\, (1-\\theta)^{N-k}\\] where \\(N\\) is the total number of possible times in which the event of interest could occur, and \\(k\\) number of times the event of interest occurs. Our maximum likelihood estimate in this simple setting would simply be the proportion of events witnessed out of the total number of samples. We’ll use the formula presented above. Technically, the first term is not required, but it serves to normalize the likelihood as we did with the prior. p_data_given_theta = choose(N, n_goals) * theta^n_goals * (1-theta)^n_missed Posterior Given the prior and likelihood, we can now compute the posterior distribution via Bayes theorem. The only thing left to calculate is the denominator from Bayes theorem, then plug in the rest. p_data = p_data_given_theta*p_theta # marginal probability of the data p_theta_given_data = p_data_given_theta*p_theta / sum(p_data) # Bayes theorem Now let’s examine what all we’ve got. theta prior likelihood posterior 0.091 0.033 0.000 0.000 0.182 0.067 0.003 0.002 0.273 0.100 0.024 0.018 0.364 0.133 0.080 0.079 0.455 0.167 0.164 0.203 0.545 0.167 0.236 0.293 0.636 0.133 0.244 0.242 0.727 0.100 0.172 0.128 0.818 0.067 0.069 0.034 0.909 0.033 0.008 0.002 Starting with the prior column, we can see that with the triangular distribution, we’ve given most of our prior probability to the middle values with probability tapering off somewhat slowly towards either extreme. The likelihood, on the other hand, suggests the data is most likely for \\(\\theta\\) values .55-.64, though we know the specific maximum likelihood estimate for \\(\\theta\\) is the proportion for the sample, or .6. Our posterior estimate will therefore fall somewhere between the prior and likelihood estimates, and we can see it has shifted the bulk of the probability slightly away from the most likely values suggested by the prior distribution, and towards a \\(\\theta\\) value suggested by the data of .6. Let’s go ahead and see what the mean is: posterior_mean = sum(p_theta_given_data*theta) posterior_mean [1] 0.5623611 So, we start with a prior centered on a value of \\(\\theta=.5\\), add data whose ML estimate is \\(\\theta=.6\\), and our posterior distribution suggests we end up somewhere in between. We can perhaps understand this further via the following visualizations. In each of these the prior is represented by the blue density, the likelihood by the red, and the posterior by purple. This first is based on a different prior than just used in our example, and instead employs the beta distribution noted among the possibilities in the code above. The beta distribution is highly flexible, and with shape parameters \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) set to 10 and 10 we get a symmetric distribution centered on \\(\\theta = .5\\). This would actually be a somewhat stronger prior than we might normally want to use, but serves to illustrate a point. The mean of the beta is \\(\\frac{\\mathcal{A}}{\\mathcal{A}+\\mathcal{B}}\\), and thus has a nice interpretation as a prior based on data with sample size equal to \\(\\mathcal{A}+\\mathcal{B}\\). The posterior distribution that results would have a mean somewhere between the maximum likelihood value and that of the prior. With the stronger prior, the posterior is pulled closer to it. The second utilizes a more diffuse prior of \\(\\beta(2,2)\\). The result of using the vague prior is that the likelihood gets more weight with regard to the posterior. In fact, if we used a uniform distribution, we would essentially be doing the equivalent of maximum likelihood estimation. In that sense, many of the commonly used methods that implement maximum likelihood can be seen as a special case of a Bayesian approach. The third graph employs the initial \\(\\beta(10,10)\\) prior again, but this time we add more observations to the data. This serves to give more weight to the likelihood, which is what we want. As scientists, we’d want the evidence, i.e. data, to eventually outweigh our prior beliefs about the state of things the more we have of it. For an interactive demonstration of the above, see this. Posterior predictive At this point it is hoped you have a better understanding of the process of Bayesian estimation. Conceptually, one starts with prior beliefs about the state of the world and adds evidence to one’s understanding, ultimately coming to a conclusion that serves as a combination of evidence and prior belief. More concretely, we have a prior distribution regarding parameters, a distribution regarding the data given those parameters, and finally a posterior distribution that is the weighted combination of the two. However, there is yet another distribution of interest to us- the posterior predictive distribution. Stated simply, once we have the posterior distribution for \\(\\theta\\), we can then feed (possibly new or unobserved) data into the data generating process and get distributions for \\(\\tilde{y}\\). Where \\(\\tilde{y}\\) can regard any potential observation, we can distinguish it from the case where we use the current data to produce \\(y^{\\textrm{Rep}}\\), i.e. a replicate of \\(y\\). For example, if a regression model had predictor variables \\(X\\), the predictor variables are identical for producing \\(y^{\\textrm{Rep}}\\) as they were in modeling \\(y\\). However, \\(\\tilde{y}\\) might be based on any values \\(\\tilde{X}\\) that might be feasible or interesting, whether actually observed in the data or not. Since \\(y^{\\textrm{Rep}}\\) is an attempt to replicate the observed data based on the parameters \\(\\theta\\), we can compare our simulated data to the observed data to see how well they match. We can implement the simulation process with the data and model at hand, given a sample of values of \\(\\theta\\) drawn from the posterior. I provide the results of such a process with the following graph. Each bar graph of frequencies represents a replication of the 10 shots taken, i.e. \\(y^{\\textrm{Rep}}\\), given an estimate of \\(\\theta\\) from the posterior distribution (16 total). These are eleven plausible sets of 10 makes and misses, given \\(\\theta\\) shown against the observed. library(rstanarm) shotres = stan_glm( shots_01 ~ 1, data = data.frame(shots_01), family = &#39;binomial&#39;, iter = 500, warmup = 250, prior = student_t() ) # pp_check(shotres) With an understanding of the key elements of Bayesian inference in hand, we can proceed to more complex settings. Source Original code available at: https://m-clark.github.io/bayesian-basics/example.html "],["bayesian-t-test.html", "Bayesian t-test Data Setup Model Code Estimation Comparison Visualization Source", " Bayesian t-test The following is based on Kruschke’s 2012 JEP article ‘Bayesian estimation supersedes the t-test (BEST)’ with only minor changes to Stan model. It uses the JAGS/BUGS code in the paper’s Appendix B as the reference. Data Setup Create two groups of data. Play around with the specs if you like. library(tidyverse) set.seed(1234) N_g = 2 # N groups N_1 = 50 # N for group 1 N_2 = 50 # N for group 2 mu_1 = 1 # mean for group 1 mu_2 = -.5 # mean for group 1 sigma_1 = 1 # sd for group 1 sigma_2 = 1 # sd for group 1 y_1 = rnorm(N_1, mu_1, sigma_1) y_2 = rnorm(N_2, mu_2, sigma_2) y = c(y_1, y_2) group_id = as.numeric(gl(2, N_1)) # if unbalanced # group = 1:2 # group_id = rep(group, c(N_1,N_2)) d = data.frame(y, group_id) tidyext::num_by(d, y, group_id) # personal package, not necessary # A tibble: 2 x 11 # Groups: group_id [2] group_id Variable N Mean SD Min Q1 Median Q3 Max `% Missing` &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 y 50 0.5 0.9 -1.3 0 0.5 1 3.4 0 2 2 y 50 -0.4 1 -2.3 -1.1 -0.5 0.3 2 0 Model Code The Stan code. data { int&lt;lower = 1&gt; N; // sample size int&lt;lower = 2&gt; N_g; // number of groups vector[N] y; // response int&lt;lower = 1, upper = N_g&gt; group_id[N]; // group ID } transformed data{ real y_mean; // mean of y; see mu prior y_mean = mean(y); } parameters { vector[2] mu; // estimated group means and sd vector&lt;lower = 0&gt;[2] sigma; // Kruschke puts upper bound as well; ignored here real&lt;lower = 0, upper = 100&gt; nu; // df for t distribution } model { // priors // note that there is a faster implementation of this for stan, // and that the sd here is more informative than in Kruschke paper mu ~ normal(y_mean, 10); sigma ~ cauchy(0, 5); // Based on Kruschke; makes average nu 29 // might consider upper bound, as if too large then might as well switch to normal nu ~ exponential(1.0/29); // likelihood for (n in 1:N) { y[n] ~ student_t(nu, mu[group_id[n]], sigma[group_id[n]]); // compare to normal; remove all nu specifications if you do this; //y[n] ~ normal(mu[group_id[n]], sigma[group_id[n]]); } } generated quantities { vector[N] y_rep; // posterior predictive distribution real mu_diff; // mean difference real cohens_d; // effect size; see footnote 1 in Kruschke paper real CLES; // common language effect size real CLES2; // a more explicit approach; the mean should roughly equal CLES for (n in 1:N) { y_rep[n] = student_t_rng(nu, mu[group_id[n]], sigma[group_id[n]]); } mu_diff = mu[1] - mu[2]; cohens_d = mu_diff / sqrt(sum(sigma)/2); CLES = normal_cdf(mu_diff / sqrt(sum(sigma)), 0, 1); CLES2 = student_t_rng(nu, mu[1], sigma[1]) - student_t_rng(nu, mu[2], sigma[2]) &gt; 0; } Estimation Run the model. stan_data = list( N = length(y), N_g = N_g, group_id = group_id, y = y ) library(rstan) fit = sampling( bayes_t_test, data = stan_data, thin = 4 ) Comparison Let’s take a look. print( fit, pars = c(&#39;mu&#39;, &#39;sigma&#39;, &#39;mu_diff&#39;, &#39;cohens_d&#39;, &#39;CLES&#39;, &#39;CLES2&#39;, &#39;nu&#39;), probs = c(.025, .5, .975), digits = 3 ) Inference for Stan model: e9624a2b7528e50b8f8b9d0fb2b3c58c. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat mu[1] 0.516 0.004 0.132 0.278 0.514 0.783 1085 0.997 mu[2] -0.377 0.005 0.161 -0.701 -0.380 -0.067 878 0.998 sigma[1] 0.826 0.004 0.114 0.614 0.822 1.056 841 1.001 sigma[2] 1.024 0.004 0.125 0.807 1.021 1.289 928 0.998 mu_diff 0.893 0.007 0.204 0.497 0.891 1.277 899 0.997 cohens_d 0.932 0.007 0.216 0.500 0.932 1.340 907 0.997 CLES 0.743 0.002 0.049 0.638 0.745 0.828 909 0.997 CLES2 0.731 0.014 0.444 0.000 1.000 1.000 1008 1.000 nu 27.721 0.654 20.877 4.201 21.365 81.484 1020 1.000 Samples were drawn using NUTS(diag_e) at Tue Nov 17 12:21:54 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Now we extract quantities of interest for more processing/visualization. Compare population and observed data values to estimates in summary to the observed mean difference. y_rep = extract(fit, par = &#39;y_rep&#39;)$y_rep mu_diff = extract(fit, par = &#39;mu_diff&#39;)$mu_diff init = d %&gt;% group_by(group_id) %&gt;% summarise( mean = mean(y), sd = sd(y), ) means = init$mean sds = init$sd mu_1 - mu_2 # based on population values [1] 1.5 abs(diff(means)) # observed in data [1] 0.9074175 Compare estimated Cohen’s d. cohens_d = extract(fit, par = &#39;cohens_d&#39;)$cohens_d (mu_1 - mu_2) / sqrt((sigma_1 ^ 2 + sigma_2 ^ 2)/2) # population [1] 1.5 (means[1] - means[2]) / sqrt(sum(sds^2)/2) # observed [1] 0.9411788 mean(cohens_d) # bayesian estimate [1] 0.9319557 Common language effect size is the probability that a randomly selected score from one population will be greater than a randomly sampled score from the other. CLES = extract(fit, par=&#39;CLES&#39;)$CLES pnorm((mu_1 - mu_2) / sqrt(sigma_1^2 + sigma_2^2)) # population [1] 0.8555778 pnorm((means[1] - means[2]) / sqrt(sum(sds^2))) # observed [1] 0.7471391 mean(CLES) # bayesian estimate [1] 0.7426372 Compare to Welch’s t-test that does not assume equal variances. t.test(y_1, y_2) Welch Two Sample t-test data: y_1 and y_2 t = 4.7059, df = 95.633, p-value = 8.522e-06 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.5246427 1.2901923 sample estimates: mean of x mean of y 0.5469470 -0.3604705 Compare to BEST. Note that it requires coda, whose traceplot function will mask rstan’s. library(BEST) BESTout = BESTmcmc( y_1, y_2, numSavedSteps = 10000, thinSteps = 10, burnInSteps = 2000 ) summary(BESTout) mean median mode HDI% HDIlo HDIup compVal %&gt;compVal mu1 0.515 0.514 0.498 95 0.260 0.7636 mu2 -0.381 -0.381 -0.375 95 -0.693 -0.0813 muDiff 0.896 0.896 0.903 95 0.514 1.2984 0 100.00 sigma1 0.833 0.829 0.821 95 0.623 1.0593 sigma2 1.023 1.015 1.003 95 0.808 1.2810 sigmaDiff -0.189 -0.187 -0.189 95 -0.493 0.0996 0 9.96 nu 31.350 22.222 10.150 95 2.287 89.8401 log10nu 1.342 1.347 1.413 95 0.660 2.0527 effSz 0.966 0.964 0.960 95 0.521 1.4185 0 100.00 Visualization We can plot the posterior predictive distribution vs. observed data density. # by hand gdat = y_rep %&gt;% as.data.frame() %&gt;% pivot_longer(everything(), names_to = &#39;iteration&#39;) %&gt;% mutate(iteration = as.integer(str_extract(iteration, &#39;[0-9]+&#39;)), observation = factor(rep(1:nrow(y_rep), e = 100))) library(bayesplot) pp_check( stan_data$y, rstan::extract(fit, par = &#39;y_rep&#39;)$y_rep[1:10, ], fun = &#39;dens_overlay&#39; ) We can expand this to incorporate the separate groups and observed values. Solid lines and dots represent the observed data. Plots from the BEST model. walk(c(&quot;mean&quot;, &quot;sd&quot;, &quot;effect&quot;, &quot;nu&quot;), function(p) plot(BESTout, which = p)) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstant_testBEST.R "],["bayesian-linear-regression.html", "Bayesian Linear Regression Data Setup Model Code Estimation Comparison Visualize Source", " Bayesian Linear Regression The following provides a simple working example of a standard regression model using Stan via rstan. It will hopefully to allow some to more easily jump in to using Stan if they are comfortable with R. You would normally just use rstanarm or brms for such a model however. Data Setup Create a correlation matrix of one’s choosing assuming response as last column/row. This approach allows for some collinearity in the predictors. library(tidyverse) cormat = matrix( c( 1, .2, -.1, .3, .2, 1, .1, .2, -.1, .1, 1, .1, .3, .2, .1, 1 ), ncol = 4, byrow = TRUE ) cormat [,1] [,2] [,3] [,4] [1,] 1.0 0.2 -0.1 0.3 [2,] 0.2 1.0 0.1 0.2 [3,] -0.1 0.1 1.0 0.1 [4,] 0.3 0.2 0.1 1.0 cormat = Matrix::nearPD(cormat, corr = TRUE)$mat n = 1000 means = rep(0, ncol(cormat)) d = MASS::mvrnorm(n, means, cormat, empirical = TRUE) colnames(d) = c(&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;y&#39;) d[,&#39;y&#39;] = d[,&#39;y&#39;] - .1 # unnecessary, just to model a non-zero intercept str(d) num [1:1000, 1:4] 0.0259 -0.3699 -0.1475 -0.8668 -0.7021 ... - attr(*, &quot;dimnames&quot;)=List of 2 ..$ : NULL ..$ : chr [1:4] &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; &quot;y&quot; cor(d) X1 X2 X3 y X1 1.0 0.2 -0.1 0.3 X2 0.2 1.0 0.1 0.2 X3 -0.1 0.1 1.0 0.1 y 0.3 0.2 0.1 1.0 # Prepare for Stan # create X (add intercept column) and y for vectorized version later X = cbind(1, d[,1:3]); colnames(X) = c(&#39;Intercept&#39;, &#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;) y = d[,4] Model Code Initial preparation, create the data list object. dat = list( N = n, k = 4, y = y, X = X ) Create the Stan model code. data { // Data block; declarations only int&lt;lower = 0&gt; N; // Sample size int&lt;lower = 0&gt; k; // Dimension of model matrix matrix [N, k] X; // Model Matrix vector[N] y; // Target } /* transformed data { // Transformed data block; declarations and statements. None needed here. } */ parameters { // Parameters block; declarations only vector[k] beta; // Coefficient vector real&lt;lower = 0&gt; sigma; // Error scale } transformed parameters { // Transformed parameters block; declarations and statements. } model { // Model block; declarations and statements. vector[N] mu; mu = X * beta; // Linear predictor // priors beta ~ normal(0, 1); sigma ~ cauchy(0, 1); // With sigma bounded at 0, this is half-cauchy // likelihood y ~ normal(mu, sigma); } generated quantities { // Generated quantities block; declarations and statements. real rss; real totalss; real R2; // Calculate Rsq as a demonstration vector[N] y_hat; y_hat = X * beta; rss = dot_self(y - y_hat); totalss = dot_self(y - mean(y)); R2 = 1 - rss/totalss; } Estimation Run the model and examine results. The following assumes a character string or file (bayes_linreg) of the previous model code. library(rstan) fit = sampling( bayes_linreg, data = dat, thin = 4, verbose = FALSE ) Note the pars argument in the following. You must specify desired parameters or it will print out everything, including the y_hat, i.e. expected values. Also note that by taking into account the additional uncertainty estimating sigma, you get a shrunken Rsq (see Gelman &amp; Pardoe 2006 sec. 3). print( fit, digits_summary = 3, pars = c(&#39;beta&#39;, &#39;sigma&#39;, &#39;R2&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: 17507cf73e3a44aeee4c4249d3521a85. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat beta[1] -0.101 0.001 0.031 -0.161 -0.102 -0.037 1001 0.998 beta[2] 0.286 0.001 0.031 0.226 0.285 0.346 971 1.001 beta[3] 0.132 0.001 0.030 0.074 0.133 0.189 1028 1.001 beta[4] 0.114 0.001 0.029 0.056 0.114 0.171 920 1.006 sigma 0.938 0.001 0.020 0.900 0.937 0.981 967 0.999 R2 0.120 0.000 0.002 0.114 0.120 0.123 739 0.999 Samples were drawn using NUTS(diag_e) at Tue Nov 10 14:01:13 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Comparison Compare to basic lm result. modlm = lm(y ~ ., data.frame(d)) # Compare summary(modlm) Call: lm(formula = y ~ ., data = data.frame(d)) Residuals: Min 1Q Median 3Q Max -2.85461 -0.63666 0.03136 0.55565 2.94898 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.10000 0.02965 -3.372 0.000774 *** X1 0.28526 0.03051 9.349 &lt; 2e-16 *** X2 0.13141 0.03051 4.307 1.82e-05 *** X3 0.11538 0.03004 3.840 0.000131 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.9377 on 996 degrees of freedom Multiple R-squared: 0.1234, Adjusted R-squared: 0.1208 F-statistic: 46.73 on 3 and 996 DF, p-value: &lt; 2.2e-16 Visualize Visualize the posterior predictive distribution. # shinystan::launch_shinystan(fit) # diagnostic plots library(bayesplot) pp_check( dat$y, rstan::extract(fit, par = &#39;y_hat&#39;)$y_hat[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstan_linregwithprior.R "],["bayesian-beta-regression.html", "Bayesian Beta Regression Data Setup Model Code Estimation Comparison Visualization Source", " Bayesian Beta Regression The following provides an example of beta regression using Stan/rstan, with comparison to results with R’s betareg package. Data Setup Several data sets from are available betareg to play with, but as they are a bit problematic in one way or another I instead focus on a simple simulated data set. library(tidyverse) library(betareg) # Data for assessing the contribution of non-verbal IQ to children&#39;s reading # skills in dyslexic and non-dyslexic children. # issue: 30% of data has a value of .99 # data(&quot;ReadingSkills&quot;) # ?ReadingSkills # y = ReadingSkills$accuracy # # brmod = betareg(accuracy ~ dyslexia + iq, data = ReadingSkills) # X = cbind(1, scale(model.matrix(brmod)[,c(&#39;dyslexia&#39;,&#39;iq&#39;)], scale=F)) # or this, issue: ignores batch effects # data(&quot;GasolineYield&quot;) # ?GasolineYield # # y = GasolineYield$yield # X = cbind(1, scale(GasolineYield[,c(&#39;gravity&#39;,&#39;pressure&#39;,&#39;temp&#39;)])) # yet another data option, issue: only two binary predictors # data(WeatherTask) # ?WeatherTask # # y = WeatherTask$agreement # brmod = betareg(agreement ~ priming + eliciting, data = WeatherTask) # X = model.matrix(brmod) # simulated data; probably a better illustration, or at least better behaved one. set.seed(1234) N = 500 # Sample size x_1 = rnorm(N) # Predictors x_2 = rnorm(N) X = cbind(1, x_1, x_2) beta = c(-1, .2, -.3) mu = plogis(X %*% beta) # add noise if desired + rnorm(N, sd=.01) phi = 10 A = mu * phi B = (1 - mu) * phi y = rbeta(N, A, B) d = data.frame(x_1, x_2, y) qplot(y, geom=&#39;density&#39;) Model Code data { int&lt;lower=1&gt; N; // sample size int&lt;lower=1&gt; K; // K predictors vector&lt;lower=0,upper=1&gt;[N] y; // response matrix[N,K] X; // predictor matrix } parameters { vector[K] theta; // reg coefficients real&lt;lower=0&gt; phi; // dispersion parameter } transformed parameters{ vector[K] beta; beta = theta * 5; // same as beta ~ normal(0, 5); fairly diffuse } model { // model calculations vector[N] LP; // linear predictor vector[N] mu; // transformed linear predictor vector[N] A; // parameter for beta distn vector[N] B; // parameter for beta distn LP = X * beta; for (i in 1:N) { mu[i] = inv_logit(LP[i]); } A = mu * phi; B = (1.0 - mu) * phi; // priors theta ~ normal(0, 1); phi ~ cauchy(0, 5); // different options for phi //phi ~ inv_gamma(.001, .001); //phi ~ uniform(0, 500); // put upper on phi if using this // likelihood y ~ beta(A, B); } generated quantities { vector[N] y_rep; for (i in 1:N) { real mu; real A; real B; mu = inv_logit(X[i] * beta); A = mu * phi; B = (1.0 - mu) * phi; y_rep[i] = beta_rng(A, B); } } Estimation We create a data list for Stan and estimate the model. # Stan data list stan_data = list(N = length(y), K = ncol(X), y = y, X = X) library(rstan) fit = sampling( bayes_beta, data = stan_data, thin = 4, verbose = FALSE ) # model for later comparison brmod = betareg(y ~ ., data = d) Comparison Estimates are almost idential in this particular case. print( fit, pars = c(&#39;beta&#39;, &#39;phi&#39;), digits_summary = 3, probs = c(.025, .5, .975) ) Inference for Stan model: aefbc3393973f4d7342276f078ada46f. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat beta[1] -0.990 0.001 0.030 -1.048 -0.990 -0.926 901 0.999 beta[2] 0.128 0.001 0.028 0.076 0.127 0.184 921 1.000 beta[3] -0.327 0.001 0.029 -0.385 -0.326 -0.268 1098 0.998 phi 10.608 0.022 0.657 9.363 10.600 11.892 909 0.997 Samples were drawn using NUTS(diag_e) at Tue Nov 17 12:24:25 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). summary(brmod) Call: betareg(formula = y ~ ., data = d) Standardized weighted residuals 2: Min 1Q Median 3Q Max -4.1584 -0.5925 0.0781 0.6632 2.6250 Coefficients (mean model with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.98934 0.02982 -33.18 &lt; 2e-16 *** x_1 0.12662 0.02801 4.52 6.18e-06 *** x_2 -0.32758 0.03049 -10.74 &lt; 2e-16 *** Phi coefficients (precision model with identity link): Estimate Std. Error z value Pr(&gt;|z|) (phi) 10.6731 0.6545 16.31 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Type of estimator: ML (maximum likelihood) Log-likelihood: 339.1 on 4 Df Pseudo R-squared: 0.2057 Number of iterations: 12 (BFGS) + 2 (Fisher scoring) Visualization Posterior predictive check. library(bayesplot) pp_check( stan_data$y, rstan::extract(fit, par = &#39;y_rep&#39;)$y_rep[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstanBetaRegression.R "],["bayesian-mixed-model.html", "Bayesian Mixed Model Data Setup Model Code Estimation Comparison Visualize Source", " Bayesian Mixed Model Explore the classic sleepstudy example of lme4. Part of this code was based on that seen on this old Stan thread, but you can look at the underlying code for rstanarm or brms for a fully optimized approach compared to this conceptual one. Data Setup The data comes from the lme4 package. It deals with reaction time to some task vs. sleep deprivation over 10 days. library(tidyverse) library(lme4) data(sleepstudy) # ?sleepstudy dat = list( N = nrow(sleepstudy), I = n_distinct(sleepstudy$Subject), Subject = as.numeric(sleepstudy$Subject), Days = sleepstudy$Days, RT = sleepstudy$Reaction ) Model Code Create the Stan model code. data { // data setup int&lt;lower = 1&gt; N; // sample size int&lt;lower = 1&gt; I; // number of subjects vector&lt;lower = 0&gt;[N] RT; // Response: reaction time vector&lt;lower = 0&gt;[N] Days; // Days in study int&lt;lower = 1, upper = I&gt; Subject[N]; // Subject } transformed data { real IntBase; real RTsd; IntBase = mean(RT); // Intercept starting point RTsd = sd(RT); } parameters { real Intercept01; // fixed effects real beta01; vector&lt;lower = 0&gt;[2] sigma_u; // sd for ints and slopes real&lt;lower = 0&gt; sigma_y; // residual sd vector[2] gamma[I]; // individual effects cholesky_factor_corr[2] Omega_chol; // correlation matrix for random intercepts and slopes (chol decomp) } transformed parameters { vector[I] gammaIntercept; // individual effects (named) vector[I] gammaDays; real Intercept; real beta; Intercept = IntBase + Intercept01 * RTsd; beta = beta01 * 10; for (i in 1:I){ gammaIntercept[i] = gamma[i, 1]; gammaDays[i] = gamma[i, 2]; } } model { matrix[2,2] D; matrix[2,2] DC; vector[N] mu; // Linear predictor vector[2] gamma_mu; // vector of Intercept and beta D = diag_matrix(sigma_u); gamma_mu[1] = Intercept; gamma_mu[2] = beta; // priors Intercept01 ~ normal(0, 1); // example of weakly informative priors; beta01 ~ normal(0, 1); // remove to essentially duplicate lme4 via improper prior Omega_chol ~ lkj_corr_cholesky(2.0); sigma_u ~ cauchy(0, 2.5); // prior for RE scale sigma_y ~ cauchy(0, 2.5); // prior for residual scale DC = D * Omega_chol; for (i in 1:I) // loop for Subject random effects gamma[i] ~ multi_normal_cholesky(gamma_mu, DC); // likelihood for (n in 1:N) mu[n] = gammaIntercept[Subject[n]] + gammaDays[Subject[n]] * Days[n]; RT ~ normal(mu, sigma_y); } generated quantities { matrix[2, 2] Omega; // correlation of RE vector[N] y_hat; Omega = tcrossprod(Omega_chol); for (n in 1:N) y_hat[n] = gammaIntercept[Subject[n]] + gammaDays[Subject[n]] * Days[n]; } Estimation Run the model and examine results. The following assumes a character string or file (bayes_mixed) of the previous model code. library(rstan) fit = sampling( bayes_mixed, data = dat, thin = 4, verbose = FALSE ) Comparison Compare to lme4 result. print( fit, digits_summary = 3, pars = c(&#39;Intercept&#39;, &#39;beta&#39;, &#39;sigma_y&#39;, &#39;sigma_u&#39;, &#39;Omega[1,2]&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: 82d45c0b016c1733e4bbc33ce7699190. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat Intercept 252.070 0.206 6.596 239.208 252.029 265.202 1024 0.998 beta 10.260 0.049 1.557 7.138 10.301 13.257 1020 1.003 sigma_y 25.906 0.050 1.593 23.037 25.758 29.166 1025 1.002 sigma_u[1] 23.970 0.192 6.184 12.407 23.438 37.623 1036 0.999 sigma_u[2] 6.003 0.045 1.352 3.833 5.857 9.131 890 1.005 Omega[1,2] 0.116 0.009 0.265 -0.374 0.110 0.626 943 1.001 Samples were drawn using NUTS(diag_e) at Tue Nov 10 14:41:51 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). mod_lme = lmer(Reaction ~ Days + (Days | Subject), sleepstudy) mod_lme Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: Reaction ~ Days + (Days | Subject) Data: sleepstudy REML criterion at convergence: 1743.628 Random effects: Groups Name Std.Dev. Corr Subject (Intercept) 24.741 Days 5.922 0.07 Residual 25.592 Number of obs: 180, groups: Subject, 18 Fixed Effects: (Intercept) Days 251.41 10.47 cbind( coef(mod_lme)$Subject, matrix(get_posterior_mean(fit, par = c(&#39;gammaIntercept&#39;, &#39;gammaDays&#39;))[, &#39;mean-all chains&#39;], ncol = 2) ) (Intercept) Days 1 2 308 253.6637 19.6662617 254.9401 19.375944 309 211.0064 1.8476053 213.2745 1.574232 310 212.4447 5.0184295 214.7572 4.692619 330 275.0957 5.6529356 272.8280 6.004261 331 273.6654 7.3973743 272.3311 7.680547 332 260.4447 10.1951090 260.1453 10.206502 333 268.2456 10.2436499 267.0088 10.444546 334 244.1725 11.5418676 245.5201 11.319058 335 251.0714 -0.2848792 249.7039 -0.052901 337 286.2956 19.0955511 285.3455 19.191933 349 226.1949 11.6407181 228.6388 11.230074 350 238.3351 17.0815038 240.2908 16.699843 351 255.9830 7.4520239 254.7578 7.642752 352 272.2688 14.0032871 271.5638 14.099152 369 254.6806 11.3395008 254.8691 11.329122 370 225.7921 15.2897709 228.3114 14.818999 371 252.2122 9.4791297 252.2465 9.444850 372 263.7197 11.7513080 262.8522 11.886185 Visualize Visualize the posterior predictive distribution. # shinystan::launch_shinystan(fit) # diagnostic plots library(bayesplot) pp_check( dat$RT, rstan::extract(fit, par = &#39;y_hat&#39;)$y_hat[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstan_MixedModelSleepstudy_withREcorrelation.R "],["bayesian-mixed-mediation.html", "Bayesian Multilevel Mediation Data Setup Model Code Estimation Comparison Visualization Source", " Bayesian Multilevel Mediation The following demonstrates an indirect effect in a multilevel situation. It is based on Yuan &amp; MacKinnon 2009, which provides some Bugs code. In what follows we essentially have two models, one where the ‘mediator’ is the response; the other regards the primary response of interest (noted y). They will be referred to with Med or Main respectively. Data Setup The two main models are expressed conceptually as follows: \\[\\textrm{Mediator} \\sim \\alpha_{Med} + \\beta_{Med}\\cdot X\\] \\[y \\sim \\alpha_{Main} + \\beta_{1\\_{Main}}\\cdot X + \\beta_{2\\_{Main}}\\cdot \\textrm{Mediator}\\] However, there will be random effects for a grouping variable for each coefficient, i.e. random intercepts and slopes, for both the mediator model and the outcome model. Let’s create data to this effect. library(tidyverse) set.seed(8675309) # random effects for mediator model # create cov matrix of RE etc. with no covariance between model random effects # covmat_RE = matrix(c(1,-.15,0,0,0, # -.15,.4,0,0,0, # 0,0,1,-.1,.15, # 0,0,-.1,.3,0, # 0,0,.15,0,.2), nrow=5, byrow = T) # or with slight cov added to indirect coefficient RE; both matrices are pos def covmat_RE = matrix(c( 1.00, -0.15, 0.00, 0.00, 0.00, -0.15, 0.64, 0.00, 0.00, -0.10, 0.00, 0.00, 1.00, -0.10, 0.15, 0.00, 0.00, -0.10, 0.49, 0.00, 0.00, -0.10, 0.15, 0.00, 0.25), nrow = 5, byrow = TRUE) # inspect covmat_RE [,1] [,2] [,3] [,4] [,5] [1,] 1.00 -0.15 0.00 0.00 0.00 [2,] -0.15 0.64 0.00 0.00 -0.10 [3,] 0.00 0.00 1.00 -0.10 0.15 [4,] 0.00 0.00 -0.10 0.49 0.00 [5,] 0.00 -0.10 0.15 0.00 0.25 # inspect as correlation cov2cor(covmat_RE) [,1] [,2] [,3] [,4] [,5] [1,] 1.0000 -0.1875 0.0000000 0.0000000 0.00 [2,] -0.1875 1.0000 0.0000000 0.0000000 -0.25 [3,] 0.0000 0.0000 1.0000000 -0.1428571 0.30 [4,] 0.0000 0.0000 -0.1428571 1.0000000 0.00 [5,] 0.0000 -0.2500 0.3000000 0.0000000 1.00 # simulate re = MASS::mvrnorm(50, mu = rep(0, 5), Sigma = covmat_RE, empirical = TRUE) # random effects for mediator model ranef_alpha_Med = rep(re[, 1], e = 10) ranef_beta_Med = rep(re[, 2], e = 10) # random effects for main model ranef_alpha_Main = rep(re[, 3], e = 10) ranef_beta1_Main = rep(re[, 4], e = 10) ranef_beta2_Main = rep(re[, 5], e = 10) ## fixed effects alpha_Med = 2 beta_Med = .2 alpha_Main = 1 beta1_Main = .3 beta2_Main = -.2 # residual variance resid_Med = MASS::mvrnorm(500, 0, .75^2, empirical = TRUE) resid_Main = MASS::mvrnorm(500, 0, .50^2, empirical = TRUE) # Collect parameters for later comparison params = c( alpha_Med = alpha_Med, beta_Med = beta_Med, sigma_Med = sd(resid_Med), alpha_Main = alpha_Main, beta1_Main = beta1_Main, beta2_Main = beta2_Main, sigma_y = sd(resid_Main), alpha_Med_sd = sqrt(diag(covmat_RE)[1]), beta_Med_sd = sqrt(diag(covmat_RE)[2]), alpha_sd = sqrt(diag(covmat_RE)[3]), beta1_sd = sqrt(diag(covmat_RE)[4]), beta2_sd = sqrt(diag(covmat_RE)[5]) ) ranefs = cbind( gamma_alpha_Med = unique(ranef_alpha_Med), gamma_beta_Med = unique(ranef_beta_Med), gamma_alpha = unique(ranef_alpha_Main), gamma_beta1 = unique(ranef_beta1_Main), gamma_beta2 = unique(ranef_beta2_Main) ) Finally, we can create the data for analysis. X = rnorm(500, sd = 2) Med = (alpha_Med + ranef_alpha_Med) + (beta_Med + ranef_beta_Med) * X + resid_Med[, 1] y = (alpha_Main + ranef_alpha_Main) + (beta1_Main + ranef_beta1_Main) * X + (beta2_Main + ranef_beta2_Main) * Med + resid_Main[, 1] group = rep(1:50, e = 10) standat = list( X = X, Med = Med, y = y, Group = group, J = length(unique(group)), N = length(y) ) Model Code In the following, the cholesky decomposition of the RE covariance matrix is used for efficiency. As a rough guide, the default data where N = 500 took about 5 min to run for the main model with iter=12000 and warmup = 2000. data { int&lt;lower = 1&gt; N; // Sample size vector[N] X; // Explanatory variable vector[N] Med; // Mediator vector[N] y; // Response int&lt;lower = 1&gt; J; // Number of groups int&lt;lower = 1,upper = J&gt; Group[N]; // Groups } parameters{ real alpha_Med; // mediator model reg parameters and related real beta_Med; real&lt;lower = 0&gt; sigma_alpha_Med; real&lt;lower = 0&gt; sigma_beta_Med; real&lt;lower = 0&gt; sigmaMed; real alpha_Main; // main model reg parameters and related real beta1_Main; real beta2_Main; real&lt;lower = 0&gt; sigma_alpha; real&lt;lower = 0&gt; sigma_beta1; real&lt;lower = 0&gt; sigma_beta2; real&lt;lower = 0&gt; sigma_y; cholesky_factor_corr[5] Omega_chol; // chol decomp of corr matrix for random effects vector&lt;lower = 0&gt;[5] sigma_ranef; // sd for random effects matrix[J,5] gamma; // random effects } transformed parameters{ vector[J] gamma_alpha_Med; vector[J] gamma_beta_Med; vector[J] gamma_alpha; vector[J] gamma_beta1; vector[J] gamma_beta2; for (j in 1:J){ gamma_alpha_Med[j] = gamma[j,1]; gamma_beta_Med[j] = gamma[j,2]; gamma_alpha[j] = gamma[j,3]; gamma_beta1[j] = gamma[j,4]; gamma_beta2[j] = gamma[j,5]; } } model { vector[N] mu_y; // linear predictors for response and mediator vector[N] mu_Med; matrix[5,5] D; matrix[5,5] DC; // priors // mediator model // fixef // for scale params the cauchy is a little more informative here due // to the nature of the data sigma_alpha_Med ~ cauchy(0, 1); sigma_beta_Med ~ cauchy(0, 1); alpha_Med ~ normal(0, sigma_alpha_Med); beta_Med ~ normal(0, sigma_beta_Med); // residual scale sigmaMed ~ cauchy(0, 1); // main model // fixef sigma_alpha ~ cauchy(0, 1); sigma_beta1 ~ cauchy(0, 1); sigma_beta2 ~ cauchy(0, 1); alpha_Main ~ normal(0, sigma_alpha); beta1_Main ~ normal(0, sigma_beta1); beta2_Main ~ normal(0, sigma_beta2); // residual scale sigma_y ~ cauchy(0, 1); // ranef sampling via cholesky decomposition sigma_ranef ~ cauchy(0, 1); Omega_chol ~ lkj_corr_cholesky(2.0); D = diag_matrix(sigma_ranef); DC = D * Omega_chol; for (j in 1:J) // loop for Group random effects gamma[j] ~ multi_normal_cholesky(rep_vector(0, 5), DC); // Linear predictors for (n in 1:N){ mu_Med[n] = alpha_Med + gamma_alpha_Med[Group[n]] + (beta_Med + gamma_beta_Med[Group[n]]) * X[n]; mu_y[n] = alpha_Main + gamma_alpha[Group[n]] + (beta1_Main + gamma_beta1[Group[n]]) * X[n] + (beta2_Main + gamma_beta2[Group[n]]) * Med[n] ; } // sampling for primary models Med ~ normal(mu_Med, sigmaMed); y ~ normal(mu_y, sigma_y); } generated quantities{ real naive_ind_effect; real avg_ind_effect; real total_effect; matrix[5,5] cov_RE; vector[N] y_hat; cov_RE = diag_matrix(sigma_ranef) * tcrossprod(Omega_chol) * diag_matrix(sigma_ranef); naive_ind_effect = beta_Med*beta2_Main; avg_ind_effect = beta_Med*beta2_Main + cov_RE[2,5]; total_effect = avg_ind_effect + beta1_Main; for (n in 1:N){ y_hat[n] = alpha_Main + gamma_alpha[Group[n]] + (beta1_Main + gamma_beta1[Group[n]]) * X[n] + (beta2_Main + gamma_beta2[Group[n]]) * Med[n] ; } } Estimation Run the model and examine results. The following assumes a character string or file (bayes_med_model) of the previous model code. library(rstan) fit = sampling( bayes_med_model, data = standat, iter = 3000, warmup = 2000, thin = 4, cores = 4, control = list(adapt_delta = .99, max_treedepth = 15) ) Comparison Main parameters include fixed and random effect standard deviation, plus those related to indirect effect. mainpars = c( &#39;alpha_Med&#39;, &#39;beta_Med&#39;, &#39;sigmaMed&#39;, &#39;alpha_Main&#39;, &#39;beta1_Main&#39;, &#39;beta2_Main&#39;, &#39;sigma_y&#39;, &#39;sigma_ranef&#39;, &#39;naive_ind_effect&#39;, &#39;avg_ind_effect&#39;, &#39;total_effect&#39; ) print( fit, digits = 3, probs = c(.025, .5, 0.975), pars = mainpars ) Inference for Stan model: baab5ebe3046f0dc079803aa2af1fb6c. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat alpha_Med 2.010 0.008 0.153 1.694 2.015 2.302 337 1.005 beta_Med 0.161 0.007 0.124 -0.059 0.154 0.399 277 1.005 sigmaMed 0.744 0.001 0.026 0.694 0.743 0.798 982 1.001 alpha_Main 0.960 0.007 0.166 0.617 0.965 1.255 494 1.003 beta1_Main 0.274 0.008 0.123 0.026 0.275 0.506 222 1.006 beta2_Main -0.178 0.004 0.079 -0.333 -0.179 -0.014 507 1.004 sigma_y 0.500 0.001 0.019 0.465 0.499 0.540 924 0.997 sigma_ranef[1] 1.067 0.004 0.116 0.855 1.062 1.311 715 0.998 sigma_ranef[2] 0.857 0.003 0.087 0.705 0.847 1.049 953 1.000 sigma_ranef[3] 1.014 0.005 0.136 0.784 1.009 1.307 885 0.998 sigma_ranef[4] 0.789 0.003 0.088 0.635 0.781 0.975 1050 0.999 sigma_ranef[5] 0.479 0.002 0.061 0.378 0.476 0.609 876 1.003 naive_ind_effect -0.031 0.002 0.030 -0.100 -0.025 0.007 274 1.008 avg_ind_effect -0.149 0.003 0.074 -0.306 -0.143 -0.014 796 1.000 total_effect 0.125 0.008 0.137 -0.139 0.120 0.383 262 1.005 Samples were drawn using NUTS(diag_e) at Wed Nov 11 14:34:19 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can use a piecemeal mixed model via lme4 for initial comparison. However, it can’t directly estimate mediated effect, and it won’t pick up on correlation of random effects between models. library(lme4) mod_Med = lmer(Med ~ X + (1 + X | group)) summary(mod_Med) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: Med ~ X + (1 + X | group) REML criterion at convergence: 1456.7 Scaled residuals: Min 1Q Median 3Q Max -2.55601 -0.66077 -0.00442 0.57621 3.12555 Random effects: Groups Name Variance Std.Dev. Corr group (Intercept) 1.0315 1.0156 X 0.6688 0.8178 -0.27 Residual 0.5513 0.7425 Number of obs: 500, groups: group, 50 Fixed effects: Estimate Std. Error t value (Intercept) 2.0221 0.1480 13.665 X 0.2197 0.1172 1.874 Correlation of Fixed Effects: (Intr) X -0.257 mod_Main = lmer(y ~ X + Med + (1 + X + Med | group)) summary(mod_Main) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: y ~ X + Med + (1 + X + Med | group) REML criterion at convergence: 1211.9 Scaled residuals: Min 1Q Median 3Q Max -2.6843 -0.6232 0.0540 0.5721 2.5242 Random effects: Groups Name Variance Std.Dev. Corr group (Intercept) 0.9208 0.9596 X 0.5657 0.7521 -0.19 Med 0.2051 0.4529 0.52 -0.03 Residual 0.2479 0.4979 Number of obs: 500, groups: group, 50 Fixed effects: Estimate Std. Error t value (Intercept) 0.98312 0.15318 6.418 X 0.27141 0.10924 2.484 Med -0.19818 0.07234 -2.739 Correlation of Fixed Effects: (Intr) X X -0.144 Med 0.218 -0.055 # should equal the naive estimate in the following code lme_indirect_effect = fixef(mod_Med)[&#39;X&#39;] * fixef(mod_Main)[&#39;Med&#39;] Using the mediation package will provide a better estimate, and can handle this simple mixed model setting. # library(mediation) mediation_mixed = mediation::mediate( model.m = mod_Med, model.y = mod_Main, treat = &#39;X&#39;, mediator = &#39;Med&#39; ) summary(mediation_mixed) Causal Mediation Analysis Quasi-Bayesian Confidence Intervals Mediator Groups: group Outcome Groups: group Output Based on Overall Averages Across Groups Estimate 95% CI Lower 95% CI Upper p-value ACME -0.1240 -0.1997 -0.06 &lt;2e-16 *** ADE 0.2684 0.0581 0.48 0.012 * Total Effect 0.1445 -0.0852 0.36 0.226 Prop. Mediated -0.6432 -7.8472 9.27 0.226 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Sample Size Used: 500 Simulations: 1000 Extract parameters for comparison. pars_primary = get_posterior_mean(fit, pars = mainpars)[, 5] pars_re_cov = get_posterior_mean(fit, pars = &#39;Omega_chol&#39;)[, 5] # or take &#39;cov_RE&#39; from monte carlo sim pars_re = get_posterior_mean(fit, pars = c(&#39;sigma_ranef&#39;))[, 5] Fixed effects and random effect variances. param true bayes lme4 alpha_Med 2.00 2.010 2.022 beta_Med 0.20 0.161 0.220 sigma_Med 0.75 0.744 0.743 alpha_Main 1.00 0.960 0.983 beta1_Main 0.30 0.274 0.271 beta2_Main -0.20 -0.178 -0.198 sigma_y 0.50 0.500 0.498 alpha_Med_sd 1.00 1.067 1.016 beta_Med_sd 0.80 0.857 0.818 alpha_sd 1.00 1.014 0.960 beta1_sd 0.70 0.789 0.752 beta2_sd 0.50 0.479 0.453 Compare the covariances of the random effects. The first shows the full covariance matrix for mediator and outcome, then broken out separately. $true [,1] [,2] [,3] [,4] [,5] [1,] 1.00 -0.15 0.00 0.00 0.00 [2,] -0.15 0.64 0.00 0.00 -0.10 [3,] 0.00 0.00 1.00 -0.10 0.15 [4,] 0.00 0.00 -0.10 0.49 0.00 [5,] 0.00 -0.10 0.15 0.00 0.25 $estimates [,1] [,2] [,3] [,4] [,5] [1,] 1.14 -0.21 0.05 0.03 0.02 [2,] -0.21 0.72 0.05 0.00 -0.11 [3,] 0.05 0.05 0.98 -0.13 0.20 [4,] 0.03 0.00 -0.13 0.58 0.00 [5,] 0.02 -0.11 0.20 0.00 0.21 $vcov_Med [,1] [,2] [1,] 1.00 -0.15 [2,] -0.15 0.64 $vcov_Med_bayes [,1] [,2] [1,] 1.14 -0.21 [2,] -0.21 0.72 $vcov_Med_lme4 (Intercept) X (Intercept) 1.03 -0.22 X -0.22 0.67 $vcov_Main [,1] [,2] [,3] [1,] 1.00 -0.10 0.15 [2,] -0.10 0.49 0.00 [3,] 0.15 0.00 0.25 $vcov_Main_bayes [,1] [,2] [,3] [1,] 0.98 -0.13 0.20 [2,] -0.13 0.58 0.00 [3,] 0.20 0.00 0.21 $vcov_Main_lme4 (Intercept) X Med (Intercept) 0.92 -0.14 0.22 X -0.14 0.57 -0.01 Med 0.22 -0.01 0.21 Compare indirect effects true est naiveBayes naiveLme mediation_pack -0.14 -0.149 -0.031 -0.044 -0.124 Note that you can use brms to estimate this model as follows. The i allows the random effects to correlate across Mediator and outcome models. We have to convert the correlation estimate back to the covariance estimate to get the indirect value to compare to our base Stan result. library(brms) f = bf(Med ~ X + (1 + X |i| group)) + bf(y ~ X + Med + (1 + X + Med |i| group)) + set_rescor(FALSE) fit_brm = brm( f, data = data.frame(X, Med, y, group), cores = 4, thin = 4, seed = 1234 ) summary(fit_brm) Family: MV(gaussian, gaussian) Links: mu = identity; sigma = identity mu = identity; sigma = identity Formula: Med ~ X + (1 + X | i | group) y ~ X + Med + (1 + X + Med | i | group) Data: data.frame(X, Med, y, group) (Number of observations: 500) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 4; total post-warmup samples = 1000 Group-Level Effects: ~group (Number of levels: 50) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Med_Intercept) 1.08 0.12 0.86 1.34 1.00 963 853 sd(Med_X) 0.86 0.09 0.70 1.06 1.00 713 787 sd(y_Intercept) 1.02 0.13 0.79 1.31 1.00 922 955 sd(y_X) 0.80 0.09 0.64 1.00 1.00 867 650 sd(y_Med) 0.48 0.07 0.37 0.63 1.00 887 755 cor(Med_Intercept,Med_X) -0.24 0.14 -0.50 0.03 1.00 619 589 cor(Med_Intercept,y_Intercept) 0.06 0.16 -0.25 0.36 1.00 953 949 cor(Med_X,y_Intercept) 0.06 0.15 -0.25 0.35 1.00 810 990 cor(Med_Intercept,y_X) 0.03 0.15 -0.25 0.32 1.01 910 915 cor(Med_X,y_X) -0.00 0.15 -0.28 0.30 1.00 956 728 cor(y_Intercept,y_X) -0.18 0.15 -0.46 0.13 1.00 741 819 cor(Med_Intercept,y_Med) 0.03 0.16 -0.29 0.37 1.00 947 902 cor(Med_X,y_Med) -0.29 0.15 -0.56 0.02 1.00 1042 782 cor(y_Intercept,y_Med) 0.45 0.17 0.10 0.75 1.00 841 782 cor(y_X,y_Med) -0.01 0.16 -0.31 0.30 1.00 1042 850 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Med_Intercept 2.02 0.15 1.72 2.32 1.00 722 849 y_Intercept 0.99 0.17 0.67 1.33 1.00 796 904 Med_X 0.22 0.13 -0.02 0.46 1.00 711 908 y_X 0.31 0.12 0.07 0.56 1.00 899 995 y_Med -0.20 0.08 -0.35 -0.05 1.00 1001 994 Family Specific Parameters: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sigma_Med 0.74 0.03 0.69 0.80 1.00 973 889 sigma_y 0.50 0.02 0.46 0.54 1.00 986 960 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). hypothesis( fit_brm, &#39;b_y_Med*b_Med_X + cor_group__Med_X__y_Med*sd_group__Med_X*sd_group__y_Med = 0&#39;, class = NULL, seed = 1234 ) Hypothesis Tests for class : Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star 1 (b_y_Med*b_Med_X+... = 0 -0.17 0.08 -0.33 -0.03 NA NA * --- &#39;CI&#39;: 90%-CI for one-sided and 95%-CI for two-sided hypotheses. &#39;*&#39;: For one-sided hypotheses, the posterior probability exceeds 95%; for two-sided hypotheses, the value tested against lies outside the 95%-CI. Posterior probabilities of point hypotheses assume equal prior probabilities. Visualization library(bayesplot) pp_check( standat$y, rstan::extract(fit, par = &#39;y_hat&#39;)$y_hat[1:10, ], fun = &#39;dens_overlay&#39; ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/rstan_multilevelMediation.R "],["bayesian-irt.html", "Bayesian IRT One Parameter IRT Two Parameter IRT Three Parameter IRT Four Parameter IRT Source", " Bayesian IRT The following shows some code demonstration for one through four parameter IRT models, though will only extensively explore the first two. You can learn more about IRT models in general in my structural equation modeling document. One Parameter IRT Data Setup This data set has the responses of 316 participants on 24 items of a questionnaire on verbal aggression. Other covariates are also provided. For simplicity I will focus on the four ‘DoShout’ items. library(tidyverse) data(&quot;VerbAgg&quot;, package = &quot;lme4&quot;) glimpse(VerbAgg) Rows: 7,584 Columns: 9 $ Anger &lt;int&gt; 20, 11, 17, 21, 17, 21, 39, 21, 24, 16, 15, 18, 36, 22, 16, 18, 23, 16, 21, 25, 22, 15, 26, 13, 33, 17, 17, 22, 21, 17, 19, 18, 33, 19, 25, 17, 12, 14, 25, 22, 20, 25, 12, 16, 23, 19, 22, 15, 25, 35, 24… $ Gender &lt;fct&gt; M, M, F, F, F, F, F, F, F, F, F, F, M, M, F, F, F, F, F, F, F, F, F, F, F, F, F, F, M, F, F, M, M, F, F, F, F, F, M, M, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, M, F, F, F, F, F, F, F, F, F… $ item &lt;fct&gt; S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantCurse, S1WantC… $ resp &lt;ord&gt; no, no, perhaps, perhaps, perhaps, yes, yes, no, no, yes, perhaps, yes, yes, yes, perhaps, perhaps, perhaps, perhaps, no, perhaps, perhaps, yes, yes, perhaps, no, no, yes, yes, perhaps, yes, yes, perhap… $ id &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,… $ btype &lt;fct&gt; curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse, curse,… $ situ &lt;fct&gt; other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other, other,… $ mode &lt;fct&gt; want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want, want… $ r2 &lt;fct&gt; N, N, Y, Y, Y, Y, Y, N, N, Y, Y, Y, Y, Y, Y, Y, Y, Y, N, Y, Y, Y, Y, Y, N, N, Y, Y, Y, Y, Y, Y, N, Y, Y, Y, N, N, N, N, Y, N, Y, N, N, Y, Y, N, N, Y, N, N, Y, Y, N, Y, Y, Y, N, N, Y, Y, Y, Y, Y, N, Y, Y… verbagg_items = VerbAgg %&gt;% filter(btype == &#39;shout&#39;, situ == &#39;self&#39;) %&gt;% select(id, item, r2) head(verbagg_items) id item r2 1 1 S3WantShout Y 2 2 S3WantShout N 3 3 S3WantShout N 4 4 S3WantShout N 5 5 S3WantShout N 6 6 S3WantShout Y verbagg_items_wide = verbagg_items %&gt;% pivot_wider(id_cols = id, names_from = item, names_prefix = &#39;item_&#39;, values_from = r2) head(verbagg_items_wide) # A tibble: 6 x 5 id item_S3WantShout item_S4WantShout item_S3DoShout item_S4DoShout &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 1 Y N N Y 2 2 N N N N 3 3 N N N N 4 4 N N N N 5 5 N N N N 6 6 Y N N N While we often think of the data in ‘wide form’, with one row per person and multiple columns respective to each item, and the subsequent Stan code will use that, it is generally both tidier and more straightforward for modeling with the long format, where one can use standard mixed model approaches. r2 is the target variable of interest in that case. In the long format, the model for a single person is as follows, where \\(Z\\) is the latent person (\\(p\\))score, and \\(i\\) is the \\(i^{th}\\) item. \\[\\textrm{logit}(\\pi) = \\textrm{disc} (Z_p - \\beta_i)\\] Another formulation is the following, and corresponds to what brms will use. \\[\\textrm{logit}(\\pi) = \\beta_i + \\textrm{disc}\\cdot Z_p\\] Model Code data { int N; // Number of people int J; // Number of items int Y[N,J]; // Binary Target } transformed data{ } parameters { vector[J] difficulty; // Item difficulty real&lt;lower = 0&gt; discrim; // Item discrimination (constant) vector[N] Z; // Latent person ability } model { matrix[N, J] lmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); for (j in 1:J){ lmat[,j] = discrim * (Z - difficulty[j]); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli_logit(lmat[,j]); } Estimation First we create a Stan-friendly data list and then estimate the model. The following assumes a character string or file (bayes_irt1_model) of the previous model code. verbagg_items_wide_mat = apply( as.matrix(verbagg_items_wide[, -1]) == &#39;Y&#39;, 2, as.integer ) stan_data = list( N = nrow(verbagg_items_wide_mat), J = ncol(verbagg_items_wide_mat), Y = verbagg_items_wide_mat ) library(rstan) fit_1pm = sampling( bayes_irt1_model, data = stan_data, thin = 4 ) Comparison Now we compare to brms. I use the author’s article as a guide for this model, and note again that it is following the second parameterization depicted above. library(brms) # half normal for variance parameter, full for coefficients prior_1pm &lt;- prior(&quot;normal(0, 3)&quot;, class = &quot;sd&quot;, group = &quot;id&quot;) + prior(&quot;normal(0, 3)&quot;, class = &quot;b&quot;) brms_1pm = brm( r2 ~ 0 + item + (1 | id), data = verbagg_items, family = bernoulli, prior = prior_1pm, thin = 4, cores = 4 ) If you want to compare to standard IRT in either parameterization, you can use the ltm package. library(ltm) irt_rasch_par1 = rasch(verbagg_items_wide_mat, IRT.param = FALSE) irt_rasch_par2 = rasch(verbagg_items_wide_mat, IRT.param = TRUE) print( fit_1pm, digits = 3, par = c(&#39;discrim&#39;, &#39;difficulty&#39;), probs = c(.025, .5, 0.975) ) Inference for Stan model: c6b556c97942aceb622c6a43bd60dc47. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat discrim 1.870 0.007 0.194 1.521 1.868 2.269 804 1.001 difficulty[1] 0.964 0.004 0.115 0.756 0.957 1.201 883 0.999 difficulty[2] 0.665 0.004 0.108 0.460 0.663 0.873 928 0.998 difficulty[3] 1.842 0.006 0.176 1.535 1.822 2.200 894 0.997 difficulty[4] 1.253 0.004 0.132 1.013 1.245 1.524 886 1.003 Samples were drawn using NUTS(diag_e) at Thu Nov 12 16:29:38 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). summary(brms_1pm) Family: bernoulli Links: mu = logit Formula: r2 ~ 0 + item + (1 | id) Data: verbagg_items (Number of observations: 1264) Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 4; total post-warmup samples = 1000 Group-Level Effects: ~id (Number of levels: 316) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Intercept) 1.91 0.21 1.52 2.33 1.00 830 884 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS itemS3WantShout -1.79 0.21 -2.19 -1.37 1.00 919 882 itemS4WantShout -1.23 0.20 -1.61 -0.82 1.00 924 909 itemS3DoShout -3.43 0.30 -4.05 -2.88 1.00 899 946 itemS4DoShout -2.34 0.24 -2.84 -1.88 1.00 893 951 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). brms_diff = fixef(brms_1pm)[,&#39;Estimate&#39;] brms_discrim = VarCorr(brms_1pm)$id$sd[1] fit_params = summary(fit_1pm, digits = 3, par = c(&#39;discrim&#39;, &#39;difficulty&#39;))$summary[,&#39;mean&#39;] After extracting, we can show either parameterization for either model. For example, brms item difficulties = our model -discrim*difficulties. # A tibble: 5 x 5 parma model brms model_par2 brms_par1 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 discrim 1.87 1.91 1.87 1.91 2 difficulty[1] 0.964 -1.79 -1.80 0.937 3 difficulty[2] 0.665 -1.23 -1.24 0.643 4 difficulty[3] 1.84 -3.43 -3.45 1.80 5 difficulty[4] 1.25 -2.34 -2.34 1.23 Two Parameter IRT Now we can try a two parameter model. Data setup is the same as before. Model Code data { int N; int J; int Y[N, J]; } parameters { vector[J] difficulty; vector&lt;lower = 0&gt;[J] discrim; // Now per-item discrimination vector[N] Z; } model { matrix[N, J] lmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); for (j in 1:J){ lmat[,j] = discrim[j] * (Z - difficulty[j]); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli_logit(lmat[,j]); } Estimation First, our custom Stan model. The following assumes a character string or file (bayes_irt2_model) of the previous model code. library(rstan) fit_2pm = sampling( bayes_irt2_model, data = stan_data, thin = 4, iter = 4000, warmup = 3000, cores = 4, control = list(adapt_delta = .99) ) Comparison Now we compare to brms. I use the author’s article as a guide for this model, and note that it is following the second parameterization. Took a little over 30 seconds on my machine, though of course you may experience differently. library(brms) # half normal for variance parameter, full for coefficients prior_2pm &lt;- prior(&quot;normal(0, 5)&quot;, class = &quot;b&quot;, nlpar = &quot;Z&quot;) + prior(&quot;normal(0, 5)&quot;, class = &quot;b&quot;, nlpar = &quot;logdiscr&quot;) + prior(&quot;constant(1)&quot;, class = &quot;sd&quot;, group = &quot;id&quot;, nlpar = &quot;Z&quot;) + prior(&quot;normal(0, 3)&quot;, class = &quot;sd&quot;, group = &quot;item&quot;, nlpar = &quot;Z&quot;) + prior(&quot;normal(0, 3)&quot;, class = &quot;sd&quot;, group = &quot;item&quot;, nlpar = &quot;logdiscr&quot;) formula_2pm = bf( r2 ~ exp(logdiscr) * Z, Z ~ 1 + (1 |i| item) + (1 | id), logdiscr ~ 1 + (1 |i| item), nl = TRUE ) brms_2pm = brm( formula_2pm, data = verbagg_items, family = bernoulli, prior = prior_2pm, thin = 4, iter = 4000, warmup = 3000, cores = 4, control = list(adapt_delta = .99, max_treedepth = 15) ) print( fit_2pm, digits = 3, par = c(&#39;discrim&#39;, &#39;difficulty&#39;), probs = c(.025, .5, 0.975) ) Inference for Stan model: 7d8bd495d2c2cd004b52077acf14f00f. 4 chains, each with iter=4000; warmup=3000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat discrim[1] 1.034 0.010 0.253 0.595 1.009 1.580 674 1.007 discrim[2] 2.985 0.095 1.717 1.624 2.600 6.708 325 1.007 discrim[3] 1.458 0.014 0.406 0.738 1.429 2.356 848 1.003 discrim[4] 4.545 0.143 3.244 2.061 3.627 12.648 515 0.997 difficulty[1] 1.421 0.012 0.320 0.939 1.371 2.159 714 1.003 difficulty[2] 0.587 0.004 0.104 0.399 0.579 0.816 865 1.000 difficulty[3] 2.204 0.016 0.454 1.571 2.114 3.422 788 1.004 difficulty[4] 1.026 0.005 0.122 0.808 1.018 1.299 679 0.999 Samples were drawn using NUTS(diag_e) at Thu Nov 12 18:31:44 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). summary(brms_2pm) Family: bernoulli Links: mu = logit Formula: r2 ~ exp(logdiscr) * Z Z ~ 1 + (1 | i | item) + (1 | id) logdiscr ~ 1 + (1 | i | item) Data: verbagg_items (Number of observations: 1264) Samples: 4 chains, each with iter = 4000; warmup = 3000; thin = 4; total post-warmup samples = 1000 Group-Level Effects: ~id (Number of levels: 316) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Z_Intercept) 1.00 0.00 1.00 1.00 1.00 1000 1000 ~item (Number of levels: 4) Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS sd(Z_Intercept) 1.12 0.75 0.34 3.12 1.01 763 822 sd(logdiscr_Intercept) 0.93 0.74 0.10 2.79 1.02 440 813 cor(Z_Intercept,logdiscr_Intercept) 0.22 0.49 -0.84 0.94 1.00 935 918 Population-Level Effects: Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS Z_Intercept -1.23 0.63 -2.58 -0.02 1.00 691 818 logdiscr_Intercept 0.67 0.56 -0.49 1.73 1.01 636 737 Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1). brms_diff = coef(brms_2pm)$item[,,&#39;Z_Intercept&#39;][,&#39;Estimate&#39;] brms_discrim = exp(coef(brms_2pm)$item[,,&#39;logdiscr_Intercept&#39;][,&#39;Estimate&#39;]) fit_diff = summary(fit_2pm, digits = 3, par = &#39;difficulty&#39;)$summary[,&#39;mean&#39;] fit_discrim = summary(fit_2pm, digits = 3, par = &#39;discrim&#39;)$summary[,&#39;mean&#39;] # A tibble: 8 x 3 parma model brms &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 discrim[1] 1.03 1.18 2 discrim[2] 2.99 2.60 3 discrim[3] 1.46 1.59 4 discrim[4] 4.54 2.99 5 difficulty[1] 1.42 -1.27 6 difficulty[2] 0.587 -0.617 7 difficulty[3] 2.20 -2.01 8 difficulty[4] 1.03 -1.10 Here is the non-Bayesian demo if interested. library(ltm) irt_2pm_par1 = ltm(verbagg_items_wide_mat ~ z1, IRT.param = FALSE) irt_2pm_par2 = ltm(verbagg_items_wide_mat ~ z1, IRT.param = TRUE) coef(irt_2pm_par1) coef(irt_2pm_par2) Three Parameter IRT For the three parameter model I only show the Stan code. This model adds a per-item guessing parameter, which serves as a lower bound, to the two parameter model. data { int N; int J; int Y[N,J]; } parameters { vector[J] difficulty; vector&lt;lower = 0&gt;[J] discrim; vector&lt;lower = 0, upper = .25&gt;[J] guess; vector[N] Z; } model { matrix[N, J] pmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); guess ~ beta(1, 19); for (j in 1:J){ pmat[,j] = guess[j] + (1 - guess[j]) * inv_logit(discrim[j] * (Z - difficulty[j])); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli(pmat[,j]); } Four Parameter IRT For the four parameter model I only show the Stan code. This model adds a per-item ceiling parameter, which serves as an upper bound, to the three parameter model. data { int N; int J; int Y[N,J]; } parameters { vector[J] difficulty; vector&lt;lower = 0&gt;[J] discrim; vector&lt;lower = 0, upper = .25&gt;[J] guess; vector&lt;lower = .95, upper = 1&gt;[J] ceiling; vector[N] Z; } model { matrix[N, J] pmat; // priors Z ~ normal(0, 1); discrim ~ student_t(3, 0, 5); difficulty ~ student_t(3, 0, 5); guess ~ beta(1, 19); ceiling ~ beta(49, 1); for (j in 1:J){ pmat[,j] = guess[j] + (ceiling[j] - guess[j]) * inv_logit(discrim[j] * (Z - difficulty[j])); } // likelihood for (j in 1:J) Y[,j] ~ bernoulli(pmat[,j]); } Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/Bayesian/StanBugsJags/IRT_models "],["bayesian-cfa.html", "Bayesian CFA Data Setup Model Code Estimation Comparison Source", " Bayesian CFA Data Setup For an empirical data set we can use the big five data from the psych package. For simplicity, I will only examine three of the five factors, and only the first 3 items of each. I have a version that has already reverse-scored the items that need be, but this is not necessary. We we will restrict ourselves to complete data and only a sample of 280 observations (~10%). library(tidyverse) data(&#39;big_five&#39;, package = &#39;noiris&#39;) # data(&#39;bfi&#39;, package = &#39;psych&#39;) big_five_no_miss = big_five %&gt;% select(matches(&#39;(^E|^O|^N)[1-3]&#39;)) %&gt;% drop_na() %&gt;% slice_sample(n = 280) Model Code The model code is quite verbose, and definitely not efficient, but hopefully clarifies this ‘latent linear model’ underlying the observations. data { int&lt;lower = 1&gt; N; // sample size int&lt;lower = 1&gt; P; // number of variables int&lt;lower = 1&gt; K; // number of factors matrix[N,P] X; // data matrix of order [N,P] } transformed data { int&lt;lower = 1&gt; L; L = P - K; // Number of free loadings } parameters { vector[P] b; // intercepts vector[L] lambda01; // initial factor loadings matrix[N, K] FS; // factor scores, matrix of order [N,K] corr_matrix[K] phi; // factor correlations vector&lt;lower = 0, upper = 2&gt;[K] sd_lv; // std dev of the latent factors vector&lt;lower = 0, upper = 2&gt;[P] sd_x; // std dev of the disturbances vector&lt;lower = 0, upper = 5&gt;[L] sd_lambda; // hyper parameter for loading std dev } transformed parameters { vector[L] lambda; // factor loadings lambda = lambda01 .* sd_lambda; // lambda as normal(0, sd_lambda) } model { matrix[N,P] mu; matrix[K,K] Ld; vector[K] muFactors; muFactors = rep_vector(0, K); // Factor means, set to zero Ld = diag_matrix(sd_lv) * cholesky_decompose(phi); for(n in 1:N) { mu[n,1] = b[1] + FS[n,1]; // Extraversion mu[n,2] = b[2] + FS[n,1]*lambda[1]; mu[n,3] = b[3] + FS[n,1]*lambda[2]; mu[n,4] = b[4] + FS[n,2]; // Neuroticism mu[n,5] = b[5] + FS[n,2]*lambda[3]; mu[n,6] = b[6] + FS[n,2]*lambda[4]; mu[n,7] = b[7] + FS[n,3]; // Openness mu[n,8] = b[8] + FS[n,3]*lambda[5]; mu[n,9] = b[9] + FS[n,3]*lambda[6]; } // priors phi ~ lkj_corr(2.0); sd_x ~ cauchy(0, 2.5); sd_lambda ~ cauchy(0, 2.5); sd_lv ~ cauchy(0, 2.5); b ~ normal(0, 10); lambda01 ~ normal(0, 1); // likelihood for(i in 1:N){ FS[i] ~ multi_normal_cholesky(muFactors, Ld); X[i] ~ normal(mu[i], sd_x); } } Estimation Note that this will likely take a while with the full data set, but you can bump the iterations down or decrease the sample size and get roughly the same estimates. With these defaults you might get some divergent warnings or other issues to possibly deal with. stan_data = list( N = nrow(big_five_no_miss), P = ncol(big_five_no_miss), K = 3, X = big_five_no_miss ) library(rstan) fit_cfa = sampling( bayes_cfa, data = stan_data, thin = 4, cores = 4, control = list(adapt_delta = .95, max_treedepth = 15) ) Comparison Here are the raw factor loadings and factor correlations. print( fit_cfa, digits = 3, par = c(&#39;lambda&#39;, &#39;phi&#39;), probs = c(.025, .5, 0.975) ) Inference for Stan model: 6522f4cb208793aee047a3160d67d5b3. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat lambda[1] 0.838 0.008 0.138 0.599 0.835 1.127 310 1.007 lambda[2] 0.564 0.006 0.109 0.364 0.562 0.787 330 1.002 lambda[3] 1.017 0.004 0.078 0.872 1.014 1.182 384 1.006 lambda[4] 0.858 0.003 0.076 0.711 0.856 1.011 680 1.008 lambda[5] 0.884 0.008 0.185 0.552 0.877 1.276 551 1.004 lambda[6] 1.304 0.020 0.255 0.878 1.280 1.898 171 1.021 phi[1,1] 1.000 NaN 0.000 1.000 1.000 1.000 NaN NaN phi[1,2] -0.195 0.003 0.079 -0.345 -0.195 -0.035 833 1.004 phi[1,3] 0.483 0.003 0.080 0.322 0.483 0.649 714 1.004 phi[2,1] -0.195 0.003 0.079 -0.345 -0.195 -0.035 833 1.004 phi[2,2] 1.000 0.000 0.000 1.000 1.000 1.000 970 0.996 phi[2,3] -0.054 0.003 0.080 -0.205 -0.054 0.093 970 0.999 phi[3,1] 0.483 0.003 0.080 0.322 0.483 0.649 714 1.004 phi[3,2] -0.054 0.003 0.080 -0.205 -0.054 0.093 970 0.999 phi[3,3] 1.000 0.000 0.000 1.000 1.000 1.000 888 0.996 Samples were drawn using NUTS(diag_e) at Mon Nov 16 13:55:22 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). We can compare our results with those of the lavaan package, which uses standard maximum likelihood via the cfa function default settings. library(lavaan) mod = &quot; E =~ E1 + E2 + E3 N =~ N1 + N2 + N3 O =~ O1 + O2 + O3 &quot; fit_lav = cfa(mod, data = big_five_no_miss) # summary(fit_lav) The following shows how to extract the parameter estimates and convert them to standardized form, followed by how to get the parameter estimates from the lavaan output. # loadings lambda = get_posterior_mean(fit_cfa, par = &#39;lambda&#39;)[,&#39;mean-all chains&#39;] lambda = c(1, lambda[1:2], 1, lambda[3:4], 1, lambda[5:6]) # standard deviations of factors and observed sd_F = rep(get_posterior_mean(fit_cfa, par = &#39;sd_lv&#39;)[,&#39;mean-all chains&#39;], e = 3) x_sd = apply(stan_data$X, 2, sd) # standardize lambda_std_F = sd_F*lambda lambda_std_all = sd_F/x_sd*lambda # get factor correlations fit_cors = matrix(get_posterior_mean(fit_cfa, par = &#39;phi&#39;)[, 5], 3, 3) lav_par = parameterEstimates(fit_lav, standardized = TRUE) First we compare the loadings, both raw and standardized (either standardize the latent variable only or the latent and observed variables). lhs op rhs est std.lv std.all std.nox lambda_est lambda_std_lv lambda_std_all E =~ E1 1.000 0.961 0.589 0.589 1.000 1.183 0.705 E =~ E2 1.165 1.119 0.694 0.694 0.838 0.992 0.617 E =~ E3 0.851 0.818 0.605 0.605 0.564 0.668 0.512 N =~ N1 1.000 1.351 0.858 0.858 1.000 1.343 0.821 N =~ N2 0.949 1.283 0.837 0.837 1.017 1.366 0.882 N =~ N3 0.779 1.053 0.660 0.660 0.858 1.152 0.698 O =~ O1 1.000 0.592 0.526 0.526 1.000 0.694 0.610 O =~ O2 0.946 0.560 0.361 0.361 0.884 0.613 0.403 O =~ O3 1.568 0.929 0.771 0.771 1.304 0.905 0.737 fit_cors E N O E 1.0000000 -0.19506672 0.48343102 N -0.1950667 1.00000000 -0.05415968 O 0.4834310 -0.05415968 1.00000000 lav_cors E N O E 1.0000000 -0.19280881 0.55486978 N -0.1928088 1.00000000 -0.09118813 O 0.5548698 -0.09118813 1.00000000 Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/StanBugsJags/cfa "],["bayesian-non-parametric.html", "Bayesian Nonparametric Models Chinese Restaurant Process Indian Buffet Process Source", " Bayesian Nonparametric Models The following provides some conceptual code for the Chinese restaurant and Indian buffet process for categorical and continuous/combinations of categorical latent variables respectively. For more detail, see the Bayesian nonparametric section of my structural equation modeling document. Chinese Restaurant Process To start, we have a couple functions demonstrating the Chinese restaurant process. The first is succinct and more conceptual, but notably slower. crp &lt;- function(alpha, n) { table_assignments = 1 for (i in 2:n){ table_counts = table(table_assignments) # counts of table assignments nt = length(table_counts) # number of tables table_prob = table_counts/(i - 1 + alpha) # probabilities of previous table assignments # sample assignment based on probability of current tables and potential next table current_table_assignment = sample(1:(nt+1), 1, prob = c(table_prob, 1 - sum(table_prob))) # concatenate new to previous table assignments table_assignments = c(table_assignments, current_table_assignment) } table_assignments } The following function is similar to the restaurant function here https://github.com/mcdickenson/shinyapps, and notably faster. crpF &lt;- function(alpha, n) { table_assignments = c(1, rep(NA, n-1)) table_counts = 1 for (i in 2:n){ init = c(table_counts, alpha) table_prob = init/sum(init) current_table_assignment = sample(seq_along(init), 1, prob = table_prob) table_assignments[i] = current_table_assignment if (current_table_assignment == length(init)) { table_counts[current_table_assignment] = 1 } else { table_counts[current_table_assignment] = table_counts[current_table_assignment] + 1 } } table_assignments } # library(microbenchmark) # test = microbenchmark(crp(alpha = 1, n = 1000), # crpF(alpha = 1, n = 1000), times = 100) # test # ggplot2::autoplot(test) Visualize some examples at a given setting. out = replicate(5 , crpF(alpha = 1, n = 500), simplify = FALSE) library(tidyverse) map_df( out, function(x) data.frame(table(x)), .id = &#39;result&#39; ) %&gt;% rename(cluster = x) %&gt;% ggplot(aes(cluster, Freq)) + geom_col() + facet_grid(~ result) Visualize cluster membership. With smaller alpha, there is more tendency to stick to fewer clusters. set.seed(123) n = 100 crp_1 = crp(alpha = 1, n = n) crp_1_mat = matrix(0, nrow = n, ncol = n_distinct(crp_1)) for (i in 1:n_distinct(crp_1)) { crp_1_mat[, i] = ifelse(crp_1 == i, 1, 0) } crp_4 = crp(alpha = 5, n = n) crp_4_mat = matrix(0, nrow = n, ncol = n_distinct(crp_4)) for (i in 1:n_distinct(crp_4)) { crp_4_mat[, i] = ifelse(crp_4 == i, 1, 0) } heatmaply::heatmaply( crp_1_mat, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) heatmaply::heatmaply( crp_4_mat, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) Indian Buffet Process The following demonstrates the Indian buffet process for continuous latent variable settings. ibp &lt;- function(alpha, N){ # preallocate assignments with upper bound of N*alpha number of latent factors assignments = matrix(NA, nrow = N, ncol = N*alpha) # start with some dishes/assignments dishes = rpois(1, alpha) zeroes = ncol(assignments) - dishes # fill in the rest of potential dishes assignments[1, ] = c(rep(1, dishes), rep(0, zeroes)) for(i in 2:N){ prev = i - 1 # esoteric line that gets the last dish sampled without a search for it last_previously_sampled_dish = sum(colSums(assignments[1:prev, , drop = FALSE]) &gt; 0) # initialize dishes_previously_sampled = matrix(0, nrow=1, ncol=last_previously_sampled_dish) # calculate probability of sampling from previous dishes dish_prob = colSums(assignments[1:prev, 1:last_previously_sampled_dish, drop = FALSE]) / i dishes_previously_sampled[1, ] = rbinom(n = last_previously_sampled_dish, size = 1, prob = dish_prob) # sample new dish and assign based on results new_dishes = rpois(1, alpha/i) zeroes = ncol(assignments) - (last_previously_sampled_dish + new_dishes) assignments[i,] = c(dishes_previously_sampled, rep(1,new_dishes), rep(0, zeroes)) } # return only the dimensions sampled last_sampled_dish = sum(colSums(assignments[1:prev,]) &gt; 0) assignments[, 1:last_sampled_dish] } As before, we can compare different settings. set.seed(123) ibp_1 = ibp(1, 100) ibp_4 = ibp(5, 100) heatmaply::heatmaply( ibp_1, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) heatmaply::heatmaply( ibp_4, Rowv = FALSE, Colv = FALSE, colors = scico::scico(n = 256, alpha = 1, begin = 0, end = 1), width = 400 ) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/crp.R "],["bayesian-stochastic-volatility.html", "Bayesian Stochastic Volatility Model Data Setup Model Code Estimation Results Visualization Source", " Bayesian Stochastic Volatility Model Stochastic Volatility Model for centered time series over \\(t\\) equally spaced points. The latent parameter \\(h\\) is the log volatility, φ the persistence of the volatility and μ the mean log volatility. ϵ is the white-noise shock and δ the shock on volatility. The Stan code is based on that in the manual (at the time I originally played with it). y_t = exp(h_t/2)*ϵ_t h_t = μ + φ*(h_{t-1}-μ) + δ_t*σ h_1 ~ N(μ, σ/sqrt(1-φ^2)) ϵ_t ~ N(0,1); δ_t ~ N(0,1) With some rearranging: ϵ_t = y_t*exp(-h_t/2) y_t ~ N(0, exp(h_t/2) h_t ~ N(μ + φ*(h_t-μ), σ) Data Setup The data regards inflation based on the U.S. consumer price index (inflation = 400*log(cpi_t/cpi_{t-1}), from the second quarter of 1947 to the second quarter of 2011 (from Statistical Computation and Modeling 2014, chap 11). library(tidyverse) d = read_csv( &#39;https://raw.githubusercontent.com/m-clark/Datasets/master/us%20cpi/USCPI.csv&#39;, col_names = &#39;inflation&#39; ) inflation = pull(d, inflation) summary(inflation) Min. 1st Qu. Median Mean 3rd Qu. Max. -9.557 1.843 3.248 3.634 4.819 15.931 inflation_cen = scale(inflation, scale = FALSE) Model Code This original code keeps to the above formulation but can take a long time to converge. ϵ_t and δ_t are implicit. data { int&lt;lower = 0&gt; N_t; // Number of time points (equally spaced) vector[N_t] y; // mean corrected response at time t } parameters { real mu; // mean log volatility real&lt;lower = -1,upper = 1&gt; phi; // persistence of volatility real&lt;lower = 0&gt; sigma; // white noise shock scale vector[N_t] h; // log volatility at time t } model { //priors phi ~ uniform(-1, 1); sigma ~ cauchy(0, 5); mu ~ cauchy(0, 10); //likelihood h[1] ~ normal(mu, sigma / sqrt(1 - phi * phi)); for (t in 2:N_t) h[t] ~ normal(mu + phi * (h[t - 1] - mu), sigma); for (t in 1:N_t) y ~ normal(0, exp(h[t] / 2)); } This code is more performant and will be used to actually estimate the model. data { int&lt;lower = 0&gt; N_t; // N time points (equally spaced) vector[N_t] y; // mean corrected response at time t } parameters { real mu; // mean log volatility real&lt;lower = -1,upper = 1&gt; phi; // persistence of volatility real&lt;lower = 0&gt; sigma; // white noise shock scale vector[N_t] h_std; // standardized log volatility at time t } transformed parameters{ vector[N_t] h; // log volatility at time t h = h_std * sigma; h[1] = h[1] / sqrt(1-phi * phi); h = h + mu; for (t in 2:N_t) h[t] = h[t] + phi * (h[t-1] - mu); } model { //priors phi ~ uniform(-1, 1); sigma ~ cauchy(0, 5); mu ~ cauchy(0, 10); h_std ~ normal(0, 1); //likelihood y ~ normal(0, exp(h/2)); } generated quantities{ vector[N_t] y_rep; for (t in 1:N_t){ y_rep[t] = normal_rng(0, exp(h[t]/2)); } } Estimation We can use c() to get rid of matrix format, or specify as matrix instead of vector in model code. stan_data = list(N_t = length(inflation_cen), y = c(inflation_cen)) library(rstan) fit = sampling( bayes_sv, data = stan_data, cores = 4, thin = 4 ) Results Explore the results. print( fit, digits = 3, par = c(&#39;mu&#39;, &#39;phi&#39;, &#39;sigma&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: c65225b34c51f358116525cb9ba6c87c. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat mu 1.586 0.016 0.446 0.673 1.600 2.416 737 0.998 phi 0.894 0.001 0.041 0.805 0.898 0.964 830 1.001 sigma 0.618 0.004 0.110 0.413 0.613 0.837 798 1.003 Samples were drawn using NUTS(diag_e) at Tue Nov 17 16:11:27 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). Visualization With the necessary components in place, we can visualize our predictions. Compare to fig. 11.1 in the text. # Create y_rep &#39;by-hand&#39; h = extract(fit, &#39;h&#39;)$h # y_rep = apply(h, 1, function(h) rnorm(length(inflation), 0, exp(h / 2))) # or just extract y_rep = extract(fit, &#39;y_rep&#39;)$y_rep h = colMeans(h) library(lubridate) library(scales) series = ymd(paste0(rep(1947:2014, e = 4), &#39;-&#39;, c(&#39;01&#39;, &#39;04&#39;, &#39;07&#39;, &#39;10&#39;), &#39;-&#39;, &#39;01&#39;)) seriestext = series[1:length(inflation)] Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/stochasticVolatility.R "],["bayesian-multinomial-model.html", "Bayesian Multinomial Models Data Setup Model Code Estimation Comparison Adding Complexity Source", " Bayesian Multinomial Models I spent some time on these models to better understand them in the traditional and Bayesian context, as well as profile potential speed gains in the Stan code. If you were doing what many would call ‘multinomial regression’ without qualification, I can recommend brms with the ‘categorical’ distribution. However, I’m not aware of it being able to accommodate choice-specific variables easily, i.e. ones that vary across choices (though it does accommodate choice specific effects). I show the standard model here with the usual demonstration, and show some code for the most complex setting of choice-specific, individual-specific, and choice-constant variables. See the multinomial chapter for the non-Bayesian approach. Data Setup Depending on the complexity of the data, you may need to create a data set specific to the problem. library(haven) library(tidyverse) program = read_dta(&quot;https://stats.idre.ucla.edu/stat/data/hsbdemo.dta&quot;) %&gt;% as_factor() %&gt;% mutate(prog = relevel(prog, ref = &quot;academic&quot;)) head(program[,1:5]) # A tibble: 6 x 5 id female ses schtyp prog &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 45 female low public vocation 2 108 male middle public general 3 15 male high public vocation 4 67 male low public vocation 5 153 male middle public vocation 6 51 female high public general library(mlogit) programLong = program %&gt;% select(id, prog, ses, write) %&gt;% mlogit.data( shape = &#39;wide&#39;, choice = &#39;prog&#39;, id.var = &#39;id&#39; ) head(programLong) ~~~~~~~ first 10 observations out of 600 ~~~~~~~ id prog ses write chid alt idx 1 1 FALSE low 44 11 academic 11:emic 2 1 FALSE low 44 11 general 11:eral 3 1 TRUE low 44 11 vocation 11:tion 4 2 FALSE middle 41 9 academic 9:emic 5 2 FALSE middle 41 9 general 9:eral 6 2 TRUE middle 41 9 vocation 9:tion 7 3 TRUE low 65 159 academic 159:emic 8 3 FALSE low 65 159 general 159:eral 9 3 FALSE low 65 159 vocation 159:tion 10 4 TRUE low 50 30 academic 30:emic ~~~ indexes ~~~~ chid id alt 1 11 1 academic 2 11 1 general 3 11 1 vocation 4 9 2 academic 5 9 2 general 6 9 2 vocation 7 159 3 academic 8 159 3 general 9 159 3 vocation 10 30 4 academic indexes: 1, 1, 2 X = model.matrix(prog ~ ses + write, data = program) y = program$prog X = X[order(y),] y = y[order(y)] Model Code data { int K; int N; int D; int y[N]; matrix[N,D] X; } transformed data { vector[D] zeros; zeros = rep_vector(0, D); } parameters { matrix[D, K-1] beta_raw; } transformed parameters { matrix[D, K] beta; beta = append_col(zeros, beta_raw); } model { matrix[N, K] L; # Linear predictor L = X * beta; // prior for coefficients to_vector(beta_raw) ~ normal(0, 10); // likelihood for (n in 1:N) y[n] ~ categorical_logit(to_vector(L[n])); } Estimation We’ll get the data prepped for Stan, and the model code is assumed to be in an object bayes_multinom. # N = sample size, x is the model matrix, y integer version of class outcome, k= # number of classes, D is dimension of model matrix stan_data = list( N = nrow(X), X = X, y = as.integer(y), K = n_distinct(y), D = ncol(X) ) library(rstan) fit = sampling( bayes_multinom, data = stan_data, thin = 4, cores = 4 ) Comparison We’ll need to do a bit of reordering, but otherwise we can see that the models come to similar conclusions. print( fit, digits = 3, par = c(&#39;beta&#39;), probs = c(.025, .5, .975) ) Inference for Stan model: 6f6b51695c6eeda4b74c0665f4447c97. 4 chains, each with iter=2000; warmup=1000; thin=4; post-warmup draws per chain=250, total post-warmup draws=1000. mean se_mean sd 2.5% 50% 97.5% n_eff Rhat beta[1,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[1,2] 2.867 0.039 1.210 0.512 2.889 5.282 959 1.001 beta[1,3] 5.273 0.040 1.157 3.138 5.220 7.552 856 1.000 beta[2,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[2,2] -0.534 0.014 0.454 -1.414 -0.528 0.371 1014 0.998 beta[2,3] 0.340 0.016 0.485 -0.559 0.340 1.265 888 1.002 beta[3,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[3,2] -1.219 0.017 0.539 -2.361 -1.200 -0.209 976 0.999 beta[3,3] -0.994 0.019 0.596 -2.234 -1.006 0.101 949 1.001 beta[4,1] 0.000 NaN 0.000 0.000 0.000 0.000 NaN NaN beta[4,2] -0.058 0.001 0.022 -0.102 -0.058 -0.013 943 1.002 beta[4,3] -0.116 0.001 0.022 -0.158 -0.115 -0.075 888 0.999 Samples were drawn using NUTS(diag_e) at Tue Nov 17 19:51:48 2020. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). fit_coefs = get_posterior_mean(fit, par = &#39;beta_raw&#39;)[, 5] mlogit_mod = mlogit(prog ~ 1 | ses + write, data = programLong) mlogit_coefs = coef(mlogit_mod)[c(1, 3, 5, 7, 2, 4, 6, 8)] m_logit fit (Intercept):general 2.852 2.867 (Intercept):vocation 5.218 5.273 sesmiddle:general -0.533 -0.534 sesmiddle:vocation 0.291 0.340 seshigh:general -1.163 -1.219 seshigh:vocation -0.983 -0.994 write:general -0.058 -0.058 write:vocation -0.114 -0.116 Adding Complexity The following adds choice-specific (a.k.a. alternative-specific) variables, e.g. among product choices, this may include price. Along with this we may have, along with choice constant, and the typical individual varying covariates. This code worked at the time, but I wasn’t interested enough to try it again recently. You can use the classic ‘travel’ data as an example (available as TravelMode in AER), or fishing from mlogit. Essentially you’ll have three separate data components- a matrix for individual-specific covariates, one for alternative specific, and one for alternative constant covariates. data { int K; // number of choices int N; // number of individuals int D; // number of indiv specific variables int G; // number of alt specific variables int T; // number of alt constant variables int y[N*K]; // choices vector[N*K] choice; // choice made (logical) matrix[N, D] X; // data for indiv specific effects matrix[N*K, G] Y; // data for alt specific effects matrix[N*(K-1), T] Z; // data for alt constant effects } parameters { matrix[D, K-1] beta; // individual specific coefs matrix[G, K] gamma; // choice specific coefs for alt-specific variables vector[T] theta; // choice constant coefs for alt-specific variables } model { matrix[N, K-1] Vx; // Utility for individual vars vector[N*K] Vy0; matrix[N, K-1] Vy; // Utility for alt-specific/alt-varying vars vector[N*(K-1)] Vz0; matrix[N, (K-1)] Vz; // Utility for alt-specific/alt-constant vars matrix[N, K-1] V; // combined utilities vector[N] baseProbVec; // reference group probabilities real ll0; // intermediate log likelihood real loglik; // final log likelihood // priors to_vector(beta) ~ normal(0, 10); // diffuse priors on coefficients to_vector(gamma) ~ normal(0, 10); to_vector(theta) ~ normal(0, 10); // likelihood // &#39;Utilities&#39; Vx = X * beta; for(alt in 1:K){ vector[G] par; int start; int end; par = gamma[,alt]; start = N*alt - N+1; end = N*alt; Vy0[start:end] = Y[start:end,] * par; if(alt &gt; 1) Vy[,alt-1] = Vy0[start:end] - Vy0[1:N]; } Vz0 = Z * theta; for(alt in 1:(K-1)){ int start; int end; start = N*alt - N+1; end = N*alt; Vz[,alt] = Vz0[start:end]; } V = Vx + Vy + Vz; for(n in 1:N) baseProbVec[n] = 1/(1 + sum(exp(V[n]))); ll0 = dot_product(to_vector(V), choice[(N+1):(N*K)]); // just going to assume no neg index loglik = sum(log(baseProbVec)) + ll0; target += loglik; } generated quantities { matrix[N, K-1] fitted_nonref; vector[N] fitted_ref; matrix[N, K] fitted; matrix[N, K-1] Vx; // Utility for individual variables vector[N*K] Vy0; matrix[N, K-1] Vy; // Utility for alt-specific/alt-varying variables vector[N*(K-1)] Vz0; matrix[N, (K-1)] Vz; // Utility for alt-specific/alt-constant variables matrix[N, K-1] V; // combined utilities vector[N] baseProbVec; // reference group probabilities Vx = X * beta; for(alt in 1:K) { vector[G] par; int start; int end; par = gamma[,alt]; start = N*alt - N+1; end = N*alt; Vy0[start:end] = Y[start:end, ] * par; if (alt &gt; 1) Vy[,alt-1] = Vy0[start:end] - Vy0[1:N]; } Vz0 = Z * theta; for(alt in 1:(K-1)){ int start; int end; start = N*alt-N+1; end = N*alt; Vz[,alt] = Vz0[start:end]; } V = Vx + Vy + Vz; for(n in 1:N) baseProbVec[n] = 1 / (1 + sum(exp(V[n]))); fitted_nonref = exp(V) .* rep_matrix(baseProbVec, K-1); for(n in 1:N) fitted_ref[n] = 1 - sum(fitted_nonref[n]); fitted = append_col(fitted_ref, fitted_nonref); } Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/Bayesian/multinomial "],["bayesian-variational.html", "Variational Bayes Regression Data Setup Function Estimation Comparison Visualization Supplemental Example Source", " Variational Bayes Regression The following provides a function for estimating the parameters of a linear regression via variational inference. See Drugowitsch (2014) for an overview of the method outlined in Bishop (2006). For the primary function I will use the notation used in the Drugowitsch article in most cases. Here w, represents the coefficients, and τ the precision (inverse variance). The likelihood for target y is N(Xw, τ-1). Priors for w and tau are normal inverse gamma N(0, (τα)-1) Gamma(a0, b0). References: Drugowitsch: http://arxiv.org/abs/1310.5438 See here and here for his Matlab implementations. Bishop: Pattern Recognition and Machine Learning Data Setup We can simulate some data as a starting point, in this case, basic tabular data used in the standard regression problem. Here, I explicitly note the intercept, as it is added to the model matrix within the vb_reg function. library(tidyverse) set.seed(1234) n = 100 d = 3 coefs = c(1, 2, 3, 5) sigma = 2 X = replicate(d, rnorm(n)) # predictors colnames(X) = paste0(&#39;X&#39;, 1:d) y = cbind(1, X) %*% coefs + rnorm(n, sd = sigma) # target df = data.frame(X, y) We can also look at the higher dimension case as done in Drugowitsch section 2.6.2. n = 150 ntest = 50 d = 100 coefs = rnorm(d + 1) sigma = 1 X_train = cbind(1, replicate(d, rnorm(n))) y_train = X_train %*% coefs + rnorm(n, sd = sigma) X_test = cbind(1, replicate(d, rnorm(ntest))) y_test = X_test %*% coefs + rnorm(ntest, sd = sigma) Function First, the main function. For this demo, automatic relevance determination is an argument rather than a separate function. vb_reg = function( X, y, a0 = 10e-2, b0 = 10e-4, c0 = 10e-2, d0 = 10e-4, tol = 1e-8, maxiter = 1000, ard = F ) { # X: model matrix # y: the response # a0, b0 prior parameters for tau # c0, d0 hyperprior parameters for alpha # tol: tolerance value to end iterations # maxiter: alternative way to end iterations # initializations X = cbind(1, X) D = ncol(X) N = nrow(X) w = rep(0, D) XX = crossprod(X) Xy = crossprod(X,y) a_N = a0 + N/2 if (!ard) { c_N = c0 + D/2 E_alpha = c0/d0 } else { c_N = c0 + 1/2 E_alpha = rep(c0/d0, D) } tolCurrent = 1 iter = 0 LQ = 0 while(iter &lt; maxiter &amp;&amp; tolCurrent &gt; tol ){ iter = iter + 1 # wold = w if(!ard){ b_N = b0 + 1/2 * (crossprod(y - X%*%w) + E_alpha * crossprod(w)) VInv = diag(E_alpha, D) + XX V = solve(VInv) w = V %*% Xy E_wtau = a_N/b_N * crossprod(w) + sum(diag(V)) d_N = d0 + 1/2*E_wtau E_alpha = c(c_N/d_N) } else { b_N = b0 + 1/2 * (crossprod(y - X%*%w) + t(w) %*% diag(E_alpha) %*% w) VInv = diag(E_alpha) + XX V = solve(VInv) w = V %*% Xy E_wtau = a_N/b_N*crossprod(w) + sum(diag(V)) d_N = d0 + 1/2*(c(w)^2 * c(a_N/b_N) + diag(V)) E_alpha = c(c_N/d_N) } LQ_old = LQ suppressWarnings({ LQ = -N/2*log(2*pi) - 1/2 * (a_N/b_N * crossprod(y- crossprod(t(X), w)) + sum(XX * V)) + 1/2 * determinant(V, log = TRUE)$modulus + D/2 - lgamma(a0) + a0 * log(b0) - b0 * a_N / b_N + lgamma(a_N) - a_N * log(b_N) + a_N - lgamma(c0) + c0*log(d0) + lgamma(c_N) - sum(c_N*log(d_N)) }) tolCurrent = abs(LQ - LQ_old) # alternate tolerance, comment out LQ_old up to this line if using # tolCurrent = sum(abs(w - wold)) } res = list( coef = w, sigma = sqrt(1 / (E_wtau / crossprod(w))), LQ = LQ, iterations = iter, tol = tolCurrent ) if (iter &gt;= maxiter) append(res, warning(&#39;Maximum iterations reached.&#39;)) else res } Estimation First we can estimate the model using the smaller data. fit_small = vb_reg(X, y, tol = 1e-8, ard = FALSE) glimpse(fit_small) List of 5 $ coef : num [1:4, 1] 1.01 2.29 3.29 5.02 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; .. ..$ : NULL $ sigma : num [1, 1] 2.08 $ LQ : num [1, 1] -233 ..- attr(*, &quot;logarithm&quot;)= logi TRUE $ iterations: num 8 $ tol : num [1, 1] 1.11e-10 ..- attr(*, &quot;logarithm&quot;)= logi TRUE # With automatic relevance determination fit_small_ard = vb_reg(X, y, tol = 1e-8, ard = TRUE) glimpse(fit_small_ard) List of 5 $ coef : num [1:4, 1] 0.955 2.269 3.283 5.047 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;&quot; &quot;X1&quot; &quot;X2&quot; &quot;X3&quot; .. ..$ : NULL $ sigma : num [1, 1] 2.09 $ LQ : num [1, 1] -229 ..- attr(*, &quot;logarithm&quot;)= logi TRUE $ iterations: num 9 $ tol : num [1, 1] 7.46e-09 ..- attr(*, &quot;logarithm&quot;)= logi TRUE lm_mod = lm(y ~ ., data = df) Now with the higher dimensional data. We fit using the training data and will estimate the error on training and test using the yardstick package. fit_vb = vb_reg(X_train[,-1], y_train) fit_glm = glm.fit(X_train, y_train) # predictions vb_pred_train = X_train %*% fit_vb[[&#39;coef&#39;]] vb_pred_test = X_test %*% fit_vb[[&#39;coef&#39;]] glm_pred_train = fitted(fit_glm) glm_pred_test = X_test %*% coef(fit_glm) # error vb_train_error = yardstick::rmse_vec(y_train[,1], vb_pred_train[,1]) vb_test_error = yardstick::rmse_vec(y_test[,1], vb_pred_test[,1]) glm_train_error = yardstick::rmse_vec(y_train[,1], glm_pred_train) glm_test_error = yardstick::rmse_vec(y_test[,1], glm_pred_test[,1]) Comparison For the smaller data, we will compare the coefficients. no_ard ard lm 1.010 0.955 1.012 2.291 2.269 2.300 3.286 3.283 3.297 5.024 5.047 5.045 For the higher dimensional data, we will compare root mean square error. vb glm train 0.574 0.566 test 1.876 1.982 Visualization In general the results are as expected where the standard approach overfits relative to VB regression. The following visualizes them, similar to Drugowitsch figure 1. Supplemental Example And now for a notably higher dimension case with irrelevant predictors as in Drugowitsch section 2.6.3. This is problematic for the GLM with having more covariates than data points (rank deficient), and as such it will throw a warning, as will the predict function. It’s really not even worth looking at but I have the code for consistency. This will take a while to estimate, and without ARD, even bumping up the iterations to 2000 it will still likely hit the max before reaching the default tolerance level. However, the results appear very similar to that of Drugowitsch Figure 2. set.seed(1234) n = 500 ntest = 50 d = 1000 deff = 100 coefs = rnorm(deff + 1) sigma = 1 X_train = cbind(1, replicate(d, rnorm(n))) y_train = X_train %*% c(coefs, rep(0, d - deff)) + rnorm(n, sd = sigma) X_test = cbind(1, replicate(d, rnorm(ntest))) y_test = X_test %*% c(coefs, rep(0, d - deff)) + rnorm(ntest, sd = sigma) fit_vb = vb_reg(X_train[,-1], y_train) fit_vb_ard = vb_reg(X_train[,-1], y_train, ard = TRUE) # fit_glm = glm(y_train ~ ., data = data.frame(X_train[,-1])) # predictions vb_pred_train = X_train %*% fit_vb[[&#39;coef&#39;]] vb_pred_test = X_test %*% fit_vb[[&#39;coef&#39;]] # vb_ard_pred_train = X_train %*% fit_vb_ard[[&#39;coef&#39;]] vb_ard_pred_test = X_test %*% fit_vb_ard[[&#39;coef&#39;]] # glm_pred_train = fitted(fit_glm) # glm_pred_test = X_test %*% coef(fit_glm) # error vb_train_error = yardstick::rmse_vec(y_train[,1], vb_pred_train[,1]) vb_test_error = yardstick::rmse_vec(y_test[,1], vb_pred_test[,1]) # error vb_ard_train_error = yardstick::rmse_vec(y_train[,1], vb_ard_pred_train[,1]) vb_ard_test_error = yardstick::rmse_vec(y_test[,1], vb_ard_pred_test[,1]) # glm_train_error = yardstick::rmse_vec(y_train[,1], glm_pred_train) # glm_test_error = yardstick::rmse_vec(y_test[,1], glm_pred_test[,1]) mse_results = data.frame( vb = c(vb_train_error, vb_test_error), vbARD = c(vb_ard_train_error, vb_ard_test_error)#, # glm = c(glm_train_error, glm_test_error) ) rownames(mse_results) = c(&#39;train&#39;, &#39;test&#39;) kable_df(mse_results) vb vbARD train 0.641 0.002 test 8.378 2.323 Note how ARD correctly estimates (nearly) zero for irrelevant predictors. N Mean SD Min Q1 Median Q3 Max % Missing 900 0 0 -0.2 0 0 0 0.4 0 Visualized, as before. Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/tree/master/ModelFitting/Bayesian/multinomial "],["bayesian-topic-model.html", "Topic Model Data Setup Function Estimation Comparison Source", " Topic Model An implementation of Gibbs sampling for topic models for the example in section 4 of Steyvers and Griffiths (2007). A very clear intro in my opinion. The core of the function’s code comprises mostly cosmetic changes to that found here. Added are the creation of a function with several arguments, plotting etc. Data Setup library(tidyverse) vocab = factor(c(&quot;river&quot;, &quot;stream&quot;, &quot;bank&quot;, &quot;money&quot;, &quot;loan&quot;)) K = 2 # n of topics v = length(vocab) # number of unique words d = 16 # number of documents Topic 1 gives equal probability to money loan and bank (zero for river and stream). Topic 2 gives equal probability to river stream and bank (zero for money and loan). Next we create a document term matrix. Each doc consists of a mix of 16 tokens of the vocab- the first few regard financial banks, the last several water banks, and the docs in between possess a mixed vocab. dtm = matrix(c(0,0,4,6,6, 0,0,5,7,4, 0,0,7,5,4, 0,0,7,6,3, 0,0,7,2,7, 0,0,9,3,4, 1,0,4,6,5, 1,2,6,4,3, 1,3,6,4,2, 2,3,6,1,4, 2,3,7,3,1, 3,6,6,1,0, 6,3,6,0,1, 2,8,6,0,0, 4,7,5,0,0, 5,7,4,0,0), ncol = v, byrow = TRUE) rownames(dtm) = paste0(&#39;doc&#39;, 1:d) colnames(dtm) = vocab Next we create additional objects to initialize the setup. # matrix of words in each document wordmat = t(apply(dtm, 1, function(row) rep(vocab, row))) # initialize random topic assignments to words T0 = apply(wordmat, c(1, 2), function(token) sample(1:2, 1)) # word by topic matrix of counts containing the number of times word w is # assigned to topic j C_wt = sapply(vocab, function(word) cbind(sum(T0[wordmat == word] == 1), sum(T0[wordmat == word] == 2))) C_wt = t(C_wt) rownames(C_wt) = vocab # topic by document matrix of counts containing the number of times topic j is # assigned to a word in document d C_dt = t(apply(T0, 1, table)) Function Note that this function is not self contained in that it uses some of the objects created above, assuming they are in the global environment. It has the capacity for a pseudo-visualization which can be instructive, but isn’t shown for this document (and it requries corrplot), and I haven’t tested it recently. topic_model = function( alpha, beta, nsim = 2000, warmup = nsim / 2, thin = 10, verbose = TRUE, plot = FALSE, dotsortext = &#39;dots&#39; ) { # Arguments: # alpha and beta: hyperparameters # nsim: the total number of simulations # warmup: the number of initial sims to discard # thin: keep every thin simulation after warmup # verbose: to print every 100th iteration and total time # plot: to visualize every x sim; the plot will show current estimates of # theta, phi, and topic assignments for each term in the docs; can be # visualized as dots or the actual terms; for the latter you may need to # fiddle with your viewing area size so that terms don&#39;t appear to run # together; plotting will increase runtime # requires corrplot for visualizations, and abind to create arrays from the # lists. # initialize Z = T0 # topic assignments saveSim = seq(warmup + 1, nsim, thin) # iterations to save theta_list = list() # saved theta estimates phi_list = list() # saved phi estimates p = proc.time() # for every simulation... for (s in 1:nsim) { if(verbose &amp;&amp; s %% 100 == 0) { # paste every 100th iteration if desired secs = round((proc.time() - p)[3],2) min = round(secs/60, 2) message(paste0( &#39;Iteration number: &#39;, s, &#39;\\n&#39;, &#39;Total time: &#39;, ifelse(secs &lt;= 60, paste(secs, &#39;seconds&#39;), paste(min, &#39;minutes&#39;)) )) } # Visualization if(plot &gt; 0 &amp;&amp; s &gt;1 &amp;&amp; s %% plot == 0) { # plot every value of argument require(corrplot) layout(matrix(c(1, 1, 2, 2, 3, 3, 3, 3, 3, 3), ncol = 10)) corrplot( theta, is.corr = FALSE, method = &#39;color&#39;, tl.cex = .75, tl.col = &#39;gray50&#39;, cl.pos = &#39;n&#39;, addgrid = NA ) corrplot( phi, is.corr = FALSE, method = &#39;color&#39;, tl.cex = 1, tl.col = &#39;gray50&#39;, cl.pos = &#39;n&#39;, addgrid = NA ) if (dotsortext == &#39;dots&#39;) { Zplot = Z Zplot[Zplot == 2] = -1 corrplot( Zplot, is.corr = FALSE, method = &#39;circle&#39;, tl.cex = .75, tl.col = &#39;gray50&#39;, cl.pos = &#39;n&#39;, addgrid = NA ) } else { cols = apply(Z, c(1, 2), function(topicvalue) ifelse(topicvalue == 1, &#39;#053061&#39;, &#39;#67001F&#39;)) plot( 1:nrow(wordmat), 1:ncol(wordmat), type = &quot;n&quot;, axes = FALSE, xlab = &#39;&#39;, ylab = &#39;&#39; ) text(col(wordmat), rev(row(wordmat)), wordmat, col = cols, cex = .75) } } # for every document and every word in the document for (i in 1:d) { for (j in 1:length(wordmat[i,])) { word.id = which(vocab == wordmat[i, j]) topic.old = Z[i, j] # Decrement counts before computing equation (3) in paper noted above C_dt[i, topic.old] = C_dt[i, topic.old] - 1 C_wt[word.id, topic.old] = C_wt[word.id, topic.old] - 1 # Calculate equation (3) for each topic vals = prop.table(C_wt + beta, 2)[word.id,] * prop.table(C_dt[i,] + alpha) # Sample the new topic from the results for (3); # note, sample function does not require you to have the probs sum to 1 # explicitly, i.e. prob=c(1,1,1) is the same as prob = c(1/3, 1/3, 1/3) Z.new = sample(1:K, 1, prob = vals) # Set the new topic and update counts Z[i,j] = Z.new C_dt[i, Z.new] = C_dt[i, Z.new] + 1 C_wt[word.id, Z.new] = C_wt[word.id, Z.new] + 1 } } theta = prop.table(C_dt + alpha,1) # doc topic distribution phi = prop.table(C_wt + beta,2) # word topic distribution # save simulations if (s %in% saveSim){ theta_list[[paste(s)]] = theta phi_list[[paste(s)]] = phi } } layout(1) # reset plot window list( theta = theta, phi = phi, theta_sims = abind::abind(theta_list, along = 3), phi_sims = abind::abind(phi_list, along = 3) ) } Estimation We use the values of alpha and beta as suggested in paper. The following will result in 500 posterior sample points, so bump if you want more than that. These settings take around 20 seconds. set.seed(1234) alpha = K/50 beta = .01 fit_tm = topic_model( alpha = alpha, beta = beta, nsim = 1000, warmup = 500, thin = 1, verbose = FALSE, plot = FALSE ) Comparison First we can explore the results, in particular topic assignment probabilities and term topic probabilities. str(fit_tm, 1) List of 4 $ theta : num [1:16, 1:2] 0.00249 0.00249 0.00249 0.00249 0.00249 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ phi : num [1:5, 1:2] 2.57e-01 4.00e-01 3.24e-01 1.91e-02 9.52e-05 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ thetaSims: num [1:16, 1:2, 1:500] 0.00249 0.00249 0.00249 0.00249 0.00249 ... ..- attr(*, &quot;dimnames&quot;)=List of 3 $ phiSims : num [1:5, 1:2, 1:500] 2.52e-01 3.92e-01 3.55e-01 9.34e-05 9.34e-05 ... ..- attr(*, &quot;dimnames&quot;)=List of 3 purrr::map(fit_tm[1:2], round, 2) $theta 1 2 doc1 0.00 1.00 doc2 0.00 1.00 doc3 0.00 1.00 doc4 0.00 1.00 doc5 0.00 1.00 doc6 0.00 1.00 doc7 0.06 0.94 doc8 0.25 0.75 doc9 0.44 0.56 doc10 0.44 0.56 doc11 0.50 0.50 doc12 1.00 0.00 doc13 0.87 0.13 doc14 1.00 0.00 doc15 1.00 0.00 doc16 1.00 0.00 $phi [,1] [,2] river 0.26 0.00 stream 0.40 0.00 bank 0.32 0.40 money 0.02 0.30 loan 0.00 0.29 Now we can compare our result to reported paper estimates for \\(\\phi\\) and topicmodels package (note delta is the beta above; the vignette actually references Steyvers &amp; Griffiths paper). library(topicmodels) fit_lda = LDA( dtm, k = 2, method = &#39;Gibbs&#39;, control = list( alpha = alpha, delta = .01, iter = 5500, burnin = 500, thin = 10, initialize = &#39;random&#39; ) ) Start with \\(\\phi\\) estimates. Note that the labels of what is topic 1 vs. topic 2 is arbitrary, the main thing is the separation of the words that should go to different topics, while bank is probable for both topics. fit.1 fit.2 paper.1 paper.2 ldapack.1 ldapack.2 river 0.26 0.00 0.00 0.25 0.00 0.29 stream 0.40 0.00 0.00 0.40 0.00 0.46 bank 0.32 0.40 0.39 0.35 0.44 0.25 money 0.02 0.30 0.32 0.00 0.29 0.00 loan 0.00 0.29 0.29 0.00 0.27 0.00 Interval estimates for term probabilities for each topic. library(coda) phi_estimates_topic_1 = as.mcmc(t(fit_tm$phi_sims[, 1, ])) phi_estimates_topic_2 = as.mcmc(t(fit_tm$phi_sims[, 2, ])) # summary(phi_estimates_topic_1) rowname Topic lower upper river…1 Topic 1 0.24 0.28 stream…2 Topic 1 0.37 0.43 bank…3 Topic 1 0.28 0.38 money…4 Topic 1 0.00 0.01 loan…5 Topic 1 0.00 0.01 river…6 Topic 2 0.00 0.01 stream…7 Topic 2 0.00 0.00 bank…8 Topic 2 0.36 0.42 money…9 Topic 2 0.30 0.33 loan…10 Topic 2 0.28 0.31 Symmetrized or mean Kullback-Liebler divergence for topic (dis)similarity. kl_divergence = .5 * sum( apply( fit_tm$phi, 1, function(row) row[1] * log2(row[1] / row[2]) + row[2] * log2(row[2] / row[1]) ) ) kl_divergence [1] 7.678387 We can compare with various other packages for KL-divergence (not shown). mean(c( entropy::KL.empirical(fit_tm$phi[,1], fit_tm$phi[,2], unit = &#39;log2&#39;), entropy::KL.empirical(fit_tm$phi[,2], fit_tm$phi[,1], unit = &#39;log2&#39;) )) Symmetrized or mean Kullback-Liebler divergence for document (dis)similarity. # example docs2compare = c(1, 16) .5 * sum(apply(fit_tm$theta[docs2compare, ], 2, function(topicprob) topicprob[1] * log(topicprob[1] / topicprob[2]) + topicprob[2] * log(topicprob[2] / topicprob[1]))) [1] 5.964141 Let’s create a function to work on theta to produce Kullback-Liebler or Jensen-Shannon divergence. divergence = function(input, method = &#39;KL&#39;) { if (method == &#39;KL&#39;) { .5 * sum(apply(fit_tm$theta[input, ], 2, function(topicprob) topicprob[1] * log2(topicprob[1] / topicprob[2]) + topicprob[2] * log2(topicprob[2] / topicprob[1]))) } else { .5 * sum(apply(fit_tm$theta[input, ], 2, function(topicprob) topicprob[1] * log2(topicprob[1] / mean(topicprob)) + topicprob[2] * log2(topicprob[2] / mean(topicprob)))) } } Now apply to all pairs of documents. # Now do for all docpairs = combn(1:d, 2) kl_divergence = apply(docpairs, 2, divergence) mat0 = matrix(0, d, d) mat0[lower.tri(mat0)] = kl_divergence kl_divergence = mat0 + t(mat0) dimnames(kl_divergence) = list(rownames(dtm)) We can visualizes as follows, red implies documents are more dissimilar. Likewise, we can visualize Jensen-Shannon divergence. js_divergence = apply(docpairs, 2, divergence, method = &#39;JS&#39;) Here are examples of diagnostics with the word topic probability estimates. These use the bayesplot package. library(bayesplot) mcmc_combo(phi_estimates_topic_1) mcmc_acf(phi_estimates_topic_1) Source Original code available at: https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/Bayesian/topicModelgibbs.R "],["maximum-likelihood.html", "Maximum Likelihood Linear Model Source", " Maximum Likelihood This is a brief refresher on maximum likelihood estimation using a standard regression approach as an example, and more or less assumes one hasn’t tried to roll their own such function in a programming environment before. Given the likelihood’s role in Bayesian estimation and statistics in general, and the ties between specific Bayesian results and maximum likelihood estimates one typically comes across, one should be conceptually comfortable with some basic likelihood estimation. The following is taken directly from my document with mostly just cleaned up code and visualization. The TLDR version can be viewed in the Linear Regression chapter. In the standard model setting we attempt to find parameters \\(\\theta\\) that will maximize the probability of the data we actually observe. We’ll start with an observed random target vector \\(y\\) with \\(i...N\\) independent and identically distributed observations and some data-generating process underlying it \\(f(\\cdot|\\theta)\\). We are interested in estimating the model parameter(s), \\(\\theta\\), that would make the data most likely to have occurred. The probability density function for \\(y\\) given some particular estimate for the parameters can be noted as \\(f(y_i|\\theta)\\). The joint probability distribution of the (independent) observations given those parameters, \\(f(y_i|\\theta)\\), is the product of the individual densities, and is our likelihood function. We can write it out generally as: \\[\\mathcal{L}(\\theta) = \\prod_{i=1}^N f(y_i|\\theta)\\] Thus, the likelihood for one set of parameter estimates given a fixed set of data y, is equal to the probability of the data given those (fixed) estimates. Furthermore, we can compare one set, \\(\\mathcal{L}(\\theta_A)\\), to that of another, \\(\\mathcal{L}(\\theta_B)\\), and whichever produces the greater likelihood would be the preferred set of estimates. We can get a sense of this with the following visualization, based on a single parameter. The data is drawn from Poisson distributed variable with mean \\(\\theta=5\\). We note the calculated likelihood increases as we estimate values for \\(\\theta\\) closer to \\(5\\), or more precisely, whatever the mean observed value is for the data. However, with more and more data, the final ML estimate will converge on the true value. Final estimate = 5.02 For computational reasons, we instead work with the sum of the natural log probabilities, and so deal with the log likelihood: \\[\\ln\\mathcal{L}(\\theta) = \\sum_{i=1}^N \\ln[f(y_i|\\theta)]\\] Concretely, we calculate a log likelihood for each observation and then sum them for the total likelihood for parameter(s) \\(\\theta\\). The likelihood function incorporates our assumption about the sampling distribution of the data given some estimate for the parameters. It can take on many forms and be notably complex depending on the model in question, but once specified, we can use any number of optimization approaches to find the estimates of the parameter that make the data most likely. As an example, for a normally distributed variable of interest we can write the log likelihood as follows: \\[\\ln\\mathcal{L}(\\theta) = \\sum_{i=1}^N \\ln[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(y-\\mu)^2}{2\\sigma^2})]\\] Example In the following we will demonstrate the maximum likelihood approach to estimation for a simple setting incorporating a normal distribution, where we estimate the mean and variance/sd for a set of values \\(y\\). First the data is created, and then we create the function that will compute the log likelihood. Using the built in R distributions makes it fairly straightforward to create our own likelihood function and feed it into an optimization function to find the best parameters. We will set things up to work with the bbmle package, which has some nice summary functionality and other features. However, one should take a glance at optim and the other underlying functions that do the work. # for replication set.seed(1234) # create the data y = rnorm(1000, mean = 5, sd = 2) starting_values = c(0, 1) # the log likelihood function LL = function(mu, sigma, verbose = TRUE) { ll = sum(dnorm(y, mean = mu, sd = sigma, log = TRUE)) if (verbose) message(paste(mu, sigma, ll)) -ll } The LL function takes starting points for the parameters as arguments, in this case we call them \\(\\mu\\) and \\(\\sigma\\), which will be set to 0 and 1 respectively. Only the first line (ll = -sum…) is actually necessary, and we use dnorm to get the density for each point. Since this optimizer is by default minimization, we reverse the sign of the sum so as to minimize the negative log likelihood, which is the same as maximizing the likelihood. Note that the bit of other code just allows you to see the estimates as the optimization procedure searches for the best values. I do not show that here but you’ll see it in your console if trace = TRUE. We are now ready to obtain maximum likelihood estimates for the parameters. For comparison we will use bbmle due to its nice summary result, but you can use optim as in the other demonstrations. For the mle2 function we will need the function we’ve created, plus other inputs related to that function or the underlying optimizing function used (by default optim). In this case we will use an optimization procedure that will allow us to set a lower bound for \\(\\sigma\\). This isn’t strictly necessary, but otherwise you would get warnings and possibly lack of convergence if negative estimates for \\(\\sigma\\) were allowed. # using optim, and L-BFGS-B so as to constrain sigma to be positive by setting # the lower bound at zero mlnorm = bbmle::mle2( LL, start = list(mu = 2, sigma = 1), method = &quot;L-BFGS-B&quot;, lower = c(sigma = 0), trace = TRUE ) mlnorm Call: bbmle::mle2(minuslogl = LL, start = list(mu = 2, sigma = 1), method = &quot;L-BFGS-B&quot;, trace = TRUE, lower = c(sigma = 0)) Coefficients: mu sigma 4.946803 1.993680 Log-likelihood: -2108.92 # compare to an intercept only regression model summary(lm(y~1)) Call: lm(formula = y ~ 1) Residuals: Min 1Q Median 3Q Max -6.7389 -1.2933 -0.0264 1.2848 6.4450 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.94681 0.06308 78.42 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.995 on 999 degrees of freedom We can see that the ML estimates are the same as the lm model estimates based on least squares, and which given the sample size are close to the true values. In terms of the parameters we estimate, instead of the curve presented previously, in the typical case of two or more parameters we can think of a likelihood surface that represents the possible likelihood values given any particular set of estimates. Given some starting point, the optimization procedure then travels along the surface looking for a minimum/maximum point. For simpler settings such as this, we can visualize the likelihood surface and its minimum point. The optimizer travels along this surface until it finds a minimum (the surface plot is interactive- feel free to adjust). I also plot the path of the optimizer from a top down view. The large dot noted represents the minimum negative log likelihood. Please note that there are many other considerations in optimization completely ignored here, but for our purposes and the audience for which this is intended, we do not want to lose sight of the forest for the trees. We now move next to a slightly more complicated regression example. Linear Model In the standard regression context, our expected value for the target variable comes from our linear predictor, i.e. the weighted combination of our explanatory variables, and we estimate the regression weights/coefficients and possibly other relevant parameters. We can expand our previous example to the standard linear model without too much change. In this case we estimate a mean for each observation, but otherwise assume the variance is constant across observations. Again, we first construct some data so that we know exactly what to expect, then write out the likelihood function with starting parameters. As we need to estimate our intercept and coefficient for the X predictor (collectively referred to as \\(\\beta\\)), we can think of our likelihood explicitly as before: \\[\\ln\\mathcal{L}(\\beta, \\sigma^2) = \\sum_{i=1}^N \\ln[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{(y-X\\beta)^2}{2\\sigma^2})]\\] # for replication set.seed(1234) # predictor X = rnorm(1000) # coefficients for intercept and predictor beta = c(5, 2) # add intercept to X and create y with some noise y = cbind(1, X) %*% beta + rnorm(1000, sd = 2.5) regression_ll = function(sigma = 1, Int = 0, b1 = 0) { coefs = c(Int, b1) mu = cbind(1,X)%*%coefs ll = -sum(dnorm(y, mean = mu, sd = sigma, log = TRUE)) message(paste(sigma, Int, b1, ll)) ll } mlopt = bbmle::mle2(regression_ll, method = &quot;L-BFGS-B&quot;, lower = c(sigma = 0)) summary(mlopt) Maximum likelihood estimation Call: bbmle::mle2(minuslogl = regression_ll, method = &quot;L-BFGS-B&quot;, lower = c(sigma = 0)) Coefficients: Estimate Std. Error z value Pr(z) sigma 2.447823 0.054735 44.721 &lt; 2.2e-16 *** Int 5.039976 0.077435 65.087 &lt; 2.2e-16 *** b1 2.139284 0.077652 27.549 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 -2 log L: 4628.273 # plot(profile(mlopt), absVal=F) modlm = lm(y ~ X) summary(modlm) Call: lm(formula = y ~ X) Residuals: Min 1Q Median 3Q Max -7.9152 -1.6097 0.0363 1.6343 7.6711 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.03998 0.07751 65.02 &lt;2e-16 *** X 2.13928 0.07773 27.52 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.45 on 998 degrees of freedom Multiple R-squared: 0.4315, Adjusted R-squared: 0.4309 F-statistic: 757.5 on 1 and 998 DF, p-value: &lt; 2.2e-16 - 2 * logLik(modlm) &#39;log Lik.&#39; 4628.273 (df=3) As before, our estimates and final log likelihood value are about where they should be, and reflect the lm output, as the OLS estimates are the maximum likelihood estimates. The visualization becomes more difficult beyond two parameters, but we can examine slices similar to the previous plot. To move to generalized linear models, very little changes of the process outside of the distribution assumed and that we are typically modeling a function of the target variable (e.g. \\(\\log(y)=X\\beta; \\mu = e^{X\\beta}\\)). Source Original code available at: https://m-clark.github.io/bayesian-basics/appendix.html#maximum-likelihood-review "],["penalized-maximum-likelihood.html", "Penalized Maximum Likelihood Data Setup Functions Estimation Comparison Source", " Penalized Maximum Likelihood This demonstration regards a standard regression model via penalized likelihood. See the linear regression example code for comparison. Here the penalty is specified (via lambda argument) but one would typically estimate via cross-validation or some other fashion. Two penalties are possible with the function. One using the (squared) L2 norm (aka ridge regression, Tikhonov regularization), another using the L1 norm (aka lasso) which has the possibility of penalizing coefficients to zero, and thus can serve as a model selection procedure. I have more technical approaches to the lasso and ridge in the lasso and ridge chapters. Note that both L2 and L1 approaches can be seen as maximum a posteriori (MAP) estimates for a Bayesian regression with a specific prior on the coefficients. The L2 approach is akin to a normal prior with zero mean, while L1 is akin to a zero mean Laplace prior. See the Bayesian regression chapter for an approach. Data Setup library(tidyverse) set.seed(123) # ensures replication # predictors and response N = 100 # sample size k = 2 # number of desired predictors X = matrix(rnorm(N * k), ncol = k) y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5) # increasing N will get estimated values closer to these dfXy = data.frame(X,y) Functions A maximum likelihood approach. penalized_ML = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par[-1] # coefficients sigma2 = par[1] # error variance sigma = sqrt(sigma2) N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = dnorm(y, mean = mu, sd = sigma, log = T) # log likelihood PL = switch(type, &#39;L2&#39; = -sum(L) + lambda * crossprod(beta[-1]), # the intercept is not penalized &#39;L1&#39; = -sum(L) + lambda * sum(abs(beta[-1])) ) } glmnet style approach that will put the lambda coefficient on equivalent scale. Uses a different objective function. Note that glmnet is actually elasticnet and mixes both L1 and L2 penalties. penalized_ML2 = function(par, X, y, lambda = .1, type = &#39;L2&#39;) { # arguments- # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # lambda: penalty coefficient # type: penalty approach # setup beta = par # coefficients N = nrow(X) # linear predictor LP = X %*% beta # linear predictor mu = LP # identity link in the glm sense obj = switch(type, &#39;L2&#39; = .5*crossprod(y - X %*% beta)/N + lambda * crossprod(beta[-1]), &#39;L1&#39; = .5*crossprod(y - X %*% beta)/N + lambda * sum(abs(beta[-1])) ) } Estimation Setup the model matrix for use with optim. X = cbind(1, X) Initial values. note we’d normally want to handle the sigma differently as it’s bounded by zero, but we’ll ignore for demonstration. Also sigma2 is not required for the LS approach. init = c(1, rep(0, ncol(X))) names(init) = c(&#39;sigma2&#39;, &#39;intercept&#39;, &#39;b1&#39;, &#39;b2&#39;) optlmpenalized_MLL2 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, control = list(reltol = 1e-12) ) optlmpenalized_MLL1 = optim( par = init, fn = penalized_ML, X = X, y = y, lambda = 1, type = &#39;L1&#39;, control = list(reltol = 1e-12) ) parspenalized_MLL2 = optlmpenalized_MLL2$par parspenalized_MLL1 = optlmpenalized_MLL1$par Comparison Compare to lm. modlm = lm(y ~ ., dfXy) round( rbind( parspenalized_MLL2, parspenalized_MLL1, modlm = c(summary(modlm)$sigma ^ 2, coef(modlm)) ), digits = 4 ) sigma2 intercept b1 b2 parspenalized_MLL2 0.2195 -0.4325 0.1327 0.1113 parspenalized_MLL1 0.2195 -0.4325 0.1306 0.1094 modlm 0.2262 -0.4325 0.1334 0.1119 Compare to glmnet. Setting alpha to 0 and 1 is equivalent to L2 and L1 penalties respectively. You also wouldn’t want to specify lambda normally, and rather let it come about as part of the estimation procedure. We do so here just for demonstration. library(glmnet) glmnetL2 = glmnet( X[, -1], y, alpha = 0, lambda = .01, standardize = FALSE ) glmnetL1 = glmnet( X[, -1], y, alpha = 1, lambda = .01, standardize = FALSE ) pars_L2 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, control = list(reltol = 1e-12) )$par pars_L1 = optim( par = init[-1], fn = penalized_ML2, X = X, y = y, lambda = .01, type = &#39;L1&#39;, control = list(reltol = 1e-12) )$par round( rbind( glmnet_L2 = t(as.matrix(coef(glmnetL2))), pars_L2 = pars_L2, glmnet_L1 = t(as.matrix(coef(glmnetL1))), pars_L1 = pars_L1 ), digits = 4 ) (Intercept) V1 V2 s0 -0.4324 0.1301 0.1094 pars_L2 -0.4324 0.1301 0.1094 s0 -0.4325 0.1207 0.1005 pars_L1 -0.4325 0.1207 0.1005 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/penalized_ML.R "],["lasso.html", "L1 (lasso) regularization Data Setup Functions Estimation Comparison Source", " L1 (lasso) regularization See Tibshirani (1996) for the source, or Murphy PML (2012) for a nice overview (watch for typos in depictions). A more conceptual depiction of the lasso can be found in penalized_ML.R. Data Setup library(tidyverse) set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N*p), ncol=p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6)) y = scale(X %*% b + rnorm(N, sd=.5)) lambda = .1 Functions Coordinate descent. lasso &lt;- function( X, # model matrix y, # target lambda = .1, # penalty parameter soft = TRUE, # soft vs. hard thresholding tol = 1e-6, # tolerance iter = 100, # number of max iterations verbose = TRUE # print out iteration number ) { # soft thresholding function soft_thresh &lt;- function(a, b) { out = rep(0, length(a)) out[a &gt; b] = a[a &gt; b] - b out[a &lt; -b] = a[a &lt; -b] + b out } w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y) tol_curr = 1 J = ncol(X) a = rep(0, J) c_ = rep(0, J) i = 1 while (tol &lt; tol_curr &amp;&amp; i &lt; iter) { w_old = w a = colSums(X^2) l = length(y)*lambda # for consistency with glmnet approach c_ = sapply(1:J, function(j) sum( X[,j] * (y - X[,-j] %*% w_old[-j]) )) if (soft) { for (j in 1:J) { w[j] = soft_thresh(c_[j]/a[j], l/a[j]) } } else { w = w_old w[c_&lt; l &amp; c_ &gt; -l] = 0 } tol_curr = crossprod(w - w_old) i = i + 1 if (verbose &amp;&amp; i%%10 == 0) message(i) } w } Estimation Note, if lambda=0, result is the same as lm.fit. result_soft = lasso( X, y, lambda = lambda, tol = 1e-12, soft = TRUE ) result_hard = lasso( X, y, lambda = lambda, tol = 1e-12, soft = FALSE ) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple values while only wanting the one set to the same lambda value as above (s). library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 1, lambda = c(10, 1, lambda), thresh = 1e-12, intercept = FALSE ), s = lambda ) library(lassoshooting) ls_res = lassoshooting( X = X, y = y, lambda = length(y) * lambda, thr = 1e-12 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), lasso_soft = result_soft, lasso_hard = result_hard, lspack = ls_res$coef, glmnet = glmnet_res[-1, 1], truth = b ) lm lasso_soft lasso_hard lspack glmnet truth X1 0.534988063 0.43542527 0.5348784 0.43542528 0.43552489 0.500 X2 -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500 X3 0.234376590 0.12436834 0.2343207 0.12436835 0.12447920 0.250 X4 -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250 X5 0.126037566 0.02036410 0.1260132 0.02036407 0.02047015 0.125 X6 -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125 X7 -0.016718534 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X8 0.009894575 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X9 -0.005441959 0.00000000 0.0000000 0.00000000 0.00000000 0.000 X10 0.010561128 0.00000000 0.0000000 0.00000000 0.00000000 0.000 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/lasso.R "],["ridge.html", "L2 (ridge) regularization Data Setup Functions Estimation Comparison Source", " L2 (ridge) regularization Compare to lasso chapter. A more conceptual depiction of the lasso can be found in the penalized ML chapter. Data Setup library(tidyverse) set.seed(8675309) N = 500 p = 10 X = scale(matrix(rnorm(N * p), ncol = p)) b = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4)) y = scale(X %*% b + rnorm(N, sd = .5)) Note, if lambda = 0, result is the same as lm.fit. Functions ridge &lt;- function(w, X, y, lambda = .1) { # X: model matrix; # y: target; # lambda: penalty parameter; # w: the weights/coefficients crossprod(y - X %*% w) + lambda * length(y) * crossprod(w) } Estimation result_ridge = optim( rep(0, ncol(X)), ridge, X = X, y = y, lambda = .1, method = &#39;BFGS&#39; ) Analytical result. result_ridge2 = solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y) Alternative with augmented data (note sigma is ignored as it equals 1, but otherwise X/sigma and y/sigma). X2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X))) y2 = c(y, rep(0, ncol(X))) result_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2) glmnet is by default a mixture of ridge and lasso penalties, setting alpha = 1 reduces to lasso, while alpha=0 would be ridge. library(glmnet) glmnet_res = coef( glmnet( X, y, alpha = 0, lambda = c(10, 1, .1), thresh = 1e-12, intercept = F ), s = .1 ) Comparison data.frame( lm = coef(lm(y ~ . - 1, data.frame(X))), ridge = result_ridge$par, ridge2 = result_ridge2, ridge3 = result_ridge3, glmnet = glmnet_res[-1, 1], truth = b ) lm ridge ridge2 ridge3 glmnet truth X1 0.534988063 0.485323748 0.485323748 0.485323748 0.485368766 0.500 X2 -0.529993422 -0.480742032 -0.480742032 -0.480742032 -0.480786661 -0.500 X3 0.234376590 0.209412833 0.209412833 0.209412833 0.209435147 0.250 X4 -0.294350608 -0.268814168 -0.268814168 -0.268814168 -0.268837476 -0.250 X5 0.126037566 0.114963716 0.114963716 0.114963716 0.114973801 0.125 X6 -0.159386728 -0.145880488 -0.145880488 -0.145880488 -0.145892837 -0.125 X7 -0.016718534 -0.021658889 -0.021658889 -0.021658889 -0.021655033 0.000 X8 0.009894575 0.006956965 0.006956965 0.006956965 0.006959470 0.000 X9 -0.005441959 0.001392244 0.001392244 0.001392244 0.001386661 0.000 X10 0.010561128 0.010985385 0.010985385 0.010985385 0.010985102 0.000 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/ridge.R "],["newton-irls.html", "Newton and IRLS Data Setup Functions Comparison Source", " Newton and IRLS Here we demonstrate Newton’s and Iterated Reweighted Least Squares approaches via logistic regression. For the following, I had Murphy’s PML text open and more or less followed the algorithms in chapter 8. Note that for Newton’s method, this doesn’t implement a line search to find a more optimal stepsize at a given iteration. Data Setup Predict graduate school admission based on gre, gpa, and school rank (higher=more prestige). See corresponding demo here: https://stats.idre.ucla.edu/stata/dae/logistic-regression/. The only difference is that I treat rank as numeric rather than categorical. library(tidyverse) admit = haven::read_dta(&#39;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&#39;) comparison_model = glm(admit ~ gre + gpa + rank, data = admit, family = binomial) summary(comparison_model) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Deviance Residuals: Min 1Q Median 3Q Max -1.5802 -0.8848 -0.6382 1.1575 2.1732 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.449549 1.132846 -3.045 0.00233 ** gre 0.002294 0.001092 2.101 0.03564 * gpa 0.777014 0.327484 2.373 0.01766 * rank -0.560031 0.127137 -4.405 1.06e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.44 on 396 degrees of freedom AIC: 467.44 Number of Fisher Scoring iterations: 4 X = model.matrix(comparison_model) y = comparison_model$y Functions Newton’s Method newton &lt;- function( X, y, tol = 1e-12, iter = 500, stepsize = .5 ) { # Args: # X: model matrix # y: target # tol: tolerance # iter: maximum number of iterations # stepsize: (0, 1) # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it +1 ll_old = ll mu = plogis(X %*% beta)[,1] g = crossprod(X, mu-y) # gradient S = diag(mu*(1-mu)) H = t(X) %*% S %*% X # hessian beta = beta - stepsize * solve(H) %*% g ll = sum(dbinom(y, prob = mu, size = 1, log = TRUE)) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll ) } Compare to base R glm. newton_result = newton( X = X, y = y, stepsize = .9, tol = 1e-8 # tol set to 1e-8 as in glm default ) newton_result $beta [,1] (Intercept) -3.449548577 gre 0.002293959 gpa 0.777013649 rank -0.560031371 $iter [1] 8 $tol [1] 2.581544e-10 $loglik [1] -229.7209 comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 rbind( newton = unlist(newton_result), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = -logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA 229.7209 IRLS Note that glm is actually using IRLS, so the results from this should be fairly spot on. irls &lt;- function(X, y, tol = 1e-12, iter = 500) { # intialize int = log(mean(y) / (1 - mean(y))) # intercept beta = c(int, rep(0, ncol(X) - 1)) currtol = 1 it = 0 ll = 0 while (currtol &gt; tol &amp;&amp; it &lt; iter) { it = it + 1 ll_old = ll eta = X %*% beta mu = plogis(eta)[,1] s = mu * (1 - mu) S = diag(s) z = eta + (y-mu)/s beta = solve(t(X) %*% S %*% X) %*% (t(X) %*% (S %*% z)) ll = sum( dbinom( y, prob = plogis(X %*% beta), size = 1, log = T ) ) currtol = abs(ll - ll_old) } list( beta = beta, iter = it, tol = currtol, loglik = ll, weights = plogis(X %*% beta) * (1 - plogis(X %*% beta)) ) } tol set to 1e-8 as in glm default. irls_result = irls(X = X, y = y, tol = 1e-8) str(irls_result) List of 5 $ beta : num [1:4, 1] -3.44955 0.00229 0.77701 -0.56003 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:4] &quot;(Intercept)&quot; &quot;gre&quot; &quot;gpa&quot; &quot;rank&quot; .. ..$ : NULL $ iter : num 4 $ tol : num 6e-09 $ loglik : num -230 $ weights: num [1:400, 1] 0.1536 0.2168 0.2026 0.1268 0.0884 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:400] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... .. ..$ : NULL comparison_model Call: glm(formula = admit ~ gre + gpa + rank, family = binomial, data = admit) Coefficients: (Intercept) gre gpa rank -3.449549 0.002294 0.777014 -0.560031 Degrees of Freedom: 399 Total (i.e. Null); 396 Residual Null Deviance: 500 Residual Deviance: 459.4 AIC: 467.4 Comparison Compare all results. rbind( newton = unlist(newton_result), irls = unlist(irls_result[-length(irls_result)]), glm_default = c( beta = coef(comparison_model), comparison_model$iter, tol = NA, loglik = logLik(comparison_model) ) ) beta1 beta2 beta3 beta4 iter tol loglik newton -3.449549 0.002293959 0.7770136 -0.5600314 8 2.581544e-10 -229.7209 irls -3.449549 0.002293959 0.7770137 -0.5600314 4 5.996583e-09 -229.7209 glm_default -3.449549 0.002293959 0.7770137 -0.5600314 4 NA -229.7209 Compare weights. head(cbind(irls_result$weights, comparison_model$weights)) [,1] [,2] 1 0.15362250 0.15362250 2 0.21679615 0.21679615 3 0.20255723 0.20255724 4 0.12676333 0.12676334 5 0.08835918 0.08835918 6 0.23528108 0.23528108 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/newton_irls.R "],["nelder-mead.html", "Nelder Mead First Version Second Version Source", " Nelder Mead This is based on the pure Python implementation by François Chollet found at https://github.com/fchollet/nelder-mead (also in the Miscellaneous R code repo at nelder_mead.py). This is mostly just an academic exercise on my part. I’m not sure how much one would use the basic NM for many situations. In my experience BFGS and other approaches would be faster, more accurate, and less sensitive to starting values for the types of problems I’ve played around with. Others who actually spend their time researching such things seem to agree. There were two issues on (GitHub)[https://github.com/fchollet/nelder-mead/issues/2] regarding the original code, and I’ve implemented the suggested corrections with notes. The initial function code is not very R-like, as the goal was to keep more similar to the original Python for comparison, which used a list approach. I also provide a more R-like/cleaner version that uses matrices instead of lists, but which still sticks the same approach for the most part. For both functions, comparisons are made using the optimx package, but feel free to use base R’s optim instead. f function to optimize, must return a scalar score and operate over an array of the same dimensions as x_start x_start initial position step look-around radius in initial step no_improve_thr See no_improv_break no_improv_break break after no_improv_break iterations with an improvement lower than no_improv_thr max_iter always break after this number of iterations. Set it to 0 to loop indefinitely. alpha parameters of the algorithm (see Wikipedia page for reference) gamma parameters of the algorithm (see Wikipedia page for reference) rho parameters of the algorithm (see Wikipedia page for reference) sigma parameters of the algorithm (see Wikipedia page for reference) verbose Print iterations? This function returns the best parameter array and best score. First Version nelder_mead = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init dim = length(x_start) prev_best = f(x_start) no_improv = 0 res = list(list(x_start = x_start, prev_best = prev_best)) for (i in 1:dim) { x = x_start x[i] = x[i] + step score = f(x) res = append(res, list(list(x_start = x, prev_best = score))) } # simplex iter iters = 0 while (TRUE) { # order idx = sapply(res, `[[`, 2) res = res[order(idx)] # ascending order best = res[[1]][[2]] # break after max_iter if (max_iter &gt; 0 &amp; iters &gt;= max_iter) return(res[[1]]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[[1]]) # centroid x0 = rep(0, dim) for (tup in 1:(length(res)-1)) { for (i in 1:dim) { x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1) } } # reflection xr = x0 + alpha * (x0 - res[[length(res)]][[1]]) rscore = f(xr) if (res[[1]][[2]] &lt;= rscore &amp; rscore &lt; res[[length(res)-1]][[2]]) { res[[length(res)]] = list(xr, rscore) next } # expansion if (rscore &lt; res[[1]][[2]]) { # xe = x0 + gamma*(x0 - res[[length(res)]][[1]]) # issue with this xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[[length(res)]] = list(xe, escore) next } else { res[[length(res)]] = list(xr, rscore) next } } # contraction # xc = x0 + rho*(x0 - res[[length(res)]][[1]]) # issue with wiki consistency for rho values (and optim) xc = x0 + rho * (res[[length(res)]][[1]] - x0) cscore = f(xc) if (cscore &lt; res[[length(res)]][[2]]) { res[[length(res)]] = list(xc, cscore) next } # reduction x1 = res[[1]][[1]] nres = list() for (tup in res) { redx = x1 + sigma * (tup[[1]] - x1) score = f(redx) nres = append(nres, list(list(redx, score))) } res = nres } } Example The function to minimize. f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } Estimate. nelder_mead( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) [[1]] [1] -1.570797e+00 -2.235577e-07 1.637460e-14 [[2]] [1] -1 Compare to optimx. You may see warnings. optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0.001 A Regression Model I find a regression model to be more applicable/intuitive for my needs, so provide an example for that case. Data Setup library(tidyverse) set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm estimates. lm.fit(X, y)$coef x1 x2 x3 x4 x5 x6 -0.96214657 0.59432481 0.04864576 0.27573466 0.97525840 -0.07470287 nm_result = nelder_mead( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12, verbose = FALSE ) Comparison Compare to optimx. opt_out = optimx::optimx( runif(ncol(X)), fn = f, # model function method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, #rho maxit = 2000, reltol = 1e-12 ) ) rbind( nm_func = unlist(nm_result), nm_optimx = opt_out[1:7] ) p1 p2 p3 p4 p5 p6 value nm_func -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389 501.3155 nm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054 501.3155 Second Version This is a more natural R approach in my opinion. nelder_mead2 = function( f, x_start, step = 0.1, no_improve_thr = 1e-12, no_improv_break = 10, max_iter = 0, alpha = 1, gamma = 2, rho = 0.5, sigma = 0.5, verbose = FALSE ) { # init npar = length(x_start) nc = npar + 1 prev_best = f(x_start) no_improv = 0 res = matrix(c(x_start, prev_best), ncol = nc) colnames(res) = c(paste(&#39;par&#39;, 1:npar, sep = &#39;_&#39;), &#39;score&#39;) for (i in 1:npar) { x = x_start x[i] = x[i] + step score = f(x) res = rbind(res, c(x, score)) } # simplex iter iters = 0 while (TRUE) { # order res = res[order(res[, nc]), ] # ascending order best = res[1, nc] # break after max_iter if (max_iter &amp; iters &gt;= max_iter) return(res[1, ]) iters = iters + 1 # break after no_improv_break iterations with no improvement if (verbose) message(paste(&#39;...best so far:&#39;, best)) if (best &lt; (prev_best - no_improve_thr)) { no_improv = 0 prev_best = best } else { no_improv = no_improv + 1 } if (no_improv &gt;= no_improv_break) return(res[1, ]) nr = nrow(res) # centroid: more efficient than previous double loop x0 = colMeans(res[(1:npar), -nc]) # reflection xr = x0 + alpha * (x0 - res[nr, -nc]) rscore = f(xr) if (res[1, &#39;score&#39;] &lt;= rscore &amp; rscore &lt; res[npar, &#39;score&#39;]) { res[nr,] = c(xr, rscore) next } # expansion if (rscore &lt; res[1, &#39;score&#39;]) { xe = x0 + gamma * (xr - x0) escore = f(xe) if (escore &lt; rscore) { res[nr, ] = c(xe, escore) next } else { res[nr, ] = c(xr, rscore) next } } # contraction xc = x0 + rho * (res[nr, -nc] - x0) cscore = f(xc) if (cscore &lt; res[nr, &#39;score&#39;]) { res[nr,] = c(xc, cscore) next } # reduction x1 = res[1, -nc] nres = res for (i in 1:nr) { redx = x1 + sigma * (res[i, -nc] - x1) score = f(redx) nres[i, ] = c(redx, score) } res = nres } } Example Function f = function(x) { sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1)) } nelder_mead2( f, c(0, 0, 0), max_iter = 1000, no_improve_thr = 1e-12 ) par_1 par_2 par_3 score -1.570797e+00 -2.235577e-07 1.622809e-14 -1.000000e+00 optimx::optimx( par = c(0, 0, 0), fn = f, method = &quot;Nelder-Mead&quot;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 1000, reltol = 1e-12 ) ) p1 p2 p3 value fevals gevals niter convcode kkt1 kkt2 xtime Nelder-Mead -1.570796 1.394018e-08 1.088215e-16 -1 861 NA NA 0 TRUE TRUE 0 A Regression Model set.seed(8675309) N = 500 npreds = 5 X = cbind(1, matrix(rnorm(N * npreds), ncol = npreds)) beta = runif(ncol(X), -1, 1) y = X %*% beta + rnorm(nrow(X)) Least squares loss function to minimize. f = function(b) { crossprod(y - X %*% b)[,1] # if using optimx need scalar } lm_par = lm.fit(X, y)$coef nm_par = nelder_mead2( f, runif(ncol(X)), max_iter = 2000, no_improve_thr = 1e-12 ) Comparison Compare to optimx. opt_par = optimx::optimx( runif(ncol(X)), fn = f, method = &#39;Nelder-Mead&#39;, control = list( alpha = 1, gamma = 2, beta = 0.5, maxit = 2000, reltol = 1e-12 ) )[1:(npreds + 1)] rbind( lm = lm_par, nm = nm_par, optimx = opt_par, truth = beta ) p1 p2 p3 p4 p5 p6 lm -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287 nm -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389 optimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054 truth -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/nelder_mead.R "],["em.html", "Expectation-Maximization Mixture Model Multivariate Mixture Model Probit Model PCA Probabilistic PCA State Space Model", " Expectation-Maximization Mixture Model The following code is based on algorithms noted in Murphy, 2012 Probabilistic Machine Learning, specifically, Chapter 11, section 4. Data Setup This example uses Old Faithful geyser eruptions. This is only a univariate mixture for either eruption time or wait time. The next example will be doing both variables, i.e. multivariate normal. ‘Geyser’ is supposedly more accurate, though seems to have arbitrarily assigned some duration values. See also http://www.geyserstudy.org/geyser.aspx?pGeyserNo=OLDFAITHFUL, but that only has intervals. Some July 1995 data is available. library(tidyverse) # faithful data set is in base R data(faithful) head(faithful) eruptions waiting 1 3.600 79 2 1.800 54 3 3.333 74 4 2.283 62 5 4.533 85 6 2.883 55 eruptions = as.matrix(faithful[, 1, drop = FALSE]) wait_times = as.matrix(faithful[, 2, drop = FALSE]) Function em_mixture &lt;- function( params, X, clusters = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are starting parameters (means, covariances, cluster probability), # data, number of clusters desired, tolerance, maximum iterations, and whether # to show iterations # Starting points N = nrow(X) nams = names(params) mu = params$mu var = params$var probs = params$probs # Other initializations # initialize cluster &#39;responsibilities&#39;, i.e. probability of cluster # membership for each observation i ri = matrix(0, ncol = clusters, nrow = N) it = 0 converged = FALSE if (showits) # Show iterations cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { probsOld = probs muOld = mu varOld = var riOld = ri # E # Compute responsibilities for (k in 1:clusters){ ri[, k] = probs[k] * dnorm(X, mu[k], sd = sqrt(var[k]), log = FALSE) } ri = ri/rowSums(ri) # M rk = colSums(ri) # rk is the weighted average cluster membership size probs = rk/N mu = (t(X) %*% ri) / rk var = (t(X^2) %*% ri) / rk - mu^2 # could do mu and var via log likelihood here, but this is more straightforward parmlistold = rbind(probsOld, muOld, varOld) parmlistcurrent = rbind(probs, mu, var) it = it + 1 # if showits true, &amp; it =1 or divisible by 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(parmlistold - parmlistcurrent)) &lt;= tol } clust = which(round(ri) == 1, arr.ind = TRUE) # create cluster membership clust = clust[order(clust[, 1]), 2] # order according to row rather than cluster out = list( probs = probs, mu = mu, var = var, resp = ri, cluster = clust ) out } Estimation Starting parameters, requires mean, variance and class probability. Note that starts for mean must be within the data range or it will break. params1 = list(mu = c(2, 5), var = c(1, 1), probs = c(.5, .5)) params2 = list(mu = c(50, 90), var = c(1, 15), probs = c(.5, .5)) mix_erupt = em_mixture(params1, X = eruptions, tol = 1e-8) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... mix_waiting = em_mixture(params2, X = wait_times, tol = 1e-8) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... Comparison Compare to flexmix package results. library(flexmix) flex_erupt = flexmix(eruptions ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) flex_wait = flexmix(wait_times ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) The following provides means, variances and probability of group membership. Note that the cluster label is arbitrary so cluster 1 for one model may be cluster 2 in another. Eruptions mean_var = rbind(mix_erupt$mu, sqrt(mix_erupt$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var) = c(&#39;cluster 1&#39;, &#39;cluster 2&#39;) mean_var_flex = parameters(flex_erupt) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var_flex) = c(&#39;cluster 1 flex&#39;, &#39;cluster 2 flex&#39;) prob_membership = mix_erupt$probs prob_membership_flex = flex_erupt@size / sum(flex_erupt@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) $params cluster 1 cluster 2 cluster 1 flex cluster 2 flex means 2.0186078 4.2733434 4.2733678 2.0186385 variances 0.2356218 0.4370631 0.4378355 0.2361098 $clusterpobs prob_membership prob_membership_flex 1 0.3484046 0.6507353 2 0.6515954 0.3492647 Waiting mean_var = rbind(mix_waiting$mu, sqrt(mix_waiting$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var) = c(&#39;cluster 1&#39;, &#39;cluster 2&#39;) mean_var_flex = parameters(flex_wait) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) colnames(mean_var_flex) = c(&#39;cluster 1 flex&#39;, &#39;cluster 2 flex&#39;) prob_membership = mix_waiting$probs prob_membership_flex = flex_wait@size / sum(flex_wait@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) $params cluster 1 cluster 2 cluster 1 flex cluster 2 flex means 54.614856 80.091069 54.616140 80.090678 variances 5.871219 5.867734 5.884422 5.879634 $clusterpobs prob_membership prob_membership_flex 1 0.3608861 0.3639706 2 0.6391139 0.6360294 qplot(x = eruptions, y = waiting, data = faithful) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_waiting$cluster))) + geom_density2d() faithful %&gt;% mutate(prob_clus_1 = mix_waiting$resp[, 1]) %&gt;% ggplot(aes(x = eruptions, y = waiting)) + geom_point(aes(color = prob_clus_1)) + geom_density2d() Supplemental Example This uses the MASS version (reversed columns). These don’t look even remotely the same data on initial inspection- geyser is even more rounded and of opposite conclusion. Turns out geyser is offset by 1, such that duration 1 should be coupled with waiting 2 and on down. Still the rounding at 2 and 4 (and whatever division was done on duration) makes this fairly poor data. I’ve cleaned this up a little bit in case someone wants to play with it for additional practice, but it’s not evaluated. library(MASS) geyser = data.frame(duration = geyser$duration[-299], waiting = geyser$waiting[-1]) # compare to faithful layout(1:2) plot(faithful) plot(geyser) X3 = matrix(geyser[,1]) X4 = matrix(geyser[,2]) # MASS version test3 = em_mixture(params1, X = X3, tol = 1e-8) test4 = em_mixture(params2, X = X4, tol = 1e-8) flexmod3 = flexmix(X3 ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) flexmod4 = flexmix(X4 ~ 1, k = 2, control = list(tolerance = 1e-8, iter.max = 100)) # note variability differences compared to faithful dataset # Eruptions/Duration mean_var = rbind(test3$mu, sqrt(test3$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) mean_var_flex = parameters(flexmod3) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) prob_membership = test3$probs prob_membership_flex = flexmod3@size / sum(flexmod3@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) # Waiting mean_var = rbind(test4$mu, sqrt(test4$var)) rownames(mean_var) = c(&#39;means&#39;, &#39;variances&#39;) mean_var_flex = parameters(flexmod4) rownames(mean_var_flex) = c(&#39;means&#39;, &#39;variances&#39;) prob_membership = test4$probs prob_membership_flex = flexmod4@size / sum(flexmod4@size) list( params = cbind(mean_var, mean_var_flex), clusterpobs = cbind(prob_membership, prob_membership_flex) ) # Some plots library(ggplot2) qplot(x = eruptions, y = waiting, data = faithful) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_waiting$cluster))) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = mix_waiting$resp[, 1])) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20Mixture.R Multivariate Mixture Model The following code is based on algorithms noted in Murphy, 2012 Probabilistic Machine Learning. Specifically, Chapter 11, section 4. Function em_mixture &lt;- function( params, X, clusters = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are # params: starting parameters (means, covariances, cluster probability) # X: data # clusters: number of clusters desired # tol: tolerance # maxits: maximum iterations # showits: whether to show iterations require(mvtnorm) # Starting points N = nrow(X) mu = params$mu var = params$var probs = params$probs # initializations # cluster &#39;responsibilities&#39;, i.e. probability of cluster membership for each # observation i ri = matrix(0, ncol=clusters, nrow=N) ll = 0 # log likelihood it = 0 # iteration count converged = FALSE # convergence # Show iterations if showits == true if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while (!converged &amp; it &lt; maxits) { probsOld = probs # muOld = mu # Use direct values or loglike for convergence check # varOld = var llOld = ll riOld = ri ### E # Compute responsibilities for (k in 1:clusters){ ri[,k] = probs[k] * dmvnorm(X, mu[k, ], sigma = var[[k]], log = FALSE) } ri = ri/rowSums(ri) ### M rk = colSums(ri) # rk is weighted average cluster membership size probs = rk/N for (k in 1:clusters){ varmat = matrix(0, ncol = ncol(X), nrow = ncol(X)) # initialize to sum matrices for (i in 1:N){ varmat = varmat + ri[i,k] * X[i,]%*%t(X[i,]) } mu[k,] = (t(X) %*% ri[,k]) / rk[k] var[[k]] = varmat/rk[k] - mu[k,]%*%t(mu[k,]) ll[k] = -.5*sum( ri[,k] * dmvnorm(X, mu[k,], sigma = var[[k]], log = TRUE) ) } ll = sum(ll) ### compare old to current for convergence parmlistold = c(llOld, probsOld) # c(muOld, unlist(varOld), probsOld) parmlistcurrent = c(ll, probs) # c(mu, unlist(var), probs) it = it + 1 # if showits true, &amp; it =1 or modulo of 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = min(abs(parmlistold - parmlistcurrent)) &lt;= tol } clust = which(round(ri) == 1, arr.ind = TRUE) # create cluster membership clust = clust[order(clust[,1]), 2] # order accoring to row rather than cluster out = list( probs = probs, mu = mu, var = var, resp = ri, cluster = clust, ll = ll ) out } Example 1: Old Faithful eruptions This example uses Old Faithful geyser eruptions. This is can be compared to the univariate code from the other chapter. See also http://www.geyserstudy.org/geyser.aspx?pGeyserNo=OLDFAITHFUL Data Setup library(tidyverse) data(&quot;faithful&quot;) Estimation Create starting values and estimate. mustart = rbind(c(3, 60), c(3, 60.1)) # must be at least slightly different covstart = list(cov(faithful), cov(faithful)) probs = c(.01, .99) # params is a list of mu, var, and probs starts = list(mu = mustart, var = covstart, probs = probs) mix_faithful = em_mixture( params = starts, X = as.matrix(faithful), clusters = 2, tol = 1e-12, maxits = 1500, showits = TRUE ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... 60... 65... 70... 75... 80... str(mix_faithful) List of 6 $ probs : num [1:2] 0.356 0.644 $ mu : num [1:2, 1:2] 2.04 4.29 54.48 79.97 $ var :List of 2 ..$ : num [1:2, 1:2] 0.0692 0.4352 0.4352 33.6973 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : NULL .. .. ..$ : chr [1:2] &quot;eruptions&quot; &quot;waiting&quot; ..$ : num [1:2, 1:2] 0.17 0.941 0.941 36.046 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 .. .. ..$ : NULL .. .. ..$ : chr [1:2] &quot;eruptions&quot; &quot;waiting&quot; $ resp : num [1:272, 1:2] 2.59e-09 1.00 8.42e-06 1.00 1.00e-21 ... $ cluster: int [1:272] 2 1 2 1 2 1 2 2 1 2 ... $ ll : num 477 Visualize. library(ggplot2) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = factor(mix_faithful$cluster))) faithful %&gt;% mutate(prob_clus_1 = mix_faithful$resp[, 1]) %&gt;% ggplot(aes(x = eruptions, y = waiting)) + geom_point(aes(color = prob_clus_1)) # relatively speaking, these are extremely well-separated clusters worst = apply(mix_faithful$resp, 1, function(x) max(x) &lt; .99) ggplot(aes(x = eruptions, y = waiting), data = faithful) + geom_point(aes(color = worst)) Comparison Compare to mclust results. Options are set to be more similar to the settings demonstrated. mix_mclust = mclust::Mclust( faithful[, 1:2], 2, modelNames = &#39;VVV&#39;, control = emControl(tol = 1e-12) ) str(mix_mclust, 1) List of 16 $ call : language mclust::Mclust(data = faithful[, 1:2], G = 2, modelNames = &quot;VVV&quot;, control = emControl(tol = 1e-12)) $ data : num [1:272, 1:2] 3.6 1.8 3.33 2.28 4.53 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ modelName : chr &quot;VVV&quot; $ n : int 272 $ d : int 2 $ G : int 2 $ BIC : &#39;mclustBIC&#39; num [1, 1] -2322 ..- attr(*, &quot;dimnames&quot;)=List of 2 ..- attr(*, &quot;G&quot;)= num 2 ..- attr(*, &quot;modelNames&quot;)= chr &quot;VVV&quot; ..- attr(*, &quot;control&quot;)=List of 4 ..- attr(*, &quot;initialization&quot;)=List of 3 ..- attr(*, &quot;warn&quot;)= logi FALSE ..- attr(*, &quot;n&quot;)= int 272 ..- attr(*, &quot;d&quot;)= int 2 ..- attr(*, &quot;oneD&quot;)= logi FALSE ..- attr(*, &quot;criterion&quot;)= chr &quot;BIC&quot; ..- attr(*, &quot;returnCodes&quot;)= num [1, 1] 0 .. ..- attr(*, &quot;dimnames&quot;)=List of 2 $ loglik : num -1130 $ df : num 11 $ bic : num -2322 $ icl : num -2323 $ hypvol : num NA $ parameters :List of 4 $ z : num [1:272, 1:2] 1.00 1.91e-09 1.00 1.07e-05 1.00 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 $ classification: Named num [1:272] 1 2 1 2 1 2 1 1 2 1 ... ..- attr(*, &quot;names&quot;)= chr [1:272] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... $ uncertainty : Named num [1:272] 2.59e-09 1.91e-09 8.42e-06 1.07e-05 0.00 ... ..- attr(*, &quot;names&quot;)= chr [1:272] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... - attr(*, &quot;class&quot;)= chr &quot;Mclust&quot; # compare means t(mix_faithful$mu) [,1] [,2] [1,] 2.036388 4.289662 [2,] 54.478516 79.968115 mix_mclust$parameters$mean [,1] [,2] eruptions 4.289662 2.036388 waiting 79.968115 54.478517 # compare variances mix_faithful$var [[1]] eruptions waiting [1,] 0.06916767 0.4351676 [2,] 0.43516762 33.6972821 [[2]] eruptions waiting [1,] 0.1699684 0.9406093 [2,] 0.9406093 36.0462113 mix_mclust$parameters$variance$sigma , , 1 eruptions waiting eruptions 0.1699684 0.9406089 waiting 0.9406089 36.0462071 , , 2 eruptions waiting eruptions 0.06916769 0.4351678 waiting 0.43516784 33.6972835 # compare classifications, reverse in case arbitrary numbering of one of them is opposite table(mix_faithful$cluster, mix_mclust$classification) 1 2 1 0 97 2 175 0 table(ifelse(mix_faithful$cluster == 2, 1, 2), mix_mclust$classification) 1 2 1 175 0 2 0 97 # compare responsibilities; reverse one if arbitrary numbering of one of them is opposite # cbind(round(mix_faithful$resp[,1], 2), round(mix_mclust$z[,2], 2)) # cluster &#39;1&#39; # cbind(round(mix_faithful$resp[,2], 2), round(mix_mclust$z[,1], 2)) # cluster &#39;2&#39; Example 2: Iris data set Data Setup Set up data iris2 = iris %&gt;% select(-Species) Estimation Run and examine. We add noise to our starting value, and the function is notably sensitive to starts, but don’t want to cheat too badly. mustart = iris %&gt;% group_by(Species) %&gt;% summarise(across(.fns = function(x) mean(x) + runif(1, 0, .5))) %&gt;% select(-Species) %&gt;% as.matrix() # use purrr::map due to mclust::map covstart = iris %&gt;% split(.$Species) %&gt;% purrr::map(select, -Species) %&gt;% purrr::map(function(x) cov(x) + diag(runif(4, 0, .5))) probs = c(.1, .2, .7) starts = list(mu = mustart, var = covstart, probs = probs) mix_mclust_iris = em_mixture( params = starts, X = as.matrix(iris2), clusters = 3, tol = 1e-8, maxits = 1500, showits = T ) Iterations of EM: 1... table(mix_mclust_iris$cluster, iris$Species) setosa versicolor virginica 1 50 0 0 2 0 48 0 3 0 2 50 Comparison Compare to mclust results. mclust_iris = mclust::Mclust(iris[,1:4], 3) table(mclust_iris$classification, iris$Species) setosa versicolor virginica 1 50 0 0 2 0 45 0 3 0 5 50 Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20Mixture%20MV.R Probit Model The following regards models for a binary response. See Murphy, 2012 Probabilistic Machine Learning Chapter 11.4. Data Setup library(tidyverse) admission = haven::read_dta(&quot;https://stats.idre.ucla.edu/stat/stata/dae/binary.dta&quot;) Probit via Maximum Likelihood Function We’ll start with the a basic maximum likelihood function for a standard probit. See the [logistic regression][Standard Logistic] and previous chapter on probit models for comparison. probit_mle &lt;- function(params, X, y){ # Arguments are starting parameters (coefficients), model matrix, response b = params mu = X %*% b # linear predictor # compute the log likelihood either way # ll = sum(y * pnorm(mu, log.p = TRUE) + (1 - y) * pnorm(-mu, log.p = TRUE)) ll = sum(dbinom(y, 1, prob = pnorm(mu), log = TRUE)) -ll } Estimation Estimate with optim. # input data X = as.matrix(cbind(1, admission[, 2:4])) y = as.matrix(admission[, 1]) init = c(0, 0, 0, 0) # Can set tolerance really low to duplicate glm result result_mle = optim( par = init, fn = probit_mle, X = X, y = y, control = list(maxit = 1000, reltol = 1e-12) ) # extract coefficients coefs_mle = result_mle$par Comparison glm_probit = glm( admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), control = list(maxit = 500, epsilon = 1e-8), data = admission ) summary(glm_probit) Call: glm(formula = admit ~ gre + gpa + rank, family = binomial(link = &quot;probit&quot;), data = admission, control = list(maxit = 500, epsilon = 1e-08)) Deviance Residuals: Min 1Q Median 3Q Max -1.5626 -0.8920 -0.6403 1.1631 2.2097 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.0915039 0.6718360 -3.113 0.00185 ** gre 0.0013982 0.0006487 2.156 0.03112 * gpa 0.4643599 0.1950263 2.381 0.01727 * rank -0.3317117 0.0745524 -4.449 8.61e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.48 on 396 degrees of freedom AIC: 467.48 Number of Fisher Scoring iterations: 4 coefs_glm = coef(glm_probit) Compare. rbind(coefs_mle, coefs_glm) (Intercept) gre gpa rank coefs_mle -2.091510 0.001398222 0.4643609 -0.3317105 coefs_glm -2.091504 0.001398222 0.4643599 -0.3317117 EM for Latent Variable Approach to Probit Function em_probit &lt;- function( params, X, y, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments are starting parameters (coefficients), model matrix, response, # tolerance, maximum iterations, and whether to show iterations #starting points b = params mu = X%*%b it = 0 converged = FALSE z = rnorm(length(y)) # z is the latent variable ~N(0,1) # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) # while no convergence and we haven&#39;t reached our max iterations do this stuff while ((!converged) &amp; (it &lt; maxits)) { z_old = z # create &#39;old&#39; values for comparison # E step create a new z based on current values z = ifelse( y == 1, mu + dnorm(mu) / pnorm(mu), mu - dnorm(mu) / pnorm(-mu) ) # M step estimate b b = solve(t(X)%*%X) %*% t(X)%*%z mu = X%*%b ll = sum(y * pnorm(mu, log.p = TRUE) + (1 - y) * pnorm(-mu, log.p = TRUE)) it = it + 1 if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(z_old - z)) &lt;= tol } # Show last iteration if (showits) cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list(b = t(b), ll = ll) } Estimation Use the same setup and starting values to estimate the parameters. # can lower tolerance to duplicate glm result result_em = em_probit( params = init, X = X, y = y, tol = 1e-12, maxit = 100 ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 51... # result_em coefs_em = result_em$b Comparison Compare all results. rbind(coefs_glm, coefs_mle, coefs_em) (Intercept) gre gpa rank coefs_glm -2.091504 0.001398222 0.4643599 -0.3317117 coefs_mle -2.091510 0.001398222 0.4643609 -0.3317105 admit -2.091504 0.001398222 0.4643599 -0.3317117 rbind(logLik(glm_probit), result$value, result_em$ll) [,1] [1,] -229.7404 [2,] 844.4590 [3,] -229.7404 Visualize Show estimates over niter iterations and visualize. X2 = X X2[, 2:3] = scale(X2[, 2:3]) niter = 20 result_em = map_df(1:niter, function(x) as_tibble( em_probit( params = init, X = X2, y = y, tol = 1e-8, maxit = x, showits = F )$b) ) gdat = result_em %&gt;% rowid_to_column(&#39;iter&#39;) %&gt;% pivot_longer(-iter, names_to = &#39;coef&#39;) %&gt;% mutate( coef = factor(coef, labels = c(&#39;Intercept&#39;, &#39;gre&#39;, &#39;gpa&#39;, &#39;rank&#39;)) ) %&gt;% arrange(iter, coef) ggplot(aes(x = iter, y = value), data = gdat) + geom_line(aes(group = coef, color = coef)) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20probit%20example.R PCA The following is an EM algorithm for principal components analysis. See Murphy, 2012 Probabilistic Machine Learning 12.2.5. Some of the constructed object is based on output from pca function used below. Data Setup state.x77 is from base R, which includes various state demographics. We will first standardize the data. library(tidyverse) X = scale(state.x77) Function em_pca &lt;- function( X, nComp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # nComp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # starting points and other initializations N = nrow(X) D = ncol(X) L = nComp Xt = t(X) Z = t(replicate(L, rnorm(N))) # latent variables W = replicate(L, rnorm(D)) # loadings it = 0 converged = FALSE if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) # while no convergence and we haven&#39;t reached our max iterations do this stuff while ((!converged) &amp; (it &lt; maxits)) { Z_old = Z # create &#39;old&#39; values for comparison Z = solve(t(W)%*%W) %*% crossprod(W, Xt) # E W = Xt%*%t(Z) %*% solve(tcrossprod(Z)) # M it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(Z_old-Z)) &lt;= tol } # calculate reconstruction error Xrecon = W %*% Z reconerr = sum((Xrecon - t(X))^2) # orthogonalize W = pracma::orth(W) # for orthonormal basis of W; pcaMethods package has also evs = eigen(cov(X %*% W)) evals = evs$values evecs = evs$vectors W = W %*% evecs Z = X %*% W if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, reconerr = reconerr, Xrecon = t(Xrecon) ) } Estimation results_pca = em_pca( X = X, nComp = 2, tol = 1e-12, maxit = 1000 ) Iterations of EM: 1... 5... 10... 15... 20... 25... 30... 35... 40... 45... 50... 55... 60... 65... 70... 70... results_pca $scores [,1] [,2] Alabama -3.78988728 0.23477897 Alaska 1.05313550 -5.45617512 Arizona -0.86742876 -0.74506148 Arkansas -2.38177761 1.28834366 California -0.24138147 -3.50952277 Colorado 2.06218136 -0.50566387 Connecticut 1.89943583 0.24300645 Delaware 0.42478394 0.50791950 Florida -1.17212341 -1.13474136 Georgia -3.29417162 -0.10995684 Hawaii 0.48704129 -0.12526216 Idaho 1.42342916 0.61114319 Illinois -0.11896424 -1.28238783 Indiana 0.47120189 0.24520088 Iowa 2.32181208 0.53685609 Kansas 1.90151483 0.07719072 Kentucky -2.12935981 1.06425233 Louisiana -4.24100842 0.34630079 Maine 0.96019374 1.70241922 Maryland 0.20342599 -0.38881112 Massachusetts 1.19589376 0.21865625 Michigan -0.18186944 -0.84711636 Minnesota 2.43361605 0.36533543 Mississippi -4.03208863 1.05124066 Missouri -0.31125449 0.14830589 Montana 1.37887297 0.03353877 Nebraska 2.18101665 0.54774825 Nevada 1.12708455 -1.13291366 New Hampshire 1.67128925 1.31239813 New Jersey 0.64958222 -0.28146986 New Mexico -1.32244692 0.29357041 New York -1.05034998 -1.89371072 North Carolina -2.69433377 0.51713890 North Dakota 2.41766786 0.78192203 Ohio 0.26795708 -0.41685336 Oklahoma -0.07391320 0.64658337 Oregon 1.32472856 -0.22767511 Pennsylvania -0.07738173 -0.26940938 Rhode Island 0.74084731 1.46130325 South Carolina -3.71100631 0.90984427 South Dakota 2.01253414 1.31509491 Tennessee -2.21813394 0.65102504 Texas -2.41364282 -2.32744119 Utah 2.26283736 0.53433138 Vermont 1.36926611 1.50938322 Virginia -0.99354796 -0.18457034 Washington 1.34001299 -0.51154448 West Virginia -1.50662213 1.60198375 Wisconsin 1.75754046 0.63572738 Wyoming 1.48379101 -0.04225606 $loadings [,1] [,2] [1,] -0.12642809 -0.41087417 [2,] 0.29882991 -0.51897884 [3,] -0.46766917 -0.05296872 [4,] 0.41161037 0.08165611 [5,] -0.44425672 -0.30694934 [6,] 0.42468442 -0.29876662 [7,] 0.35741244 0.15358409 [8,] 0.03338461 -0.58762446 $reconerr [1] 135.6901 $Xrecon Population Income Illiteracy Life Exp Murder HS Grad Frost Area Alabama 0.38268358 -1.25437699 1.759977489 -1.54078578 1.611617628 -1.67965020 -1.31849456 -0.26448579 Alaska 2.10865553 3.14634780 -0.203512400 -0.01204852 1.206906839 2.07737324 -0.46157798 3.24134051 Arizona 0.41579388 0.12745748 0.445134639 -0.41788150 0.614057185 -0.14578398 -0.42445943 0.40885758 Arkansas -0.22822355 -1.38036948 1.045642041 -0.87516325 0.662664464 -1.39641793 -0.65340786 -0.83657698 California 1.47248966 1.74923604 0.298781592 -0.38592908 1.184481039 0.94601731 -0.62527962 2.05422301 Colorado -0.05295342 0.87867032 -0.937634273 0.80752470 -0.760924727 1.02685178 0.65938735 0.36598559 Connecticut -0.33998711 0.44149303 -0.901179312 0.80167045 -0.918427793 0.73405859 0.71620393 -0.07938460 Delaware -0.26239562 -0.13666132 -0.225562197 0.21632020 -0.344618674 0.02864973 0.22983142 -0.28428468 Florida 0.61442524 0.23864122 0.608271775 -0.57511671 0.869031809 -0.15875971 -0.59320972 0.62767090 Georgia 0.46165424 -0.92733174 1.546406772 -1.36489386 1.497209045 -1.36613193 -1.19426555 -0.04536132 Hawaii -0.01010871 0.21055091 -0.221139217 0.19024283 -0.177922225 0.24426300 0.15483634 0.08986680 Idaho -0.43106438 0.10819283 -0.698065403 0.63580178 -0.819957964 0.42191900 0.60261317 -0.31160205 Illinois 0.54194046 0.62998207 0.123562347 -0.15368171 0.446478763 0.33261242 -0.23947367 0.74959089 Indiana -0.16031986 0.01355515 -0.233354573 0.21397374 -0.284598854 0.12685427 0.20607238 -0.12835514 Iowa -0.51412256 0.41520995 -1.114276503 0.99951952 -1.196268234 0.82564274 0.91229709 -0.23795697 Kansas -0.27212055 0.52816916 -0.893368551 0.78898632 -0.868454375 0.78448171 0.69148033 0.01812218 Kentucky -0.16806291 -1.18864084 0.939463847 -0.78956388 0.619310844 -1.22226901 -0.59760746 -0.69646856 Louisiana 0.39389653 -1.44706295 1.965045766 -1.71736548 1.777799672 -1.90455332 -1.46260289 -0.34507925 Maine -0.82087554 -0.59658494 -0.539227972 0.53423863 -0.949128976 -0.10084671 0.60464971 -0.96832748 Maryland 0.13403369 0.26257452 -0.074541237 0.05198345 0.028971956 0.20255563 0.01299178 0.23526623 Massachusetts -0.24103476 0.24389086 -0.570864580 0.51009689 -0.598400225 0.44255026 0.46100943 -0.08856331 Michigan 0.37105164 0.38528744 0.129925395 -0.14403157 0.340818527 0.17585298 -0.19510600 0.49171466 Minnesota -0.45778431 0.53763591 -1.157478542 1.03153348 -1.193289744 0.92436879 0.92591437 -0.13343470 Mississippi 0.07784161 -1.75048034 1.830000664 -1.57380929 1.468604826 -2.02644084 -1.27966480 -0.75234446 Missouri -0.02158375 -0.16997977 0.137708556 -0.11600550 0.092754503 -0.17649378 -0.08846880 -0.09753928 Montana -0.18810848 0.39464258 -0.646632879 0.57029706 -0.622868279 0.57556560 0.49797738 0.02632494 Nebraska -0.50079737 0.36748326 -1.049007763 0.94245607 -1.137062259 0.76259490 0.86364791 -0.24905787 Nevada 0.32298982 0.92476479 -0.467093711 0.37141038 -0.152967781 0.81713204 0.22883653 0.70335507 New Hampshire -0.75052840 -0.18167564 -0.851126498 0.79508531 -1.145321214 0.31766975 0.79890305 -0.71540190 New Jersey 0.03352326 0.34019150 -0.288880479 0.24439105 -0.202184277 0.35996125 0.18893948 0.18708463 New Mexico 0.04657394 -0.54754352 0.602917601 -0.52036105 0.497394682 -0.64933164 -0.42757124 -0.21665853 New York 0.91087056 0.66891979 0.591523732 -0.58696799 1.047898292 0.11971028 -0.66625200 1.07772522 North Carolina 0.12816045 -1.07353166 1.232664646 -1.06678818 1.038240427 -1.29874542 -0.88356411 -0.39383276 North Dakota -0.62693269 0.31667048 -1.172086124 1.05898588 -1.314075638 0.79313367 0.98419537 -0.37876361 Ohio 0.13739698 0.29641166 -0.103235078 0.07625529 0.008911129 0.23833907 0.03174915 0.25389887 Oklahoma -0.25631970 -0.35765056 0.000318234 0.02237404 -0.165631902 -0.22456731 0.07288742 -0.38241577 Oregon -0.07393708 0.51402708 -0.607475042 0.52668095 -0.518634834 0.63061330 0.43850719 0.17801302 Pennsylvania 0.12047658 0.11669379 0.050459318 -0.05385004 0.117072383 0.04762771 -0.06903419 0.15572818 Rhode Island -0.69407567 -0.53699812 -0.423874803 0.42426477 -0.777672460 -0.12196232 0.48922098 -0.83396464 South Carolina 0.09534391 -1.58114961 1.687329949 -1.45319435 1.369363378 -1.84783766 -1.18662222 -0.65853727 South Dakota -0.79477937 -0.08110103 -1.010859058 0.93576546 -1.297749324 0.46178543 0.92128241 -0.70559427 Tennessee 0.01294506 -1.00071299 1.002868892 -0.85984677 0.785589193 -1.13651148 -0.69280158 -0.45660979 Texas 1.26143772 0.48662406 1.252067903 -1.18353020 1.786683571 -0.32967476 -1.22012392 1.28708285 Utah -0.50562916 0.39889681 -1.086562111 0.97503875 -1.169293358 0.80135139 0.89083103 -0.23844224 Vermont -0.79328027 -0.37416028 -0.720313637 0.68685449 -1.071609849 0.13055266 0.72121000 -0.84123808 Virginia 0.20144755 -0.20111375 0.474428203 -0.42402594 0.498044099 -0.36680088 -0.38345347 0.07528883 Washington 0.04076514 0.66591672 -0.599586905 0.50979252 -0.438291530 0.72191506 0.40037222 0.34533187 West Virginia -0.46773439 -1.28161942 0.619745694 -0.48932954 0.177599146 -1.11845822 -0.29244627 -0.99166284 Wisconsin -0.48340644 0.19527660 -0.855621146 0.77533290 -0.975935252 0.55646593 0.72580444 -0.31489415 Wyoming -0.17023094 0.46533114 -0.691685058 0.60729331 -0.646213653 0.64276763 0.52383551 0.07436648 Comparison Extract reconstructed values and loadings for comparison. Xrecon = results_pca$Xrecon loadings_em = results_pca$loadings scores_em = results_pca$scores Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) # install via BiocManager::install(&quot;pcaMethods&quot;) result_pcam = pca( X, nPcs = 2, method = &#39;svd&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(result_pcam) scores_pcam = scores(result_pcam) Compare loadings and scores. sum((abs(loadings_pcam) - abs(loadings_em))^2) [1] 1.520589e-24 abs(round(cbind(scores_pcam, scores_em), 2)) PC1 PC2 Alabama 3.79 0.23 3.79 0.23 Alaska 1.05 5.46 1.05 5.46 Arizona 0.87 0.75 0.87 0.75 Arkansas 2.38 1.29 2.38 1.29 California 0.24 3.51 0.24 3.51 Colorado 2.06 0.51 2.06 0.51 Connecticut 1.90 0.24 1.90 0.24 Delaware 0.42 0.51 0.42 0.51 Florida 1.17 1.13 1.17 1.13 Georgia 3.29 0.11 3.29 0.11 Hawaii 0.49 0.13 0.49 0.13 Idaho 1.42 0.61 1.42 0.61 Illinois 0.12 1.28 0.12 1.28 Indiana 0.47 0.25 0.47 0.25 Iowa 2.32 0.54 2.32 0.54 Kansas 1.90 0.08 1.90 0.08 Kentucky 2.13 1.06 2.13 1.06 Louisiana 4.24 0.35 4.24 0.35 Maine 0.96 1.70 0.96 1.70 Maryland 0.20 0.39 0.20 0.39 Massachusetts 1.20 0.22 1.20 0.22 Michigan 0.18 0.85 0.18 0.85 Minnesota 2.43 0.37 2.43 0.37 Mississippi 4.03 1.05 4.03 1.05 Missouri 0.31 0.15 0.31 0.15 Montana 1.38 0.03 1.38 0.03 Nebraska 2.18 0.55 2.18 0.55 Nevada 1.13 1.13 1.13 1.13 New Hampshire 1.67 1.31 1.67 1.31 New Jersey 0.65 0.28 0.65 0.28 New Mexico 1.32 0.29 1.32 0.29 New York 1.05 1.89 1.05 1.89 North Carolina 2.69 0.52 2.69 0.52 North Dakota 2.42 0.78 2.42 0.78 Ohio 0.27 0.42 0.27 0.42 Oklahoma 0.07 0.65 0.07 0.65 Oregon 1.32 0.23 1.32 0.23 Pennsylvania 0.08 0.27 0.08 0.27 Rhode Island 0.74 1.46 0.74 1.46 South Carolina 3.71 0.91 3.71 0.91 South Dakota 2.01 1.32 2.01 1.32 Tennessee 2.22 0.65 2.22 0.65 Texas 2.41 2.33 2.41 2.33 Utah 2.26 0.53 2.26 0.53 Vermont 1.37 1.51 1.37 1.51 Virginia 0.99 0.18 0.99 0.18 Washington 1.34 0.51 1.34 0.51 West Virginia 1.51 1.60 1.51 1.60 Wisconsin 1.76 0.64 1.76 0.64 Wyoming 1.48 0.04 1.48 0.04 Calculate mean squared reconstruction error and compare. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon - X)^2) [1] 0.3392252 mean((Xrecon_pcam - X)^2) [1] 0.3392252 mean(abs(Xrecon_pcam - Xrecon)) [1] 5.120166e-13 Visualize qplot(Xrecon_pcam[,1], X[,1]) qplot(Xrecon_pcam[,2], X[,2]) qplot(Xrecon[,1], Xrecon_pcam[,1]) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20for%20pca.R Probabilistic PCA The following is an EM algorithm for probabilistic principal components analysis. Based on Tipping and Bishop, 1999, and also Murphy 2012 Probabilistic ML, with some code snippets inspired by the ppca function used below. See also ModelFitting/EM Examples/EM for pca.R Data Setup state.x77 is from base R, which includes various state demographics. We will first standardize the data. library(tidyverse) X = scale(state.x77) Function em_ppca &lt;- function( X, nComp = 2, tol = .00001, maxits = 100, showits = TRUE ) { # Arguments # X: numeric data # nComp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # require(pracma) tr = function(x) sum(diag(x), na.rm = TRUE) # starting points and other initializations N = nrow(X) D = ncol(X) L = nComp S = (1/N) * t(X)%*%X evals = eigen(S)$values evecs = eigen(S)$vectors V = evecs[,1:L] Lambda = diag(evals[1:L]) # latent variables Z = t(replicate(L, rnorm(N))) # variance; average variance associated with discarded dimensions sigma2 = 1/(D - L) * sum(evals[(L+1):D]) # loadings; this and sigma2 starting points will be near final estimate W = V %*% chol(Lambda - sigma2 * diag(L)) %*% diag(L) it = 0 converged = FALSE ll = 0 # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { # create &#39;old&#39; values for comparison if(exists(&#39;W_new&#39;)){ W_old = W_new } else { W_old = W } ll_old = ll Psi = sigma2*diag(L) M = t(W_old) %*% W_old + Psi # E and M W_new = S %*% W_old %*% solve( Psi + solve(M) %*% t(W_old) %*% S %*% W_old ) sigma2 = 1/D * tr(S - S %*% W_old %*% solve(M) %*% t(W_new)) Z = solve(M) %*% t(W_new) %*% t(X) ZZ = sigma2*solve(M) + Z%*%t(Z) # log likelihood as in paper # ll = .5*sigma2*D + .5*tr(ZZ) + .5*sigma2 * X%*%t(X) - # 1/sigma2 * t(Z)%*%t(W_new)%*%t(X) + .5*sigma2 * tr(t(W_new)%*%W_new%*%ZZ) # ll = -sum(ll) # more straightforward ll = dnorm(X, mean = t(W_new %*% Z), sd = sqrt(sigma2), log = TRUE) ll = -sum(ll) it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(ll_old-ll)) &lt;= tol } W = pracma::orth(W_new) # for orthonormal basis of W; pcaMethods package has also evs = eigen(cov(X %*% W)) evecs = evs$vectors W = W %*% evecs Z = X %*% W Xrecon = Z %*% t(W) reconerr = sum((Xrecon - X)^2) if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, Xrecon = Xrecon, reconerr = reconerr, ll = ll, sigma2 = sigma2 ) } Estimation results_ppca = em_ppca( X = X, nComp = 2, tol = 1e-12, maxit = 100 ) Iterations of EM: 1... 2... str(results_ppca) List of 6 $ scores : num [1:50, 1:2] 3.79 -1.053 0.867 2.382 0.241 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings: num [1:8, 1:2] 0.126 -0.299 0.468 -0.412 0.444 ... $ Xrecon : num [1:50, 1:8] 0.383 2.109 0.416 -0.228 1.472 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ reconerr: num 136 $ ll : num 369 $ sigma2 : num 0.452 Comparison Extract reconstructed values and loadings for comparison. Xrecon = results_ppca$Xrecon loadings_em = results_ppca$loadings scores_em = results_ppca$scores Compare to standard pca on full data set if desired. standard_pca = princomp(scale(state.x77)) scores_standard_pca = standard_pca$scores[,1:2] loadings_standard_pca = standard_pca$loadings[,1:2] Xrecon_standard_pca = scores_standard_pca%*%t(loadings_standard_pca) Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) results_pcam = pca( X, nPcs = 2, threshold = 1e-8, method = &#39;ppca&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(results_pcam) scores_pcam = scores(results_pcam) Compare loadings and scores. round(cbind(loadings_pcam, loadings_em, loadings_standard_pca), 3) PC1 PC2 Comp.1 Comp.2 Population 0.126 0.411 0.126 0.411 0.126 0.411 Income -0.299 0.519 -0.299 0.519 -0.299 0.519 Illiteracy 0.468 0.053 0.468 0.053 0.468 0.053 Life Exp -0.412 -0.082 -0.412 -0.082 -0.412 -0.082 Murder 0.444 0.307 0.444 0.307 0.444 0.307 HS Grad -0.425 0.299 -0.425 0.299 -0.425 0.299 Frost -0.357 -0.154 -0.357 -0.154 -0.357 -0.154 Area -0.033 0.588 -0.033 0.588 -0.033 0.588 sum((abs(loadings_pcam) - abs(loadings_em)) ^ 2) [1] 3.572549e-16 round(cbind(abs(scores_pcam), abs(scores_em)), 2) PC1 PC2 Alabama 3.79 0.23 3.79 0.23 Alaska 1.05 5.46 1.05 5.46 Arizona 0.87 0.75 0.87 0.75 Arkansas 2.38 1.29 2.38 1.29 California 0.24 3.51 0.24 3.51 Colorado 2.06 0.51 2.06 0.51 Connecticut 1.90 0.24 1.90 0.24 Delaware 0.42 0.51 0.42 0.51 Florida 1.17 1.13 1.17 1.13 Georgia 3.29 0.11 3.29 0.11 Hawaii 0.49 0.13 0.49 0.13 Idaho 1.42 0.61 1.42 0.61 Illinois 0.12 1.28 0.12 1.28 Indiana 0.47 0.25 0.47 0.25 Iowa 2.32 0.54 2.32 0.54 Kansas 1.90 0.08 1.90 0.08 Kentucky 2.13 1.06 2.13 1.06 Louisiana 4.24 0.35 4.24 0.35 Maine 0.96 1.70 0.96 1.70 Maryland 0.20 0.39 0.20 0.39 Massachusetts 1.20 0.22 1.20 0.22 Michigan 0.18 0.85 0.18 0.85 Minnesota 2.43 0.37 2.43 0.37 Mississippi 4.03 1.05 4.03 1.05 Missouri 0.31 0.15 0.31 0.15 Montana 1.38 0.03 1.38 0.03 Nebraska 2.18 0.55 2.18 0.55 Nevada 1.13 1.13 1.13 1.13 New Hampshire 1.67 1.31 1.67 1.31 New Jersey 0.65 0.28 0.65 0.28 New Mexico 1.32 0.29 1.32 0.29 New York 1.05 1.89 1.05 1.89 North Carolina 2.69 0.52 2.69 0.52 North Dakota 2.42 0.78 2.42 0.78 Ohio 0.27 0.42 0.27 0.42 Oklahoma 0.07 0.65 0.07 0.65 Oregon 1.32 0.23 1.32 0.23 Pennsylvania 0.08 0.27 0.08 0.27 Rhode Island 0.74 1.46 0.74 1.46 South Carolina 3.71 0.91 3.71 0.91 South Dakota 2.01 1.32 2.01 1.32 Tennessee 2.22 0.65 2.22 0.65 Texas 2.41 2.33 2.41 2.33 Utah 2.26 0.53 2.26 0.53 Vermont 1.37 1.51 1.37 1.51 Virginia 0.99 0.18 0.99 0.18 Washington 1.34 0.51 1.34 0.51 West Virginia 1.51 1.60 1.51 1.60 Wisconsin 1.76 0.64 1.76 0.64 Wyoming 1.48 0.04 1.48 0.04 Compare reconstructed data sets. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_pcam - X)^2) [1] 0.3392252 mean(abs(Xrecon_pcam - Xrecon)) [1] 6.414878e-09 mean(abs(Xrecon_pcam - Xrecon)) [1] 6.414878e-09 Visualize GGally::ggpairs(data.frame( data = X[, 1], custom = Xrecon[, 1], pcaMeth = Xrecon_pcam[, 1] )) GGally::ggpairs(data.frame( data = X[, 2], custom = Xrecon[, 2], pcaMeth = Xrecon_pcam[, 2] )) qplot(Xrecon[, 1], Xrecon_pcam[, 1]) GGally::ggpairs(data.frame(scores_em, scores_pcam) ) Missing Data Example A slightly revised approach can be taken in the case of missing value. Data Setup # create some missing values set.seed(123) X_miss = X NAindex = sample(length(X), 20) X_miss[NAindex] = NA Function em_ppca_miss = function(X, nComp=2, tol=.00001, maxits=100, showits=T){ # Arguments # X: numeric data # nComp: number of components # tol = tolerance level # maxits: maximum iterations # showits: show iterations # require(pracma) # for orthonormal basis of W; pcaMethods package has also tr = function(x) sum(diag(x), na.rm = TRUE) # starting points and other initializations X_orig = X X = X N = nrow(X_orig) D = ncol(X_orig) L = nComp NAs = is.na(X_orig) X[NAs] = 0 S = (1/N) * t(X)%*%X evals = eigen(S)$values evecs = eigen(S)$vectors V = evecs[,1:L] Lambda = diag(evals[1:L]) # latent variables Z = t(replicate(L, rnorm(N))) # variance; average variance associated with discarded dimensions sigma2 = 1/(D-L) * sum(evals[(L+1):D]) # loadings W = V %*% chol(Lambda-sigma2*diag(L)) %*% diag(L) it = 0 converged = FALSE ll = 0 # Show iterations if (showits) cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { if(exists(&#39;W_new&#39;)){ W_old = W_new } else { W_old = W } ll_old = ll # deal with missingness via projection proj = t(W_old%*%Z) X_new = X_orig X_new[NAs] = proj[NAs] X = X_new Psi = sigma2*diag(L) M = t(W_old) %*% W_old + Psi # E and M W_new = S %*% W_old %*% solve( Psi + solve(M)%*%t(W_old)%*%S%*%W_old ) sigma2 = 1/D * tr(S - S%*%W_old%*%solve(M)%*%t(W_new)) Z = solve(M)%*%t(W_new)%*%t(X) # log likelihood as in paper # ZZ = sigma2*solve(M) + Z%*%t(Z) # ll = .5*sigma2*D + .5*tr(ZZ) + .5*sigma2 * X%*%t(X) - # 1/sigma2 * t(Z)%*%t(W_new)%*%t(X) + .5*sigma2 * tr(t(W_new)%*%W_new%*%ZZ) # ll = -sum(ll) # more straightforward ll = dnorm(X, mean = t(W_new %*% Z), sd = sqrt(sigma2), log = TRUE) ll = -sum(ll) it = it + 1 # if showits, show first and every 5th iteration if (showits &amp; (it == 1 | it%%5 == 0)) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) converged = max(abs(ll_old-ll)) &lt;= tol } W = pracma::orth(W_new) # for orthonormal basis of W evs = eigen(cov(X %*% W)) evecs = evs$vectors W = W %*% evecs Z = X %*% W Xrecon = Z %*% t(W) reconerr = sum((Xrecon-X)^2) if (showits) # Show last iteration cat(paste0(format(it), &quot;...&quot;, &quot;\\n&quot;)) list( scores = Z, loadings = W, Xrecon = Xrecon, reconerr = reconerr, ll = ll, sigma2 = sigma2 ) } Estimation Run the PCA. results_ppca_miss = em_ppca_miss( X = X_miss, nComp = 2, tol = 1e-8, maxit = 100 ) Iterations of EM: 1... 5... 10... 15... 19... str(results_ppca_miss) List of 6 $ scores : num [1:50, 1:2] 3.79 -1.07 0.86 2.35 0.24 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ loadings: num [1:8, 1:2] 0.133 -0.299 0.422 -0.431 0.464 ... $ Xrecon : num [1:50, 1:8] 0.414 1.998 0.448 -0.2 1.521 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... .. ..$ : NULL $ reconerr: num 130 $ ll : num 368 $ sigma2 : num 0.475 Comparison Extract reconstructed values and loadings for comparison. Xrecon = results_ppca_miss$Xrecon loadings_em = results_ppca_miss$loadings scores_em = results_ppca_miss$scores Compare to standard pca on full data set if desired. standard_pca = princomp(scale(state.x77)) scores_standard_pca = standard_pca$scores[,1:2] loadings_standard_pca = standard_pca$loadings[,1:2] Xrecon_standard_pca = scores_standard_pca%*%t(loadings_standard_pca) Compare results to output from pcaMethods, which also has probabilistic PCA (demonstrated next). Note that the signs for loadings/scores may be different library(pcaMethods) results_pcam = pca( X_miss, nPcs = 2, threshold = 1e-8, method = &#39;ppca&#39;, scale = &#39;none&#39;, center = FALSE ) loadings_pcam = loadings(results_pcam) scores_pcam = scores(results_pcam) Compare loadings and scores. round(cbind(loadings_pcam, loadings_em, loadings_standard_pca), 3) PC1 PC2 Comp.1 Comp.2 Population -0.128 -0.416 0.133 0.396 0.126 0.411 Income 0.305 -0.492 -0.299 0.537 -0.299 0.519 Illiteracy -0.434 -0.046 0.422 0.076 0.468 0.053 Life Exp 0.432 0.077 -0.431 -0.080 -0.412 -0.082 Murder -0.464 -0.284 0.464 0.290 0.444 0.307 HS Grad 0.424 -0.288 -0.433 0.318 -0.425 0.299 Frost 0.346 0.170 -0.353 -0.185 -0.357 -0.154 Area 0.041 -0.620 -0.037 0.569 -0.033 0.588 sum((abs(loadings_pcam) - abs(loadings_em)) ^ 2) [1] 0.00738241 round(cbind(abs(scores_pcam), abs(scores_em)), 2) PC1 PC2 Alabama 3.80 0.23 3.79 0.23 Alaska 1.08 5.49 1.07 5.41 Arizona 0.87 0.78 0.86 0.84 Arkansas 2.36 1.25 2.35 1.30 California 0.19 4.02 0.24 3.77 Colorado 1.47 0.40 1.47 0.41 Connecticut 1.94 0.29 1.94 0.19 Delaware 0.40 0.55 0.40 0.47 Florida 1.16 1.12 1.18 1.18 Georgia 3.31 0.10 3.31 0.10 Hawaii 0.60 0.10 0.61 0.29 Idaho 1.41 0.59 1.42 0.62 Illinois 0.16 1.23 0.18 1.24 Indiana 0.49 0.43 0.49 0.43 Iowa 2.33 0.53 2.33 0.53 Kansas 1.91 0.07 1.91 0.06 Kentucky 2.14 1.05 2.14 1.10 Louisiana 3.63 0.39 3.59 0.41 Maine 0.93 1.68 0.94 1.73 Maryland 0.18 0.32 0.16 0.41 Massachusetts 1.22 0.23 1.23 0.17 Michigan 0.22 0.81 0.24 0.80 Minnesota 2.45 0.35 2.45 0.38 Mississippi 4.05 1.30 4.03 1.24 Missouri 0.35 0.14 0.36 0.19 Montana 1.35 0.01 1.35 0.07 Nebraska 2.19 0.53 2.20 0.54 Nevada 0.47 1.32 0.46 1.39 New Hampshire 1.81 1.34 1.81 1.32 New Jersey 0.66 0.24 0.65 0.31 New Mexico 1.29 0.28 1.26 0.29 New York 1.06 1.86 1.08 1.87 North Carolina 2.71 0.51 2.71 0.55 North Dakota 2.45 0.79 2.45 0.79 Ohio 0.24 0.40 0.23 0.38 Oklahoma 0.06 0.61 0.06 0.64 Oregon 1.12 0.29 1.10 0.33 Pennsylvania 0.07 0.63 0.09 0.51 Rhode Island 0.78 1.46 0.79 1.43 South Carolina 3.71 0.90 3.69 0.88 South Dakota 2.22 0.94 2.20 1.02 Tennessee 2.22 0.64 2.22 0.67 Texas 2.35 2.41 2.37 2.30 Utah 2.41 0.60 2.42 0.62 Vermont 1.34 1.52 1.36 1.54 Virginia 1.00 0.16 1.01 0.19 Washington 1.28 0.54 1.26 0.61 West Virginia 1.59 1.54 1.58 1.59 Wisconsin 1.96 0.46 1.96 0.47 Wyoming 1.43 0.01 1.45 0.02 Compare reconstructed data sets. Xrecon_pcam = scores_pcam %*% t(loadings_pcam) mean((Xrecon_pcam - X_miss)^2) [1] NA mean(abs(Xrecon_pcam - Xrecon)) [1] 0.02893291 mean(abs(Xrecon_pcam - Xrecon)) [1] 0.02893291 Visualize GGally::ggpairs(data.frame( data = X_miss[, 1], custom = Xrecon[, 1], pcaMeth = Xrecon_pcam[, 1] )) GGally::ggpairs(data.frame( data = X_miss[, 2], custom = Xrecon[, 2], pcaMeth = Xrecon_pcam[, 2] )) qplot(Xrecon[, 1], Xrecon_pcam[, 1]) GGally::ggpairs(data.frame(scores_em, scores_pcam) ) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20ppca.R Original code for the missing example found at (https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20algorithm%20for%20ppca%20with%20missing.R State Space Model The following regards chapter 11 in Statistical Modeling and Computation, the first example for an unobserved components model. The data regards inflation based on the U.S. consumer price index (infl = 400*log(cpi_t/cpi_{t-1})), from the second quarter of 1947 to the second quarter of 2011. You can acquire the data here or in Datasets repo. Just note that it has 2 mystery columns and one mystery row presumably supplied by Excel. You can also get the CPI data yourself at the Bureau of Labor Statistics in a frustrating fashion, or in a much easier fashion here. For the following I use n instead of t or T because those are transpose and TRUE in R. The model is basically y = τ + ϵ, with ϵ ~ N(0, σ^2), and τ = τ_{n-1} + υ_n with υ ~ N(0, ω^2). Thus each y is associated with a latent variable that follows a random walk over time. ω^2 serves as a smoothing parameter, which itself may be estimated but which is fixed in the following. See the text for more details. Data Setup library(tidyverse) d = read_csv( &#39;https://raw.githubusercontent.com/m-clark/Datasets/master/us%20cpi/USCPI.csv&#39;, col_names = FALSE ) inflation = as.matrix(d$X1) summary(inflation) V1 Min. :-9.557 1st Qu.: 1.843 Median : 3.248 Mean : 3.634 3rd Qu.: 4.819 Max. :15.931 Function EM function for a state space model. em_state_space &lt;- function( params, y, omega2_0, omega2, tol = .00001, maxits = 100, showits = FALSE ) { # Arguments are # params: starting parameters (variance as &#39;sigma2&#39;), # y: data, # tol: tolerance, # omega2: latent variance (2_0) is a noisier starting variance # maxits: maximum iterations # showits: whether to show iterations # Not really needed here, but would be a good idea generally to take advantage # of sparse representation for large data # require(spam) # see usage below # Starting points n = length(y) sigma2 = params$sigma2 # Other initializations H = diag(n) for (i in 1:(ncol(H) - 1)) { H[i + 1, i] = -1 } Omega2 = spam::as.spam(diag(omega2, n)) Omega2[1, 1] = omega2_0 H = spam::as.spam(H) HinvOmega2H = t(H) %*% spam::chol2inv(spam::chol(Omega2)) %*% H # tau ~ N(0, HinvOmmega2H^-1) it = 0 converged = FALSE if (showits) # Show iterations cat(paste(&quot;Iterations of EM:&quot;, &quot;\\n&quot;)) while ((!converged) &amp; (it &lt; maxits)) { sigma2Old = sigma2[1] Sigma2invOld = diag(n)/sigma2Old K = HinvOmega2H + Sigma2invOld # E tau = solve(K, y/sigma2Old) # tau|y, sigma2_{n-1}, omega2 ~ N(0, K^-1) K_inv_tr = sum(1/eigen(K)$values) sigma2 = 1/n * (K_inv_tr + crossprod(y-tau)) # M converged = max(abs(sigma2 - sigma2Old)) &lt;= tol it = it + 1 # if showits true, &amp; it =1 or divisible by 5 print message if (showits &amp; it == 1 | it%%5 == 0) cat(paste(format(it), &quot;...&quot;, &quot;\\n&quot;, sep = &quot;&quot;)) } Kfinal = HinvOmega2H + diag(n) / sigma2[1] taufinal = solve(K, (y / sigma2[1])) list(sigma2 = sigma2, tau = taufinal) } Estimation ss_mod_1 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = 1^2 ) 5... 10... 15... 20... 25... 30... ss_mod_.5 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = .5^2 ) 5... 10... 15... 20... # more smooth ss_mod_.1 = em_state_space( params = data.frame(sigma2 = var(inflation)), y = inflation, tol = 1e-10, omega2_0 = 9, omega2 = .1^2 ) 5... 10... ss_mod_1$sigma2 [,1] [1,] 2.765182 ss_mod_.5$sigma2 [,1] [1,] 4.404707 ss_mod_.1$sigma2 [,1] [1,] 7.489429 Visualization library(lubridate) series = ymd( paste0( rep(1947:2014, e = 4), &#39;-&#39;, c(&#39;01&#39;, &#39;04&#39;, &#39;07&#39;, &#39;10&#39;), &#39;-&#39;, &#39;01&#39;) ) The following corresponds to Fig. 11.1 in the text. library(tidyverse) data.frame( series = series[1:length(inflation)], inflation = inflation, Mod_1 = ss_mod_1$tau, Mod_.5 = ss_mod_.5$tau, Mod_.1 = ss_mod_.1$tau ) %&gt;% ggplot(aes(x = series, y = inflation)) + geom_point(color = &#39;gray50&#39;) + geom_line(aes(y = Mod_1), color = &#39;#ff5500&#39;) + geom_line(aes(y = Mod_.5), color = &#39;skyblue3&#39;) + geom_line(aes(y = Mod_.1), color = &#39;#00aaff&#39;) + geom_smooth(formula = y ~ s(x), # compare to generalized additive model (thicker line) se = FALSE, method = &#39;gam&#39;) + scale_x_date(date_breaks = &#39;10 years&#39;) Source Original code found at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/EM%20Examples/EM%20for%20state%20space%20unobserved%20components.R "],["gradient-descent.html", "Gradient Descent Data Setup Function Estimation Comparison Source", " Gradient Descent Gradient descent for a standard linear regression model. The function takes arguments starting points for the parameters to be estimated, a tolerance or maximum iteration value to provide a stopping point, stepsize (or starting stepsize for adaptive approach), whether to print out iterations, and whether to plot the loss over each iteration. Data Setup Create some basic data for standard regression. library(tidyverse) set.seed(8675309) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) # model matrix Function (Batch) Gradient Descent Algorithm. gd = function( par, X, y, tolerance = 1e-3, maxit = 1000, stepsize = 1e-3, adapt = FALSE, verbose = TRUE, plotLoss = TRUE ) { # initialize beta = par; names(beta) = colnames(X) loss = crossprod(X %*% beta - y) tol = 1 iter = 1 while(tol &gt; tolerance &amp;&amp; iter &lt; maxit){ LP = X %*% beta grad = t(X) %*% (LP - y) betaCurrent = beta - stepsize * grad tol = max(abs(betaCurrent - beta)) beta = betaCurrent loss = append(loss, crossprod(LP - y)) iter = iter + 1 if (adapt) stepsize = ifelse( loss[iter] &lt; loss[iter - 1], stepsize * 1.2, stepsize * .8 ) if (verbose &amp;&amp; iter %% 10 == 0) message(paste(&#39;Iteration:&#39;, iter)) } if (plotLoss) plot(loss, type = &#39;l&#39;, bty = &#39;n&#39;) list( par = beta, loss = loss, RSE = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), iter = iter, fitted = LP ) } Estimation Set starting values. init = rep(0, 3) For any particular data you’d have to fiddle with the stepsize, which could be assessed via cross-validation, or alternatively one can use an adaptive approach, a simple one of which is implemented in this function. gd_result = gd( init, X = X, y = y, tolerance = 1e-8, stepsize = 1e-4, adapt = TRUE ) str(gd_result) List of 5 $ par : num [1:3, 1] 0.985 0.487 0.218 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ loss : num [1:70] 2315 2315 2075 1918 1760 ... $ RSE : num [1, 1] 1.03 $ iter : num 70 $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ... Comparison We can compare to standard linear regression. rbind( gd = round(gd_result$par[, 1], 5), lm = coef(lm(y ~ x1 + x2)) ) Intercept x1 x2 gd 0.9847800 0.4867900 0.2175200 lm 0.9847803 0.4867896 0.2175169 # summary(lm(y ~ x1 + x2)) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/gradient_descent.R "],["stochastic-gradient-descent.html", "Stochastic Gradient Descent Data Setup Function Estimation Comparison Visualize Estimates Data Set Shift Source", " Stochastic Gradient Descent Here we have ‘online’ learning via stochastic gradient descent. See also, standard gradient descent In the following, we have basic data for standard regression, but in this ‘online’ learning case, we can assume each observation comes to us as a stream over time rather than as a single batch, and would continue coming in. Note that there are plenty of variations of this, and it can be applied in the batch case as well. Currently no stopping point is implemented in order to trace results over all data points/iterations. On revisiting this much later, I thought it useful to add that I believe this was motivated by the example in Murphy’s Probabilistic Machine Learning. I also made some cleanup to my original code, added some comments, but mostly left it as it was. Data Setup library(tidyverse) set.seed(1234) n = 1000 x1 = rnorm(n) x2 = rnorm(n) y = 1 + .5*x1 + .2*x2 + rnorm(n) X = cbind(Intercept = 1, x1, x2) Function sgd = function( par, # parameter estimates X, # model matrix y, # target variable stepsize = 1, # the learning rate stepsizeTau = 0, # if &gt; 0, a check on the LR at early iterations average = FALSE ){ # initialize beta = par names(beta) = colnames(X) betamat = matrix(0, nrow(X), ncol = length(beta)) # Collect all estimates fits = NA # fitted values s = 0 # adagrad per parameter learning rate adjustment loss = NA # Collect loss at each point for (i in 1:nrow(X)) { Xi = X[i, , drop = FALSE] yi = y[i] LP = Xi %*% beta # matrix operations not necessary, grad = t(Xi) %*% (LP - yi) # but makes consistent with the standard gd s = s + grad^2 beta = beta - stepsize * grad/(stepsizeTau + sqrt(s)) # adagrad approach if (average &amp; i &gt; 1) { beta = beta - 1/i * (betamat[i - 1, ] - beta) # a variation } betamat[i,] = beta fits[i] = LP loss[i] = (LP - yi)^2 } LP = X %*% beta lastloss = crossprod(LP - y) list( par = beta, # final estimates parvec = betamat, # all estimates loss = loss, # observation level loss RMSE = sqrt(sum(lastloss)/nrow(X)), fitted = fits ) } Estimation Set starting values. init = rep(0, 3) For any particular data you might have to fiddle with the stepsize, perhaps choosing one based on cross-validation with old data. sgd_result = sgd( init, X = X, y = y, stepsize = .1, stepsizeTau = .5, average = FALSE ) str(sgd_result) List of 5 $ par : num [1:3, 1] 1.024 0.537 0.148 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ... $ loss : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ... $ RMSE : num 1.01 $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ... sgd_result$par [,1] Intercept 1.0241049 x1 0.5368198 x2 0.1478470 Comparison We can compare to standard linear regression. # summary(lm(y ~ x1 + x2)) coef1 = coef(lm(y ~ x1 + x2)) rbind( sgd_result = sgd_result$par[, 1], lm = coef1 ) Intercept x1 x2 sgd_result 1.024105 0.5368198 0.1478470 lm 1.029957 0.5177020 0.1631026 Visualize Estimates library(tidyverse) gd = data.frame(sgd_result$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes( x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration == n), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, size = 4, data = filter(gd, Iteration == n) ) Data Set Shift This data includes a shift of the previous data. set.seed(1234) n2 = 1000 x1.2 = rnorm(n2) x2.2 = rnorm(n2) y2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2) X2 = rbind(X, cbind(1, x1.2, x2.2)) coef2 = coef(lm(y2 ~ x1.2 + x2.2)) y2 = c(y, y2) n3 = 1000 x1.3 = rnorm(n3) x2.3 = rnorm(n3) y3 = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3) coef3 = coef(lm(y3 ~ x1.3 + x2.3)) X3 = rbind(X2, cbind(1, x1.3, x2.3)) y3 = c(y2, y3) Estimation sgd_result2 = sgd( init, X = X3, y = y3, stepsize = 1, stepsizeTau = 0, average = FALSE ) str(sgd_result2) List of 5 $ par : num [1:3, 1] 0.821 -0.223 0.211 ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:3] &quot;Intercept&quot; &quot;x1&quot; &quot;x2&quot; .. ..$ : NULL $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ... $ loss : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ... $ RMSE : num 1.57 $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ... Comparison Compare with lm for each data part. sgd_result2$parvec[c(n, n + n2, n + n2 + n3), ] [,1] [,2] [,3] [1,] 1.0859378 0.5128904 0.1457697 [2,] -0.9246994 0.2945723 -0.2941759 [3,] 0.8213521 -0.2229918 0.2112883 rbind(coef1, coef2, coef3) (Intercept) x1 x2 coef1 1.0299573 0.5177020 0.1631026 coef2 -0.9700427 0.2677020 -0.2868974 coef3 1.0453166 -0.2358521 0.2418489 Visualize Estimates Visualize estimates. gd = data.frame(sgd_result2$parvec) %&gt;% mutate(Iteration = 1:n()) gd = gd %&gt;% pivot_longer(cols = -Iteration, names_to = &#39;Parameter&#39;, values_to = &#39;Value&#39;) %&gt;% mutate(Parameter = factor(Parameter, labels = colnames(X))) ggplot(aes(x = Iteration, y = Value, group = Parameter, color = Parameter ), data = gd) + geom_path() + geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 3) + geom_text( aes(label = round(Value, 2)), hjust = -.5, angle = 45, data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)), size = 4, show.legend = FALSE ) Source Original code available at https://github.com/m-clark/Miscellaneous-R-Code/blob/master/ModelFitting/stochastic_gradient_descent.R "],["metropolis-hastings.html", "Metropolis Hastings Example Data Setup Functions Estimation Comparison Source", " Metropolis Hastings Example The following demonstrates a random walk Metropolis-Hastings algorithm using the data and model from prior sections of the document. I had several texts open while cobbling together this code (noted below), and some oriented towards the social sciences. Some parts of the code reflect information and code examples found therein, and follows Lynch’s code a bit more. References: Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis. 3rd ed. Gill, Jeff. 2008. Bayesian Methods : A Social and Behavioral Sciences Approach. Second. Jackman, Simon. 2009. Bayesian Analysis for the Social Sciences. Lynch, Scott M. 2007. Introduction to Applied Bayesian Statistics and Estimation for Social Scientists. Data Setup library(tidyverse) # set seed for replicability set.seed(8675309) # create a N x k matrix of covariates N = 250 K = 3 covariates = replicate(K, rnorm(n = N)) colnames(covariates) = c(&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;) # create the model matrix with intercept X = cbind(Intercept = 1, covariates) # create a normally distributed variable that is a function of the covariates coefs = c(5, .2, -1.5, .9) sigma = 2 mu = X %*% coefs y = rnorm(N, mu, sigma) # same as # y = 5 + .2*X1 - 1.5*X2 + .9*X3 + rnorm(N, mean = 0, sd = 2) # Run lm for later comparison; but go ahead and examine now if desired fit_lm = lm(y ~ ., data = data.frame(X[, -1])) # summary(fit_lm) Functions The primary functions that we need to specify regard the posterior distribution, an update step for beta coefficients, and an update step for the variance estimate. We assume a normal distribution for the β coefficients, inverse gamma on σ2. log_posterior = function(x, y, b, s2) { # Args: X is the model matrix; y the target vector; b and s2 the parameters # to be estimated beta = b sigma = sqrt(s2) sigma2 = s2 mu = X %*% beta # priors are b0 ~ N(0, sd = 10), sigma2 ~ invGamma(.001, .001) priorbvarinv = diag(1/100, 4) prioralpha = priorbeta = .001 if (is.nan(sigma) | sigma&lt;=0) { # scale parameter must be positive return(-Inf) } # Note that you will not find the exact same presentation across texts and # other media for the log posterior in this conjugate setting. In the end # they are conceptually still (log) prior + (log) likelihood (See commented &#39;else&#39;) else { -.5*nrow(X)*log(sigma2) - (.5*(1/sigma2) * (crossprod(y-mu))) + -.5*ncol(X)*log(sigma2) - (.5*(1/sigma2) * (t(beta) %*% priorbvarinv %*% beta)) + -(prioralpha + 1)*log(sigma2) + log(sigma2) - priorbeta/sigma2 } # else { # ll = mvtnorm::dmvnorm(y, mean=mu, sigma=diag(sigma2, length(y)), log=T) # priorb = mvtnorm::dmvnorm(beta, mean=rep(0, length(beta)), sigma=diag(100, length(beta)), log=T) # priors2 = dgamma(1/sigma2, prioralpha, priorbeta, log=T) # logposterior = ll + priorb + priors2 # logposterior # } } Update functions. # update step for regression coefficients updatereg = function(i, x, y, b, s2) { # Args are the same as above but with additional i iterator argument. b[i, ] = MASS::mvrnorm(1, mu = b[i-1, ], Sigma = b_var_scale) # proposal/jumping distribution # Compare to past- does it increase the posterior probability? post_diff = log_posterior(x = x, y = y, b = b[i, ], s2 = s2[i-1]) - log_posterior(x = x, y = y, b = b[i-1, ], s2 = s2[i-1]) # Acceptance phase unidraw = runif(1) accept = unidraw &lt; min(exp(post_diff), 1) # accept if so if (accept) b[i,] else b[i-1,] } # update step for sigma2 updates2 = function(i, x, y, b, s2) { s2candidate = rnorm(1, s2[i-1], sd = sigma_scale) if (s2candidate &lt; 0) { accept = FALSE } else { s2_diff = log_posterior(x = x, y = y, b = b[i, ], s2 = s2candidate) - log_posterior(x = x, y = y, b = b[i, ], s2 = s2[i - 1]) unidraw = runif(1) accept = unidraw &lt; min(exp(s2_diff), 1) } ifelse(accept, s2candidate, s2[i - 1]) } Estimation Now we can set things up for the MCMC chain. Aside from the typical MCMC setup and initializing the parameter matrices to hold the draws from the posterior, we also require scale parameters to use for the jumping/proposal distribution. While this code regards only one chain, though a simple loop or any number of other approaches would easily extend it to two or more. # Setup, starting values etc. nsim = 5000 warmup = 1000 thin = 10 b = matrix(0, nsim, ncol(X)) # initialize beta update matrix s2 = rep(1, nsim) # initialize sigma vector For the following, this c_ term comes from BDA3 12.2 and will produce an acceptance rate of .44 in 1 dimension and declining from there to about .23 in high dimensions. For the sigma_scale, the magic number comes from starting with a value of one and fiddling from there to get around .44. c_ = 2.4/sqrt(ncol(b)) b_var = vcov(fit_lm) b_var_scale = b_var * c_^2 sigma_scale = .9 We can now run and summarize the model with tools from the coda package. # Run for (i in 2:nsim) { b[i, ] = updatereg( i = i, y = y, x = X, b = b, s2 = s2 ) s2[i] = updates2( i = i, y = y, x = X, b = b, s2 = s2 ) } # calculate acceptance rates b_acc_rate = mean(diff(b[(warmup+1):nsim,]) != 0) s2_acc_rate = mean(diff(s2[(warmup+1):nsim]) != 0) b_acc_rate s2_acc_rate # get final chain library(coda) b_mcmc = as.mcmc(b[seq(warmup + 1, nsim, by = thin),]) s2_mcmc = as.mcmc(s2[seq(warmup + 1, nsim, by = thin)]) # get summaries summary(b_mcmc) summary(s2_mcmc) b_acc_rate = mean(diff(b[(warmup+1):nsim,]) != 0) s2_acc_rate = mean(diff(s2[(warmup+1):nsim]) != 0) b_acc_rate s2_acc_rate 0.298 0.43 Summarize results. The following table is uses rstan’s monitor function to produce typical Stan output. parameter mean sd 2.5% 97.5% n_eff Rhat Bulk_ESS Tail_ESS beta.1 4.900 0.135 4.638 5.162 258 1.007 252 235 beta.2 0.083 0.136 -0.192 0.354 289 1.003 294 425 beta.3 -1.468 0.122 -1.706 -1.232 304 1.001 306 427 beta.4 0.831 0.120 0.598 1.061 354 1.000 359 518 sigmasq 4.091 0.377 3.431 4.880 843 1.001 832 964 Comparison We can compare to the lm result or rstanarm. fit_rstan = rstanarm::stan_glm(y ~ ., data = data.frame(X[, -1])) SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 1). Chain 1: Chain 1: Gradient evaluation took 7e-05 seconds Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.7 seconds. Chain 1: Adjust your expectations accordingly! Chain 1: Chain 1: Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) Chain 1: Chain 1: Elapsed Time: 0.029119 seconds (Warm-up) Chain 1: 0.042325 seconds (Sampling) Chain 1: 0.071444 seconds (Total) Chain 1: SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 2). Chain 2: Chain 2: Gradient evaluation took 8e-06 seconds Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. Chain 2: Adjust your expectations accordingly! Chain 2: Chain 2: Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) Chain 2: Chain 2: Elapsed Time: 0.028329 seconds (Warm-up) Chain 2: 0.042675 seconds (Sampling) Chain 2: 0.071004 seconds (Total) Chain 2: SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 3). Chain 3: Chain 3: Gradient evaluation took 1.6e-05 seconds Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. Chain 3: Adjust your expectations accordingly! Chain 3: Chain 3: Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) Chain 3: Chain 3: Elapsed Time: 0.031293 seconds (Warm-up) Chain 3: 0.041503 seconds (Sampling) Chain 3: 0.072796 seconds (Total) Chain 3: SAMPLING FOR MODEL &#39;continuous&#39; NOW (CHAIN 4). Chain 4: Chain 4: Gradient evaluation took 1e-05 seconds Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. Chain 4: Adjust your expectations accordingly! Chain 4: Chain 4: Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) Chain 4: Chain 4: Elapsed Time: 0.030578 seconds (Warm-up) Chain 4: 0.042411 seconds (Sampling) Chain 4: 0.072989 seconds (Total) Chain 4: parameter fit lm rstanarm (Intercept) 4.900 4.898 4.898 X1 0.083 0.084 0.088 X2 -1.468 -1.469 -1.468 X3 0.831 0.820 0.821 sigma_sq 4.091 4.084 4.110 Source Original demo here: https://m-clark.github.io/bayesian-basics/appendix.html#metropolis-hastings-example "],["hamiltonian-monte-carlo.html", "Hamiltonian Monte Carlo Example Data Setup Functions Estimation Comparison", " Hamiltonian Monte Carlo Example The following demonstrates Hamiltonian Monte Carlo, the technique that Stan uses, and which is a different estimation approach than the Gibbs sampler in BUGS/JAGS. If you are interested in the details enough to be reading this, I highly recommend Betancourt’s conceptual introduction to HMC. This example is largely based on the code in the appendix of Gelman. Gelman et al. 2013. Bayesian Data Analysis. 3rd ed. Data Setup library(tidyverse) # set seed for replicability set.seed(8675309) # create a N x k matrix of covariates N = 250 K = 3 covariates = replicate(K, rnorm(n = N)) colnames(covariates) = c(&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;) # create the model matrix with intercept X = cbind(Intercept = 1, covariates) # create a normally distributed variable that is a function of the covariates coefs = c(5, .2, -1.5, .9) sigma = 2 mu = X %*% coefs y = rnorm(N, mu, sigma) # same as # y = 5 + .2*X1 - 1.5*X2 + .9*X3 + rnorm(N, mean = 0, sd = 2) # Run lm for later comparison; but go ahead and examine now if desired fit_lm = lm(y ~ ., data = data.frame(X[, -1])) # summary(fit_lm) Functions First we start with the log posterior function. log_posterior = function(X, y, th) { # Args # X: the model matrix # y: the target vector # th: theta, the current parameter estimates beta = th[-length(th)] # reg coefs to be estimated sigma = th[length(th)] # sigma to be estimated sigma2 = sigma^2 mu = X %*% beta # priors are b0 ~ N(0, sd=10), sigma2 ~ invGamma(.001, .001) priorbvarinv = diag(1/100, 4) prioralpha = priorbeta = .001 if (is.nan(sigma) | sigma&lt;=0) { # scale parameter must be positive, so post return(-Inf) # density is zero if it jumps below zero } # log posterior in this conjugate setting. conceptually it&#39;s (log) prior + # (log) likelihood. (See commented &#39;else&#39; for alternative) else { -.5*nrow(X)*log(sigma2) - (.5*(1/sigma2) * (crossprod(y-mu))) + -.5*ncol(X)*log(sigma2) - (.5*(1/sigma2) * (t(beta) %*% priorbvarinv %*% beta)) + -(prioralpha + 1)*log(sigma2) + log(sigma2) - priorbeta/sigma2 } # else { # ll = mvtnorm::dmvnorm(y, mean=mu, sigma=diag(sigma2, length(y)), log=T) # priorb = mvtnorm::dmvnorm(beta, mean=rep(0, length(beta)), sigma=diag(100, length(beta)), log=T) # priors2 = dgamma(1/sigma2, prioralpha, priorbeta, log=T) # logposterior = ll + priorb + priors2 # logposterior # } } The following is the numerical gradient function as given in BDA3 p. 602. It has the same arguments as the log posterior function. gradient_theta = function(X, y, th) { d = length(th) e = .0001 diffs = numeric(d) for (k in 1:d) { th_hi = th th_lo = th th_hi[k] = th[k] + e th_lo[k] = th[k] - e diffs[k] = (log_posterior(X, y, th_hi) - log_posterior(X, y, th_lo)) / (2 * e) } diffs } The following is a function for a single HMC iteration. ϵ and L are drawn randomly at each iteration to explore other areas of the posterior (starting with epsilon0 and L0); The mass matrix M, expressed as a vector, is a bit of a magic number in this setting. It regards the mass of a particle whose position is represented by \\(\\theta\\), and momentum by \\(\\phi\\). See the sampling section of the Stan manual for more detail. hmc_iteration = function(X, y, th, epsilon, L, M) { # Args # epsilon: the stepsize # L: the number of leapfrog steps # M: a diagonal mass matrix # initialization M_inv = 1/M d = length(th) phi = rnorm(d, 0, sqrt(M)) th_old = th log_p_old = log_posterior(X, y, th) - .5*sum(M_inv * phi^2) phi = phi + .5 * epsilon * gradient_theta(X, y, th) for (l in 1:L) { th = th + epsilon*M_inv*phi phi = phi + ifelse(l == L, .5, 1) * epsilon * gradient_theta(X, y, th) } # here we get into standard MCMC stuff, jump or not based on a draw from a # proposal distribution phi = -phi log_p_star = log_posterior(X, y, th) - .5*sum(M_inv * phi^2) r = exp(log_p_star - log_p_old) if (is.nan(r)) r = 0 p_jump = min(r, 1) if (runif(1) &lt; p_jump) { th_new = th } else { th_new = th_old } # returns estimates and acceptance rate list(th = th_new, p_jump = p_jump) } Main HMC function. hmc_run = function(starts, iter, warmup, epsilon_0, L_0, M, X, y) { # # Args: # starts: starting values # iter: total number of simulations for each chain (note chain is based on the dimension of starts) # warmup: determines which of the initial iterations will be ignored for inference purposes # epsilon0: the baseline stepsize # L0: the baseline number of leapfrog steps # M: is the mass vector chains = nrow(starts) d = ncol(starts) sims = array(NA, c(iter, chains, d), dimnames = list(NULL, NULL, colnames(starts))) p_jump = matrix(NA, iter, chains) for (j in 1:chains) { th = starts[j,] for (t in 1:iter) { epsilon = runif(1, 0, 2*epsilon_0) L = ceiling(2*L_0*runif(1)) temp = hmc_iteration(X, y, th, epsilon, L, M) p_jump[t,j] = temp$p_jump sims[t,j,] = temp$th th = temp$th } } # acceptance rate acc = round(colMeans(p_jump[(warmup + 1):iter,]), 3) message(&#39;Avg acceptance probability for each chain: &#39;, paste0(acc[1],&#39;, &#39;,acc[2]), &#39;\\n&#39;) list(sims = sims, p_jump = p_jump) } Estimation With the primary functions in place, we set the starting values and choose other settings for for the HMC process. The coefficient starting values are based on random draws from a uniform distribution, while \\(\\sigma\\) is set to a value of one in each case. As in the other examples we’ll have 5000 total draws with warm-up set to 2500. I don’t have any thinning option here, but that could be added or simply done as part of the coda package preparation. # Starting values and mcmc settings parnames = c(paste0(&#39;beta[&#39;, 1:4, &#39;]&#39;), &#39;sigma&#39;) d = length(parnames) chains = 2 theta_start = t(replicate(chains, c(runif(d-1, -1, 1), 1))) colnames(theta_start) = parnames nsim = 1000 wu = 500 We can fiddle with these sampling parameters to get a desirable acceptance rate of around .80. The following work well with the data we have. stepsize = .08 nLeap = 10 vars = rep(1, 5) mass_vector = 1 / vars We are now ready to run the model. On my machine and with the above settings, it took a couple seconds. Once complete we can use the coda package if desired as we have done before. # Run the model fit_hmc = hmc_run( starts = theta_start, iter = nsim, warmup = wu, epsilon_0 = stepsize, L_0 = nLeap, M = mass_vector, X = X, y = y ) # str(fit_hmc, 1) # use coda if desired library(coda) theta = as.mcmc.list(list(as.mcmc(fit_hmc$sims[(wu+1):nsim, 1,]), as.mcmc(fit_hmc$sims[(wu+1):nsim, 2,]))) summary(theta) Iterations = 1:500 Thinning interval = 1 Number of chains = 2 Sample size per chain = 500 1. Empirical mean and standard deviation for each variable, plus standard error of the mean: Mean SD Naive SE Time-series SE beta[1] 4.89772 0.1279 0.004044 0.005822 beta[2] 0.07741 0.1333 0.004217 0.005806 beta[3] -1.47220 0.1216 0.003845 0.004795 beta[4] 0.82477 0.1108 0.003504 0.004359 sigma 2.01291 0.0899 0.002843 0.003583 2. Quantiles for each variable: 2.5% 25% 50% 75% 97.5% beta[1] 4.6451 4.80901 4.9043 4.9855 5.1393 beta[2] -0.1924 -0.01143 0.0730 0.1694 0.3336 beta[3] -1.7102 -1.55404 -1.4713 -1.3933 -1.2244 beta[4] 0.6124 0.75492 0.8238 0.8965 1.0424 sigma 1.8439 1.95009 2.0120 2.0705 2.2039 fit_summary = summary(theta)$statistics[,&#39;Mean&#39;] beta_est = fit_summary[1:4] sigma_est = fit_summary[5] # log_posterior(X, y, fit_summary) The following table is uses rstan’s monitor function to produce typical Stan output. Table 1: log posterior = -301.716 parameter mean sd 2.5% 97.5% n_eff Rhat Bulk_ESS Tail_ESS beta[1] 4.898 0.128 4.645 5.139 505 1.006 521 305 beta[2] 0.077 0.133 -0.192 0.334 560 1.012 566 240 beta[3] -1.472 0.122 -1.710 -1.224 628 1.010 630 457 beta[4] 0.825 0.111 0.612 1.042 618 1.000 630 569 sigma 2.013 0.090 1.844 2.204 682 1.004 693 425 Comparison Our estimates look pretty good, and inspection of the diagnostics would show good mixing and convergence as well. At this point we can compare it to the Stan output. For the following, I use the same inverse gamma prior and tweaked the control options for a little bit more similarity, but that’s not necessary. data { // Data block int&lt;lower = 1&gt; N; // Sample size int&lt;lower = 1&gt; K; // Dimension of model matrix matrix [N, K] X; // Model Matrix vector[N] y; // Target variable } parameters { // Parameters block; declarations only vector[K] beta; // Coefficient vector real&lt;lower = 0&gt; sigma; // Error scale } model { // Model block vector[N] mu; mu = X * beta; // Creation of linear predictor // priors beta ~ normal(0, 10); sigma ~ inv_gamma(.001, .001); // changed to gamma a la code above // likelihood y ~ normal(mu, sigma); } SAMPLING FOR MODEL &#39;36376eea30b8a42e7dd71dd3e6747dd8&#39; NOW (CHAIN 1). Chain 1: Chain 1: Gradient evaluation took 3.6e-05 seconds Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds. Chain 1: Adjust your expectations accordingly! Chain 1: Chain 1: Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) Chain 1: Chain 1: Elapsed Time: 0.007385 seconds (Warm-up) Chain 1: 0.007252 seconds (Sampling) Chain 1: 0.014637 seconds (Total) Chain 1: SAMPLING FOR MODEL &#39;36376eea30b8a42e7dd71dd3e6747dd8&#39; NOW (CHAIN 2). Chain 2: Chain 2: Gradient evaluation took 1e-05 seconds Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. Chain 2: Adjust your expectations accordingly! Chain 2: Chain 2: Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) Chain 2: Chain 2: Elapsed Time: 0.023831 seconds (Warm-up) Chain 2: 0.023475 seconds (Sampling) Chain 2: 0.047306 seconds (Total) Chain 2: Here are the results. Table 2: log posterior = -1270.794 term estimate std.error conf.low conf.high beta[1] 1.421 3.336 -1.761 5.095 beta[2] 1.075 0.818 -0.121 1.710 beta[3] 0.220 1.474 -1.667 1.480 beta[4] -0.094 0.786 -0.738 1.013 sigma 2.034 0.064 1.889 2.199 And finally, the standard least squares fit (Residual standard error = sigma). Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.898 0.1284 38.13 3.026e-105 X1 0.08408 0.1296 0.6488 0.5171 X2 -1.469 0.1261 -11.64 3.049e-25 X3 0.8196 0.1207 6.793 8.207e-11 Fitting linear model: y ~ . Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 250 2.021 0.4524 0.4458 "],["supplemental.html", "Supplemental Other Languages", " Supplemental IN PROGRESS Other Languages When doing some of these models and algorithms, I had some other code to work with in another language, or, at the time, just wanted to try it in that language. There is not a whole lot here, but it still may be useful to some. Refer to the corresponding chapter of R code for context. Python Demos Linear Regression Data Setup import numpy as np import pandas as pd from scipy.stats import norm from scipy.optimize import minimize np.random.seed(123) # ensures replication # predictors and response # increasing N will get estimated values closer to the known parameters N = 1000 # sample size k = 2 # number of desired predictors X = np.matrix(np.random.normal(size = N * k)).reshape(N, k) y = -.5 + .2*X[:, 0] + .1*X[:, 1] + np.random.normal(scale = .5, size = N).reshape(N, 1) dfXy = pd.DataFrame(np.column_stack([X, y]), columns = [&#39;X1&#39;, &#39;X2&#39;, &#39;y&#39;]) Functions A maximum likelihood approach. def lm_ml(par, X, y): # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par[1:].reshape(3, 1) # coefficients sigma = par[0] # error sd # N = X.shape[0] # linear predictor LP = X * beta # linear predictor mu = LP # identity link in the glm sense # calculate likelihood L = norm.pdf(y, loc = mu, scale = sigma) # log likelihood # L = -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu) # alternate log likelihood form L = -np.sum(np.log(L)) # optim by default is minimization, and we want to maximize the likelihood return(L) An approach via least squares loss function. def lm_ls(par, X, y): # par: parameters to be estimated # X: predictor matrix with intercept column # y: response # setup beta = par.reshape(3, 1) # coefficients N = X.shape[0] p = X.shape[1] # linear predictor LP = X * beta # linear predictor mu = LP # identity link in the glm sense # # # squared error loss # return(np.sum(np.square(y - mu))) Estimation X_mm = np.column_stack([np.repeat(1, N).reshape(N, 1), X]) # you may get warnings, they can be ignored result_ml = minimize( fun = lm_ml, x0 = [1, 0, 0, 0], args = (X_mm, y), bounds = ((0, None), (None, None), (None, None), (None, None)), method = &#39;L-BFGS-B&#39;, tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) # can use least_squares directly # from scipy.optimize import least_squares result_ls = minimize( lm_ls, x0 = np.array([0, 0, 0]), args = (X_mm, y), tol = 1e-12, options = {&#39;maxiter&#39;: 500} ) Comparison import statsmodels.formula.api as smf model_sm_ols = smf.ols(&#39;y ~ X1 + X2&#39;, data = dfXy) result_sm_ols = model.fit() pd.DataFrame( [ np.append(result_ml.x[0]**2, result_ml.x[1:]), np.append(result_ls.fun/(N - X_mm.shape[1] - 1), result_ls.x), np.append(result_sm_ols.scale, result_sm_ols.params) ], columns = [&#39;sigma&#39;,&#39;Int&#39;, &#39;b_X1&#39;, &#39;b_X2&#39;], index = [&#39;ML&#39;, &#39;OLS&#39;, &#39;SM&#39;] ) sigma Int b_X1 b_X2 ML 0.240332 -0.494647 0.213036 0.0815 OLS 0.241297 -0.494647 0.213036 0.0815 SM 0.241055 -0.494647 0.213036 0.0815 Nelder-Mead import copy &#39;&#39;&#39; Francois Chollet&#39;s Nelder-Mead in Python. https://github.com/fchollet/nelder-mead/blob/master/nelder_mead.py Pure Python/Numpy implementation of the Nelder-Mead algorithm. Reference: https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method &#39;&#39;&#39; def nelder_mead( f, x_start, step = 0.1, no_improve_thr = 10e-6, no_improv_break = 10, max_iter = 0, alpha = 1., gamma = 2., rho = 0.5, sigma = 0.5 ): &#39;&#39;&#39; @param f (function): function to optimize, must return a scalar score and operate over a numpy array of the same dimensions as x_start @param x_start (numpy array): initial position @param step (float): look-around radius in initial step @no_improv_thr, no_improv_break (float, int): break after no_improv_break iterations with an improvement lower than no_improv_thr @max_iter (int): always break after this number of iterations. Set it to 0 to loop indefinitely. @alpha, gamma, rho, sigma (floats): parameters of the algorithm (see Wikipedia page for reference) return: tuple (best parameter array, best score) &#39;&#39;&#39; # init dim = len(x_start) prev_best = f(x_start) no_improv = 0 res = [[x_start, prev_best]] for i in range(dim): x = copy.copy(x_start) x[i] = x[i] + step score = f(x) res.append([x, score]) # simplex iter iters = 0 while 1: # order res.sort(key=lambda x: x[1]) best = res[0][1] # break after max_iter if max_iter and iters &gt;= max_iter: return res[0] iters += 1 # break after no_improv_break iterations with no improvement if iters//10 == 0: print(&#39;...best so far:&#39;, best) if best &lt; prev_best - no_improve_thr: no_improv = 0 prev_best = best else: no_improv += 1 if no_improv &gt;= no_improv_break: return res[0] # centroid x0 = [0.] * dim for tup in res[:-1]: for i, c in enumerate(tup[0]): x0[i] += c / (len(res)-1) # reflection xr = x0 + alpha*(x0 - res[-1][0]) rscore = f(xr) if res[0][1] &lt;= rscore &lt; res[-2][1]: del res[-1] res.append([xr, rscore]) continue # expansion if rscore &lt; res[0][1]: xe = x0 + gamma*(x0 - res[-1][0]) escore = f(xe) if escore &lt; rscore: del res[-1] res.append([xe, escore]) continue else: del res[-1] res.append([xr, rscore]) continue # contraction xc = x0 + rho*(x0 - res[-1][0]) cscore = f(xc) if cscore &lt; res[-1][1]: del res[-1] res.append([xc, cscore]) continue # reduction x1 = res[0][0] nres = [] for tup in res: redx = x1 + sigma*(tup[0] - x1) score = f(redx) nres.append([redx, score]) res = nres if __name__ == &quot;__main__&quot;: # test import math import numpy as np def f(x): return math.sin(x[0]) * math.cos(x[1]) * (1. / (abs(x[2]) + 1)) nelder_mead(f, np.array([0., 0., 0.])) HMM #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; From the wikipedia page with slight modification https://en.wikipedia.org/wiki/Viterbi_algorithm#Example &quot;&quot;&quot; def viterbi(obs, states, start_p, trans_p, emit_p): V = [{}] for st in states: V[0][st] = {&quot;prob&quot;: start_p[st] * emit_p[st][obs[0]], &quot;prev&quot;: None} # Run Viterbi when t &gt; 0 for t in range(1, len(obs)): V.append({}) for st in states: max_tr_prob = max(V[t-1][prev_st][&quot;prob&quot;]*trans_p[prev_st][st] for prev_st in states) for prev_st in states: if V[t-1][prev_st][&quot;prob&quot;] * trans_p[prev_st][st] == max_tr_prob: max_prob = max_tr_prob * emit_p[st][obs[t]] V[t][st] = {&quot;prob&quot;: max_prob, &quot;prev&quot;: prev_st} break for line in dptable(V): print(line) opt = [] # The highest probability max_prob = max(value[&quot;prob&quot;] for value in V[-1].values()) previous = None # Get most probable state and its backtrack for st, data in V[-1].items(): if data[&quot;prob&quot;] == max_prob: opt.append(st) previous = st break # Follow the backtrack till the first observation for t in range(len(V) - 2, -1, -1): opt.insert(0, V[t + 1][previous][&quot;prev&quot;]) previous = V[t + 1][previous][&quot;prev&quot;] print(&#39;The steps of states are &#39; + &#39; &#39;.join(opt) + &#39; with highest probability of %s&#39; % max_prob) def dptable(V): # Print a table of steps from dictionary yield &quot; &quot;.join((&quot;%12d&quot; % i) for i in range(len(V))) for state in V[0]: yield &quot;%.7s: &quot; % state + &quot; &quot;.join(&quot;%.7s&quot; % (&quot;%f&quot; % v[state][&quot;prob&quot;]) for v in V) # The function viterbi takes the following arguments: obs is the sequence of # observations, e.g. [&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;]; states is the set of hidden # states; start_p is the start probability; trans_p are the transition # probabilities; and emit_p are the emission probabilities. For simplicity of # code, we assume that the observation sequence obs is non-empty and that # trans_p[i][j] and emit_p[i][j] is defined for all states i,j. # In the running example, the forward/Viterbi algorithm is used as follows: obs = (&#39;normal&#39;, &#39;cold&#39;, &#39;dizzy&#39;) states = (&#39;Healthy&#39;, &#39;Fever&#39;) start_p = {&#39;Healthy&#39;: 0.6, &#39;Fever&#39;: 0.4} trans_p = { &#39;Healthy&#39; : {&#39;Healthy&#39;: 0.7, &#39;Fever&#39;: 0.3}, &#39;Fever&#39; : {&#39;Healthy&#39;: 0.4, &#39;Fever&#39;: 0.6} } emit_p = { &#39;Healthy&#39; : {&#39;normal&#39;: 0.5, &#39;cold&#39;: 0.4, &#39;dizzy&#39;: 0.1}, &#39;Fever&#39; : {&#39;normal&#39;: 0.1, &#39;cold&#39;: 0.3, &#39;dizzy&#39;: 0.6} } viterbi(obs,states,start_p,trans_p,emit_p) Julia Demos I haven’t played with Julia in a very long time, but briefly hacked the old code to get something that worked. As Julia has gone though notable changes, it’s doubtful these are very good as far as Julia code goes, though conceptually they may still provide some utility. Perhaps at some point I’ll reteach myself the basics and come back to these. In any case the code did run on a Jupyter notebook. Mixed Models One-factor ##################### ### Main function ### ##################### using LinearAlgebra using Statistics function one_factor_re_loglike(par::Vector) d, ni = size(y) mu = par[1] sigma2_mu = par[2] sigma2 = par[3] Sigmai = sigma2*I(ni) + sigma2_mu*ones(ni, ni) l = -(ni*d)/2*log(2*pi) - d/2*log(det(Sigmai)) for i in 1:d yi = y[i,:] l = l - .5(yi .- mu)&#39; * (Sigmai\\(yi .- mu)) end l = -l[1] return l end ################### ### Data set up ### ################### y = [22.6 20.5 20.8 22.6 21.2 20.5 17.3 16.2 16.6 21.4 23.7 23.2 20.9 22.2 22.6 14.5 10.5 12.3 20.8 19.1 21.3 17.4 18.6 18.6 25.1 24.8 24.9 14.9 16.3 16.6] ################################ ### Starting values and test ### ################################ using Statistics mu0 = mean(y) sigma2_mu0 = var(mean(y, dims = 2)) sigma20 = mean(var(y, dims = 2)) theta0 = [mu0, sigma2_mu0, sigma20] ### test one_factor_re_loglike(theta0) ########### ### Run ### ########### using Optim res = optimize(one_factor_re_loglike, theta0, LBFGS()) res * Status: success * Candidate solution Final objective value: 6.226441e+01 * Found with Algorithm: L-BFGS * Convergence measures |x - x&#39;| = 2.93e-08 ≰ 0.0e+00 |x - x&#39;|/|x&#39;| = 1.49e-09 ≰ 0.0e+00 |f(x) - f(x&#39;)| = 1.42e-14 ≰ 0.0e+00 |f(x) - f(x&#39;)|/|f(x&#39;)| = 2.28e-16 ≰ 0.0e+00 |g(x)| = 1.17e-09 ≤ 1.0e-08 * Work counters Seconds run: 1 (vs limit Inf) Iterations: 7 f(x) calls: 22 ∇f(x) calls: 22 Optim.minimizer(res) 3-element Array{Float64,1}: 19.599999980440952 12.193999992338886 1.1666666662195693 Optim.minimum(res) 62.30661224610756 Two-factor using LinearAlgebra using Statistics function sfran2_loglike(par::Vector) n = length(y) mu = par[1] sigma2_alpha = exp(par[2]) sigma2_gamma = exp(par[3]) sigma2 = exp(par[4]) Sigma = sigma2*I(n) + sigma2_alpha*(Xalpha * Xalpha&#39;) + sigma2_gamma * (Xgamma * Xgamma&#39;) l = -n/2*log(2*pi) - sum(log.(diag(cholesky(Sigma).L))) - .5*(y .- mu)&#39; * (Sigma\\(y .- mu)) l = -l[1] return l end ################## ### Data setup ### ################## y = [1.39,1.29,1.12,1.16,1.52,1.62,1.88,1.87,1.24,1.18, .95,.96,.82,.92,1.18,1.20,1.47,1.41,1.57,1.65] ################################ ### Starting values and test ### ################################ yhat = mean(reshape(y, 4, 5), 1) theta0 = [mean(y), log(var(yhat)), log(var(y)/3), log(var(y)/3)] sfran2_loglike(theta0) ########### ### Run ### ########### using Optim res = optimize(sfran2_loglike, theta0, method = :l_bfgs) res * Status: success * Candidate solution Final objective value: -1.199315e+01 * Found with Algorithm: L-BFGS * Convergence measures |x - x&#39;| = 6.60e-09 ≰ 0.0e+00 |x - x&#39;|/|x&#39;| = 1.08e-09 ≰ 0.0e+00 |f(x) - f(x&#39;)| = 5.33e-15 ≰ 0.0e+00 |f(x) - f(x&#39;)|/|f(x&#39;)| = 4.44e-16 ≰ 0.0e+00 |g(x)| = 7.02e-10 ≤ 1.0e-08 * Work counters Seconds run: 0 (vs limit Inf) Iterations: 9 f(x) calls: 23 ∇f(x) calls: 23 exp.(Optim.minimizer(res)) 4-element Array{Float64,1}: 3.7434213772629223 0.053720000000540405 0.031790000003692476 0.002290000000530042 -2*Optim.minimum(res) 23.98630759443859 Matlab Demos I don’t code in Matlab, nor have any particular desire to, so this is provided here just for reference. Mixed Models One-factor % matlab from Statistical Modeling and Computation (2014 p 311). See the % associated twofactorRE.R file for details. function one_factor_re_loglike(mu, sigma2_mu, sigma2, y) [d ni] = size(y); Sigmai = sigma2*eye(ni) + sigma2_mu*ones(ni,ni); l = -(ni*d) / 2*log(2*pi) - d / 2*log(det(Sigmai)); for i=1:d yi = y(i, :)&#39;; l = l - .5*(yi - mu)&#39; * (Sigmai\\(yi - mu)); end end y = [22.6 20.5 20.8; 22.6 21.2 20.5; 17.3 16.2 16.6; 21.4 23.7 23.2; 20.9 22.2 22.6; 14.5 10.5 12.3; 20.8 19.1 21.3; 17.4 18.6 18.6; 25.1 24.8 24.9; 14.9 16.3 16.6]; f = @(theta) -one_factor_re_loglike(theta(1), theta(2), theta(3), y); ybar = mean(y, 2); theta0 = [mean(ybar) var(ybar) mean(var(y, 0, 2))]; thetahat = fminsearch(f, theta0); Two-factor % matlab from Statistical Modeling and Computation (2014 p 314). See the % associated twofactorRE.R file for details. function sfran2_loglike(mu, eta_alpha, eta_gamma, eta, y, Xalpha, Xgamma) sigma2_alpha = exp(eta_alpha); sigma2_gamma = exp(eta_gamma); sigma2 = exp(eta); n = length(y); Sigma = sigma2*speye(n) + sigma2_alpha * (Xalpha * Xalpha&#39;) + sigma2_gamma * (Xgamma*Xgamma&#39;); l = -n/2 * log(2*pi) - sum(log(diag(chol(Sigma)))) - .5*(y - mu)&#39; * (Sigma\\(y - mu)); end y = [1.39 1.29 1.12 1.16 1.52 1.62 1.88 1.87 1.24 1.18 .95 .96 .82 .92 1.18 1.20 1.47 1.41 1.57 1.65]; Xalpha = kron(speye(5), ones(4,1)); Xgamma = kron(speye(10), ones(2,1)); f = @(theta) -sfran_loglike(theta(1), theta(2), theta(3), theta(4), y, Xalpha, Xgamma); yhat = mean(reshape(y, 4, 5)); theta0 = [mean(y) log(var(yhat)) log(var(y)/3) log(var(y)/3)]; thetahat = fminsearch(f, theta0) Gaussian Processes Any updates on the following can be found at the repo. function S = gaussSample(arg1, arg2, arg3) % Returns n samples (in the rows) from a multivariate Gaussian distribution % % Examples: % S = gaussSample(mu, Sigma, 10) % S = gaussSample(model, 100) % S = gaussSample(struct(&#39;mu&#39;,[0], &#39;Sigma&#39;, eye(1)), 3) % This file is from pmtk3.googlecode.com switch nargin case 3, mu = arg1; Sigma = arg2; n = arg3; case 2, model = arg1; mu = model.mu; Sigma = model.Sigma; n = arg2; case 1, model = arg1; mu = model.mu; Sigma = model.Sigma; n = 1; otherwise error(&#39;bad num args&#39;) end A = chol(Sigma, &#39;lower&#39;); Z = randn(length(mu), n); S = bsxfun(@plus, mu(:), A*Z)&#39;; end %% Visualize the effect of change the hyper-params for a 1d GP regression % based on demo_gpr by Carl Rasmussen % %% Generate data % This file is from pmtk3.googlecode.com n = 20; rand(&#39;state&#39;,18); randn(&#39;state&#39;,20); covfunc = {&#39;covSum&#39;, {&#39;covSEiso&#39;,&#39;covNoise&#39;}}; loghyper = [log(1.0); log(1.0); log(0.1)]; x = 15*(rand(n,1)-0.5); y = chol(feval(covfunc{:}, loghyper, x))&#39;*randn(n,1); % Cholesky decomp. xstar = linspace(-7.5, 7.5, 201)&#39;; hyps = [log(1), log(1), log(0.1);... log(0.3),log(1.08),log(0.00005);... log(3),log(1.16),log(0.89)]; %% compute post pred and plot marginals for i=1:size(hyps,1) loghyper = hyps(i,:)&#39;; [mu, S2] = gpr(loghyper, covfunc, x, y, xstar); S2 = S2 - exp(2*loghyper(3)); % remove observation noise figure; f = [mu+2*sqrt(S2);flipdim(mu-2*sqrt(S2),1)]; fill([xstar; flipdim(xstar,1)], f, [7 7 7]/8, &#39;EdgeColor&#39;, [7 7 7]/8); hold on plot(xstar,mu,&#39;k-&#39;,&#39;LineWidth&#39;,2); plot(x, y, &#39;k+&#39;, &#39;MarkerSize&#39;, 17); axis([-8 8 -3 3]) printPmtkFigure(sprintf(&#39;gprDemoChangeHparams%d&#39;, i)); end %% Reproduce figure 2.2 from GP book % %% % This file is from pmtk3.googlecode.com setSeed(0); L = 1; xs = (-5:0.2:5)&#39;; ns = length(xs); keps = 1e-8; muFn = @(x) 0*x(:).^2; Kfn = @(x,z) 1*exp(-sq_dist(x&#39;/L,z&#39;/L)/2); % plot sampled functions from the prior figure; hold on for i=1:3 model = struct(&#39;mu&#39;, muFn(xs), &#39;Sigma&#39;, Kfn(xs, xs) + 1e-15*eye(size(xs, 1))); fs = gaussSample(model, 1); plot(xs, fs, &#39;k-&#39;, &#39;linewidth&#39;, 2) end printPmtkFigure(&#39;gprDemoNoiseFreePrior&#39;) % generate noise-less training data Xtrain = [-4, -3, -2, -1, 1]&#39;; ftrain = sin(Xtrain); % compute posterior predictive K = Kfn(Xtrain, Xtrain); % K Ks = Kfn(Xtrain, xs); %K_* Kss = Kfn(xs, xs) + keps*eye(length(xs)); % K_** (keps is essential!) Ki = inv(K); postMu = muFn(xs) + Ks&#39;*Ki*(ftrain - muFn(Xtrain)); postCov = Kss - Ks&#39;*Ki*Ks; figure; hold on % plot marginal posterior variance as gray band mu = postMu(:); S2 = diag(postCov); f = [mu+2*sqrt(S2);flipdim(mu-2*sqrt(S2),1)]; fill([xs; flipdim(xs,1)], f, [7 7 7]/8, &#39;EdgeColor&#39;, [7 7 7]/8); % plot samples from posterior predictive for i=1:3 model = struct(&#39;mu&#39;, postMu(:)&#39;, &#39;Sigma&#39;, postCov); fs = gaussSample(model, 1); plot(xs, fs, &#39;k-&#39;, &#39;linewidth&#39;, 2) h=plot(Xtrain, ftrain, &#39;kx&#39;, &#39;markersize&#39;, 12, &#39;linewidth&#39;, 3); end printPmtkFigure(&#39;gprDemoNoiseFreePost&#39;) "]]
